@article{MERINO2016123,
title = {A Data Quality in Use model for Big Data},
journal = {Future Generation Computer Systems},
volume = {63},
pages = {123-130},
year = {2016},
note = {Modeling and Management for Big Data Analytics and Visualization},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15003817},
author = {Jorge Merino and Ismael Caballero and Bibiano Rivas and Manuel Serrano and Mario Piattini},
keywords = {Data Quality, Big Data, Measurement, Quality-in-Use, Model},
abstract = {Beyond the hype of Big Data, something within business intelligence projects is indeed changing. This is mainly because Big Data is not only about data, but also about a complete conceptual and technological stack including raw and processed data, storage, ways of managing data, processing and analytics. A challenge that becomes even trickier is the management of the quality of the data in Big Data environments. More than ever before the need for assessing the Quality-in-Use gains importance since the real contribution–business value–of data can be only estimated in its context of use. Although there exists different Data Quality models for assessing the quality of regular data, none of them has been adapted to Big Data. To fill this gap, we propose the “3As Data Quality-in-Use model”, which is composed of three Data Quality characteristics for assessing the levels of Data Quality-in-Use in Big Data projects: Contextual Adequacy, Operational Adequacy and Temporal Adequacy. The model can be integrated into any sort of Big Data project, as it is independent of any pre-conditions or technologies. The paper shows the way to use the model with a working example. The model accomplishes every challenge related to Data Quality program aimed for Big Data. The main conclusion is that the model can be used as an appropriate way to obtain the Quality-in-Use levels of the input data of the Big Data analysis, and those levels can be understood as indicators of trustworthiness and soundness of the results of the Big Data analysis.}
}
@article{VYDRA2019101383,
title = {Techno-optimism and policy-pessimism in the public sector big data debate},
journal = {Government Information Quarterly},
volume = {36},
number = {4},
pages = {101383},
year = {2019},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2019.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X18302326},
author = {Simon Vydra and Bram Klievink},
keywords = {Big data, Analytics, Government, Public administration, Policy-making, Decision-making, Science-policy interface, Network governance},
abstract = {Despite great potential, high hopes and big promises, the actual impact of big data on the public sector is not always as transformative as the literature would suggest. In this paper, we ascribe this predicament to an overly strong emphasis the current literature places on technical-rational factors at the expense of political decision-making factors. We express these two different emphases as two archetypical narratives and use those to illustrate that some political decision-making factors should be taken seriously by critiquing some of the core ‘techno-optimist’ tenets from a more ‘policy-pessimist’ angle. In the conclusion we have these two narratives meet ‘eye-to-eye’, facilitating a more systematized interrogation of big data promises and shortcomings in further research, paying appropriate attention to both technical-rational and political decision-making factors. We finish by offering a realist rejoinder of these two narratives, allowing for more context-specific scrutiny and balancing both technical-rational and political decision-making concerns, resulting in more realistic expectations about using big data for policymaking in practice.}
}
@article{SHARDT2020104,
title = {Data Quality Assessment for System Identification in the Age of Big Data and Industry 4.0},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {104-113},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.103},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320303591},
author = {Yuri A.W. Shardt and Xu Yang and Kevin Brooks and Andrei Torgashov},
keywords = {data quality assessment, system identification, big data, Industry 4.0, soft sensors},
abstract = {As the amount of data stored from industrial processes increases with the demands of Industry 4.0, there is an increasing interest in finding uses for the stored data. However, before the data can be used its quality must be determined and appropriate regions extracted. Initially, such testing was done manually using graphs or basic rules, such as the value of a variable. With large data sets, such an approach will not work, since the amount of data to tested and the number of potential rules is too large. Therefore, there is a need for automated segmentation of the data set into different components. Such an approach has recently been proposed and tested using various types of industrial data. Although the industrial results are promising, there still remain many unanswered questions including how to handle a priori knowledge, over- or undersegmentation of the data set, and setting the appropriate thresholds for a given application. Solving these problems will provide a robust and reliable method for determining the data quality of a given data set.}
}
@article{LIU2020123,
title = {Urban big data fusion based on deep learning: An overview},
journal = {Information Fusion},
volume = {53},
pages = {123-133},
year = {2020},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2019.06.016},
url = {https://www.sciencedirect.com/science/article/pii/S1566253519301393},
author = {Jia Liu and Tianrui Li and Peng Xie and Shengdong Du and Fei Teng and Xin Yang},
keywords = {Urban computing, Big data, Data fusion, Deep learning},
abstract = {Urban big data fusion creates huge values for urban computing in solving urban problems. In recent years, various models and algorithms based on deep learning have been proposed to unlock the power of knowledge from urban big data. To clarify the methodologies of urban big data fusion based on deep learning (DL), this paper classifies them into three categories: DL-output-based fusion, DL-input-based fusion and DL-double-stage-based fusion. These methods use deep learning to learn feature representation from multi-source big data. Then each category of fusion methods is introduced and some examples are shown. The difficulties and ideas of dealing with urban big data will also be discussed.}
}
@article{ELIA2020617,
title = {A multi-dimension framework for value creation through Big Data},
journal = {Industrial Marketing Management},
volume = {90},
pages = {617-632},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2020.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S0019850120302212},
author = {Gianluca Elia and Gloria Polimeno and Gianluca Solazzo and Giuseppina Passiante},
keywords = {Big Data analytics, Cognitive computing, Framework, Model, Systematic literature review, Value creation},
abstract = {Big Data represents a promising area for value creation and frontier research. The potential to extract actionable insights from Big Data has gained increasing attention of both academics and practitioners operating in several industries. Marketing domain has become from the start a field for experiments with Big Data approaches, even if the adoption of Big Data solutions does not always generate effective value for the adopters. Therefore, the gap existing between the potential of value creation embedded in the Big Data paradigm and the current limited exploitation of this value represents an area of investigation that this paper aims to explore. In particular, by following a systematic literature review, this study aims at presenting a framework that outlines the multiple value directions that the Big Data paradigm can generate for the adopting organizations. Eleven distinct value directions have been identified and then grouped in five dimensions (Informational, Transactional, Transformational, Strategic, Infrastructural Value), which constitute the pillars of the proposed framework. Finally, the framework has been also preliminarily applied in three case studies conducted within three Italian based companies operating in different industries (e-commerce, fast-moving consumer goods, and banking) in the final aim to see its applicability in real business scenarios.}
}
@article{MAJOR202056,
title = {Using big data in pediatric oncology: Current applications and future directions},
journal = {Seminars in Oncology},
volume = {47},
number = {1},
pages = {56-64},
year = {2020},
note = {Pediatric Oncology},
issn = {0093-7754},
doi = {https://doi.org/10.1053/j.seminoncol.2020.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0093775420300063},
author = {Ajay Major and Suzanne M. Cox and Samuel L. Volchenboum},
keywords = {Pediatric oncology, Pediatric cancer, Big data, Data sharing, Data science, Informatics},
abstract = {Pediatric cancer is a rare disease with a low annual incidence, which presents a significant challenge in being able to collect enough data to fuel clinical discoveries. Big data registry trials hold promise to advance the study of pediatric cancers by allowing for the combination of traditional randomized controlled trials with the power of larger cohort sizes. The emergence of big data resources and data-sharing initiatives are becoming transformative for pediatric cancer diagnosis and treatment. This review discusses the uses of big data in pediatric cancer, existing pediatric cancer registry initiatives and research, the challenges in harmonizing these data to improve accessibility for study, and building pediatric data commons and other important future endeavors.}
}
@article{GHASEMAGHAEI201938,
title = {Can big data improve firm decision quality? The role of data quality and data diagnosticity},
journal = {Decision Support Systems},
volume = {120},
pages = {38-49},
year = {2019},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619300466},
author = {Maryam Ghasemaghaei and Goran Calic},
keywords = {Big data utilization, Data quality, Decision quality, Data diagnosticity},
abstract = {Anecdotal evidence suggests that, despite the large variety of data, the huge volume of generated data, and the fast velocity of obtaining data (i.e., big data), quality of big data is far from perfect. Therefore, many firms defer collecting and integrating big data as they have concerns regarding the impact of utilizing big data on data diagnosticity (i.e., retrieval of valuable information from data) and firm decision making quality. In this study, we use the Organizational Learning Theory and Wang and Strong's data quality framework to explore the impact of processing big data on firm decision quality and the mediating role of data quality (DQ) and data diagnosticity on this relationship. We validate the proposed research model using survey data from 130 firms, obtained from data analysts and IT managers. Results confirm the critical role of DQ in increasing data diagnosticity and improving firm decision quality when processing big data; suggesting important implications for practice and theory. Findings also reveal that while big data utilization positively impacts contextual DQ, accessibility DQ, and representational DQ, interestingly, it negatively impacts intrinsic DQ. Furthermore, findings show that while intrinsic DQ, contextual DQ, and representational DQ significantly increase data diagnosticity, accessibility DQ does not influence it. Most importantly, the findings show that big data utilization does not significantly impact the quality of firm decisions and it is fully mediated through DQ and data diagnosticity. The results of this study contribute to practice by providing important guidelines for managers to improve firm decision quality through the use of big data.}
}
@article{CARRA2020300,
title = {Data-driven ICU management: Using Big Data and algorithms to improve outcomes},
journal = {Journal of Critical Care},
volume = {60},
pages = {300-304},
year = {2020},
issn = {0883-9441},
doi = {https://doi.org/10.1016/j.jcrc.2020.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0883944120306791},
author = {Giorgia Carra and Jorge I.F. Salluh and Fernando José {da Silva Ramos} and Geert Meyfroidt},
keywords = {Big data, Data mining, Machine learning, Predictive modeling, Intensive care unit},
abstract = {The digitalization of the Intensive Care Unit (ICU) led to an increasing amount of clinical data being collected at the bedside. The term “Big Data” can be used to refer to the analysis of these datasets that collect enormous amount of data of different origin and format. Complexity and variety define the value of Big Data. In fact, the retrospective analysis of these datasets allows to generate new knowledge, with consequent potential improvements in the clinical practice. Despite the promising start of Big Data analysis in medical research, which has seen a rising number of peer-reviewed articles, very limited applications have been used in ICU clinical practice. A close future effort should be done to validate the knowledge extracted from clinical Big Data and implement it in the clinic. In this article, we provide an introduction to Big Data in the ICU, from data collection and data analysis, to the main successful examples of prognostic, predictive and classification models based on ICU data. In addition, we focus on the main challenges that these models face to reach the bedside and effectively improve ICU care.}
}
@article{SAFHI201930,
title = {Assessing reliability of Big Data Knowledge Discovery process},
journal = {Procedia Computer Science},
volume = {148},
pages = {30-36},
year = {2019},
note = {THE SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919300055},
author = {Hicham Moad Safhi and Bouchra Frikh and Brahim Ouhbi},
keywords = {Knowledge discovery, Reliability, Trustworthiness, Quality, Big Data mining},
abstract = {Extracting knowledge from Big Data is the process of transforming this data into actionable information. The exponential growth of data has initiated a myriad of new opportunities, and made data become the most valuable raw material of production for many organizations. Mining Big Data is coupled with some challenges, known as the 3V’s of Big Data: Volume, Variety and Velocity. However, a major challenge that needs to be addressed, and often is ignored in the literature, concerns reliability. Actually, data is agglomerated from multiple disparate sources, and each of Knowledge Discovery (KDD) process steps may be carried out by different organizations. These considerations lead us to ask a critical question that is weather the information we have at each step is reliable enough to proceed to the next one? This paper therefore aims to provide a framework that automatically assesses reliability of the knowledge discovery process. We focus on Linked Open Data (LOD) as a source of data, as it constitutes a relevant data provider in many Big Data applications. However, our framework can also be adapted for unstructured data. This framework will assist scientists to automatically and efficiently measure the reliability of each KDD process stage as well as detect unreliable steps that should be revised. Following this methodology, KDD process will be optimized and therefore produce knowledge with higher quality.}
}
@article{TALHA2019916,
title = {Big Data: Trade-off between Data Quality and Data Security},
journal = {Procedia Computer Science},
volume = {151},
pages = {916-922},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.127},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919305915},
author = {M. TALHA and A. ABOU {EL KALAM} and N. ELMARZOUQI},
keywords = {Big Data, Data Quality, Data Security, Trade-off between Quality, Security},
abstract = {The essence of an information system lies in the data; if it is not of good quality or not sufficiently protected, the consequences will undoubtedly be harmful. Quality and Security are two essential aspects that add value to data and their implementation has become a real need and must be adopted before any data exploitation. Due to the high volume of data, their diversity and their rapid generation, effective implementation of such systems requires well thought out mechanisms and strategies. This paper provides an overview of Data Quality and Data Security in a Big Data context. We want through this paper to highlight the conflicts that may exist during the implementation of data security and data quality management systems. Such a conflict makes the complexity greater and requires new adapted solutions.}
}
@article{VALENCIAPARRA2020101180,
title = {Unleashing Constraint Optimisation Problem solving in Big Data environments},
journal = {Journal of Computational Science},
volume = {45},
pages = {101180},
year = {2020},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2020.101180},
url = {https://www.sciencedirect.com/science/article/pii/S1877750320304816},
author = {Álvaro Valencia-Parra and Ángel Jesús Varela-Vaca and Luisa Parody and María Teresa Gómez-López},
keywords = {Big Data, Optimisation problem, Constraint programming, Distributed data, Heterogeneous data format},
abstract = {The application of the optimisation problems in the daily decisions of companies is able to be used for finding the best management according to the necessities of the organisations. However, optimisation problems imply a high computational complexity, increased by the current necessity to include a massive quantity of data (Big Data), for the creation of optimisation problems to customise products and services for their clients. The irruption of Big Data technologies can be a challenge but also an important mechanism to tackle the computational difficulties of optimisation problems, and the possibility to distribute the problem performance. In this paper, we propose a solution that lets the query of a data set supported by Big Data technologies that imply the resolution of Constraint Optimisation Problem (COP). This proposal enables to: (1) model COPs whose input data are obtained from distributed and heterogeneous data; (2) facilitate the integration of different data sources to create the COPs; and, (3) solve the optimisation problems in a distributed way, to improve the performance. It is done by means of a framework and supported by a tool capable of modelling, solving and querying the results of optimisation problems. The tool integrates the Big Data technologies and commercial solvers of constraint programming. The suitability of the proposal and the development have been evaluated with real data sets whose computational study and results are included and discussed.}
}
@article{KUN2019556,
title = {Application of Big Data Technology in Scientific Research Data Management of Military Enterprises},
journal = {Procedia Computer Science},
volume = {147},
pages = {556-561},
year = {2019},
note = {2018 International Conference on Identification, Information and Knowledge in the Internet of Things},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.221},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919302406},
author = {Wang Kun and Liu Tong and Xie Xiaodan},
keywords = {big data technology, scientific research data, data analysis, decision},
abstract = {Scientific research data has an important strategic position for the development of enterprises and countries, and is an important basis for management to conduct strategic research and decision-making. Compared with the Internet industry, big data technology started late in the military enterprises, while military enterprises research data often has the characteristics of decentralization, low relevance, and diverse data types. It cannot fully utilize the advantages of data resources to enhance the core competitiveness of enterprises. To this end, this paper deeply explores the application methods of big data technology in military scientific research data management, and lays a foundation for the construction of scientific research big data platform.}
}
@article{HONG2018175,
title = {Big Data in Health Care: Applications and Challenges},
journal = {Data and Information Management},
volume = {2},
number = {3},
pages = {175-197},
year = {2018},
issn = {2543-9251},
doi = {https://doi.org/10.2478/dim-2018-0014},
url = {https://www.sciencedirect.com/science/article/pii/S2543925122000791},
author = {Liang Hong and Mengqi Luo and Ruixue Wang and Peixin Lu and Wei Lu and Long Lu},
keywords = {Big Data, public health, cloud computing, medical applications},
abstract = {The concept of Big Data is popular in a variety of domains. The purpose of this review was to summarize the features, applications, analysis approaches, and challenges of Big Data in health care. Big Data in health care has its own features, such as heterogeneity, incompleteness, timeliness and longevity, privacy, and ownership. These features bring a series of challenges for data storage, mining, and sharing to promote health-related research. To deal with these challenges, analysis approaches focusing on Big Data in health care need to be developed and laws and regulations for making use of Big Data in health care need to be enacted. From a patient perspective, application of Big Data analysis could bring about improved treatment and lower costs. In addition to patients, government, hospitals, and research institutions could also benefit from the Big Data in health care.}
}
@article{CARPIOPINEDO2020102859,
title = {Consumption and symbolic capital in the metropolitan space: Integrating ‘old’ retail data sources with social big data},
journal = {Cities},
volume = {106},
pages = {102859},
year = {2020},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2020.102859},
url = {https://www.sciencedirect.com/science/article/pii/S0264275120312075},
author = {José Carpio-Pinedo and Javier Gutiérrez},
keywords = {Commercial space, Retail, Retail geography, Symbolic capital, Big data, Foursquare, Madrid},
abstract = {While commerce is one of the key activities in cities, its spatial description still requires further attention, especially by considering the different dimensions of commercial space: physical, economic and socio-symbolic. The latter is becoming more and more important in an era where consumption is at the centre of social relations. Further, although data availability has been an enduring obstacle in commercial research, we are witnessing the advent of new data sources, and social-network big data is an opportunity to unveil the places to which consumers attribute prestige or symbolic capital, at the extent of entire metropolitan areas. This paper compares the physical, economic and socio-symbolic dimensions of commercial spaces through the analysis of three different commercial data sources: cadastral micro-data, business register and social-network big data. For the case of Madrid Metropolitan Area, the three databases are compared with correlation analysis and density maps, coming out as partly redundant and partly complementary. Getis-Ord's hotspot statistics integrated into a cluster analysis enable a comprehensive understanding of commercial environments, enriching previous spatial hierarchies. The spatial distribution of symbolic capital unveils a relation with socio-spatial segregation and paves the way to new reflections on the spatiality of consumption as a social practice.}
}
@article{JANSSEN2017338,
title = {Factors influencing big data decision-making quality},
journal = {Journal of Business Research},
volume = {70},
pages = {338-345},
year = {2017},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2016.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0148296316304945},
author = {Marijn Janssen and Haiko {van der Voort} and Agung Wahyudi},
keywords = {Big data, Big data analytics, Big data chain, E-government, Governance, Decision-making, Decision-making quality},
abstract = {Organizations are looking for ways to harness the power of big data (BD) to improve their decision making. Despite its significance the effects of BD on decision-making quality has been given scant attention in the literature. In this paper factors influencing decision-making based on BD are identified using a case study. BD is collected from different sources that have various data qualities and are processed by various organizational entities resulting in the creation of a big data chain. The veracity (manipulation, noise), variety (heterogeneity of data) and velocity (constantly changing data sources) amplified by the size of big data calls for relational and contractual governance mechanisms to ensure BD quality and being able to contextualize data. The case study reveals that taking advantage of big data is an evolutionary process in which the gradually understanding of the potential of big data and the routinization of processes plays a crucial role.}
}
@article{SBAI2020938,
title = {A real-time Decision Support System for Big Data Analytic: A case of Dynamic Vehicle Routing Problems},
journal = {Procedia Computer Science},
volume = {176},
pages = {938-947},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.089},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319876},
author = {Ines Sbai and Saoussen Krichen},
keywords = {Big data analytic, Decision Support System, DVRP, S-GA, Spark},
abstract = {Recently, the explosion of large amounts of traffic data has guided data scientists to create models with big data for a better decision-making. Big Data applications process and analyze this huge amounts of data (collected from a variety of heterogeneous data sources) that cannot be processed with traditional technologies. In this paper, Big Data frameworks are used for solving an optimization problem known as Dynamic Vehicle Routing Problem (DVRP). Hence, due to the NP-Hardness of the problem and to deal with a large size of data, we develop a parallel Spark Genetic Algorithm named (S-GA). This parallelism aims to take the advantage of Spark’s in-memory computing ability (as a master-slave distribution computing) and GA’s iterations operations. Parallel operations were used for fitness evaluation and genetic operations. Based on the parallel S-GA a decision support system is developed for the DVRP in order to generate the best routes. The experiments show that our proposed architecture is improved due to its capacity when coping with Big Data optimization problems by interconnecting components and deploying on different nodes of a cluster.}
}
@article{BINDER2020307,
title = {Big Data Management Using Ontologies for CPQ Solutions},
journal = {Procedia Manufacturing},
volume = {52},
pages = {307-312},
year = {2020},
note = {System-Integrated Intelligence – Intelligent, Flexible and Connected Systems in Products and ProductionProceedings of the 5th International Conference on System-Integrated Intelligence (SysInt 2020), Bremen, Germany},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.11.051},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920321958},
author = {Alexander Binder and Eva-Maria Iwer and Werner Quint},
keywords = {CPQ, Semantic Technologies, Ontologies, Ontology Matching, Data Quality},
abstract = {In recent years, due to a progressive complexity of handling and processing business data, proper big data management has become a challenge, especially for SMEs that have limited resources for investing in the requested business transformation process. As a solution, we suggest an ontology-based CPQ software approach, where we show how the implementation of semantic technologies and ontologies affects data integration processes. We also propose a method called "ontology-based data matching", which allows the semiautomatic generation of alignments used to formalize the coherence between ontologies. The proposed method will ensure consistency during integration, significantly improving the productivity of enterprises.}
}
@article{WILKIN2020100470,
title = {Big data prioritization in SCM decision-making: Its role and performance implications},
journal = {International Journal of Accounting Information Systems},
volume = {38},
pages = {100470},
year = {2020},
note = {2019 UW CISA Symposium},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2020.100470},
url = {https://www.sciencedirect.com/science/article/pii/S1467089520300385},
author = {Carla Wilkin and Aldónio Ferreira and Kristian Rotaru and Luigi Red Gaerlan},
keywords = {Big data, Big data availability, Big data prioritization, Supply chain management, Performance},
abstract = {Given exponential growth in the size of big data, its multi-channel sources and variability in quality that create challenges concerning cost-effective use, firms have invested significantly in databases and analytical tools to inform decision-making. In this regard, one means to avoid the costs associated with producing less than insightful reports and negative effects on performance through wasted resources is prioritizing data in terms of relevance and quality. The aim of this study is to investigate this approach by developing and testing a scale to evaluate Big Data Availability and the role of Big Data Prioritization for more effective use of big data in decision-making and performance. Focusing on the context of supply chain management (SCM), we validate this scale through a survey involving 84 managers. Findings support a positive association between Big Data Availability and its use in SCM decision-making, and suggest that Big Data Prioritization, as conceptualized in the study, has a positive impact on the use of big data in SCM decision-making and SCM performance. Through developing a scale to evaluate association between Big Data Availability and use in SCM decision-making, we make an empirical contribution to value generation from big data.}
}
@article{HADJSASSI2019534,
title = {A New Architecture for Cognitive Internet of Things and Big Data},
journal = {Procedia Computer Science},
volume = {159},
pages = {534-543},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.208},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919313924},
author = {Mohamed Saifeddine {Hadj Sassi} and Faiza Ghozzi Jedidi and Lamia Chaari Fourati},
keywords = {Internet of Things, Big-Data, Architecture, Cognitive, Data-flow},
abstract = {Big data and the Internet of Things (IoT) are considered as the main paradigms when defining new information architecture projects. Accordingly, technologies that make up these solutions could have an important role to play in business information architecture. Solutions that have approached big data and the IoT as unique technology initiatives, struggle in finding value in such efforts and in the technology itself. A connection to the requirements (volume, velocity, and variety) is mandatory to reach the potential business goals. In this context, we propose a new architecture for Cognitive Internet of Things (CIoT) and big data. The proposed architecture benefits computing mechanisms by combining the data WareHouse (DWH) and Data Lake (DL), and defining a tool for heterogeneous data collection.}
}
@article{REN20191343,
title = {A comprehensive review of big data analytics throughout product lifecycle to support sustainable smart manufacturing: A framework, challenges and future research directions},
journal = {Journal of Cleaner Production},
volume = {210},
pages = {1343-1365},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618334255},
author = {Shan Ren and Yingfeng Zhang and Yang Liu and Tomohiko Sakao and Donald Huisingh and Cecilia M.V.B. Almeida},
keywords = {Big data analytics, Smart manufacturing, Servitization, Sustainable production, Conceptual framework, Product lifecycle},
abstract = {Smart manufacturing has received increased attention from academia and industry in recent years, as it provides competitive advantage for manufacturing companies making industry more efficient and sustainable. As one of the most important technologies for smart manufacturing, big data analytics can uncover hidden knowledge and other useful information like relations between lifecycle decisions and process parameters helping industrial leaders to make more-informed business decisions in complex management environments. However, according to the literature, big data analytics and smart manufacturing were individually researched in academia and industry. To provide theoretical foundations for the research community to further develop scientific insights in applying big data analytics to smart manufacturing, it is necessary to summarize the existing research progress and weakness. In this paper, through combining the key technologies of smart manufacturing and the idea of ubiquitous servitization in the whole lifecycle, the term of sustainable smart manufacturing was coined. A comprehensive overview of big data in smart manufacturing was conducted, and a conceptual framework was proposed from the perspective of product lifecycle. The proposed framework allows analyzing potential applications and key advantages, and the discussion of current challenges and future research directions provides valuable insights for academia and industry.}
}
@article{KHALAJZADEH2020100964,
title = {An end-to-end model-based approach to support big data analytics development},
journal = {Journal of Computer Languages},
volume = {58},
pages = {100964},
year = {2020},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2020.100964},
url = {https://www.sciencedirect.com/science/article/pii/S2590118420300241},
author = {Hourieh Khalajzadeh and Andrew J. Simmons and Mohamed Abdelrazek and John Grundy and John Hosking and Qiang He},
keywords = {Big data analytics, Big data modeling, Big data toolkits, Domain-specific visual languages, Multidisciplinary teams, End-user tools},
abstract = {We present BiDaML 2.0, an integrated suite of visual languages and supporting tool to help multidisciplinary teams with the design of big data analytics solutions. BiDaML tool support provides a platform for efficiently producing BiDaML diagrams and facilitating their design, creation, report and code generation. We evaluated BiDaML using two types of evaluations, a theoretical analysis using the “physics of notations”, and an empirical study with 1) a group of 12 target end-users and 2) five individual end-users. Participants mostly agreed that BiDaML was straightforward to understand/learn, and prefer BiDaML for supporting complex data analytics solution modeling than other modeling languages.}
}
@article{ARDAGNA2018548,
title = {Context-aware data quality assessment for big data},
journal = {Future Generation Computer Systems},
volume = {89},
pages = {548-562},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17329151},
author = {Danilo Ardagna and Cinzia Cappiello and Walter Samá and Monica Vitali},
keywords = {Data quality, Big data, Context-awareness, Data profiling, DQ assessment},
abstract = {Big data changed the way in which we collect and analyze data. In particular, the amount of available information is constantly growing and organizations rely more and more on data analysis in order to achieve their competitive advantage. However, such amount of data can create a real value only if combined with quality: good decisions and actions are the results of correct, reliable and complete data. In such a scenario, methods and techniques for the Data Quality assessment can support the identification of suitable data to process. If for traditional database numerous assessment methods are proposed, in the Big Data scenario new algorithms have to be designed in order to deal with novel requirements related to variety, volume and velocity issues. In particular, in this paper we highlight that dealing with heterogeneous sources requires an adaptive approach able to trigger the suitable quality assessment methods on the basis of the data type and context in which data have to be used. Furthermore, we show that in some situations it is not possible to evaluate the quality of the entire dataset due to performance and time constraints. For this reason, we suggest to focus the Data Quality assessment only on a portion of the dataset and to take into account the consequent loss of accuracy by introducing a confidence factor as a measure of the reliability of the quality assessment procedure. We propose a methodology to build a Data Quality adapter module, which selects the best configuration for the Data Quality assessment based on the user main requirements: time minimization, confidence maximization, and budget minimization. Experiments are performed by considering real data gathered from a smart city case study.}
}
@article{PAL2020100869,
title = {Big data in biology: The hope and present-day challenges in it},
journal = {Gene Reports},
volume = {21},
pages = {100869},
year = {2020},
issn = {2452-0144},
doi = {https://doi.org/10.1016/j.genrep.2020.100869},
url = {https://www.sciencedirect.com/science/article/pii/S2452014420302831},
author = {Subhajit Pal and Sudip Mondal and Gourab Das and Sunirmal Khatua and Zhumur Ghosh},
keywords = {Big data, Cloud computing, Bioinformatics, High throughput data, MapReduce, Machine learning},
abstract = {The wave of new technologies has opened up the opportunity for cost-effective generation of high-throughput profiles of biological systems. This is generating tons of biological data. It is thus leading us towards the “big data” era which is creating a pressing need to bridge the gap between high-throughput technological development and our ability for managing, analyzing, and integrating the biological big data. To harness the maximum out of it, sufficient expertise needs to be developed for big data management and analysis. In this review, we discuss the challenges related to storage, transfer, access and analysis of unstructured and structured biological big data. Subsequently, it provides a comprehensive summary regarding the important strategies adopted for biological big data management which includes a discussion on all the recently used tools or software built for high throughput processing and analysis of biological big data. Finally it discusses the future perspectives of big data bioinformatics.}
}
@article{LIN2019197,
title = {Data source selection for information integration in big data era},
journal = {Information Sciences},
volume = {479},
pages = {197-213},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.11.029},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518309162},
author = {Yiming Lin and Hongzhi Wang and Jianzhong Li and Hong Gao},
keywords = {Source selection, Data integration, Data cleaning},
abstract = {In big data era, information integration often requires abundant data extracted from massive data sources. Due to a large number of data sources, data source selection plays a crucial role in information integration, since it is costly and even impossible to access all data sources. Data Source selection should consider both efficiency and effectiveness issues. For efficiency, the approach should scale to large data source amount. From effectiveness aspect, data quality and overlapping of sources are to be considered. In this paper, we study source selection problem in Big Data and propose methods which can scale to datasets with up to millions of data sources and guarantee the quality of results. Motivated by this, we propose a new metric taking the expected number of true values a source can provide as a criteria to evaluate the contribution of a data source. Based on our proposed index, we present a scalable algorithm and two pruning strategies to improve the efficiency without sacrificing precision. Experimental results on both real world and synthetic data sets show that our methods can select sources providing a large proportion of true values efficiently and can scale to massive data sources.}
}
@article{ALAOUI2019803,
title = {The Impact of Big Data Quality on Sentiment Analysis Approaches},
journal = {Procedia Computer Science},
volume = {160},
pages = {803-810},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919317077},
author = {Imane El Alaoui and Youssef Gahi},
keywords = {Big Data Quality Metrics, Big Data Value Chain, Big Data, Big Social Data, Sentiment Analysis, Opinion Mining},
abstract = {Human beings share their good or bad opinions about subjects, products, and services through internet and social networks. The ability to effectively analyze this kind of information is now seen as a key competitive advantage to better inform decisions. In order to do so, organizations employ Sentiment Analysis (SA) techniques on these data. However, the usage of social media around the world is ever-increasing, which considerably accelerates massive data generation and makes traditional SA systems unable to deliver useful insights. Such volume of data can be efficiently analyzed using the combination of SA techniques and Big Data technologies. In fact, big data is not a luxury but an essential necessary to make valuable predictions. However, there are some challenges associated with big data such as quality that could highly affect the SA systems’ accuracy that use huge volume of data. Thus, the quality aspect should be addressed in order to build reliable and credible systems. For this, the goal of our research work is to consider Big Data Quality Metrics (BDQM) in SA that rely of big data. In this paper, we first highlight the most eloquent BDQM that should be considered throughout the Big Data Value Chain (BDVC) in any big data project. Then, we measure the impact of BDQM on a novel SA method accuracy in a real case study by giving simulation results.}
}
@article{NASHIPUDIMATH2020100033,
title = {An efficient integration and indexing method based on feature patterns and semantic analysis for big data},
journal = {Array},
volume = {7},
pages = {100033},
year = {2020},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2020.100033},
url = {https://www.sciencedirect.com/science/article/pii/S2590005620300187},
author = {Madhu Mahesh Nashipudimath and Subhash K. Shinde and Jayshree Jain},
keywords = {Big data, Integration, Feature patterns, Indexing, Semantic analysis},
abstract = {Big Data has received much attention in the multi-domain industry. In the digital and computing world, information is generated and collected at a rate that quickly exceeds the boundaries. The traditional data integration system interconnects the limited number of resources and is built with relatively stable and generally complex and time-consuming design activities. However, the rapid growth of these large data sets creates difficulties in learning heterogeneous data structures for integration and indexing. It also creates difficulty in information retrieval for the various data analysis requirements. In this paper, a probabilistic feature Patterns (PFP) approach using feature transformation and selection method is proposed for efficient data integration and utilizing the features latent semantic analysis (F-LSA) method for indexing the unsupervised multiple heterogeneous integrated cluster data sources. The PFP approach takes the advantage of the features transformation and selection mechanism to map and cluster the data for the integration, and an analysis of the data features context relation using LSA to provide the appropriate index for fast and accurate data extraction. A huge volume of BibText dataset from different publication sources are processed to evaluated to understand the effectiveness of the proposal. The analytical study and the outcome results show the improvisation in integration and indexing of the work.}
}
@article{ZHAO20201624,
title = {Advancing computer-aided drug discovery (CADD) by big data and data-driven machine learning modeling},
journal = {Drug Discovery Today},
volume = {25},
number = {9},
pages = {1624-1638},
year = {2020},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2020.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1359644620302646},
author = {Linlin Zhao and Heather L. Ciallella and Lauren M. Aleksunes and Hao Zhu},
abstract = {Advancing a new drug to market requires substantial investments in time as well as financial resources. Crucial bioactivities for drug candidates, including their efficacy, pharmacokinetics (PK), and adverse effects, need to be investigated during drug development. With advancements in chemical synthesis and biological screening technologies over the past decade, a large amount of biological data points for millions of small molecules have been generated and are stored in various databases. These accumulated data, combined with new machine learning (ML) approaches, such as deep learning, have shown great potential to provide insights into relevant chemical structures to predict in vitro, in vivo, and clinical outcomes, thereby advancing drug discovery and development in the big data era.}
}
@article{VANDERVOORT201927,
title = {Rationality and politics of algorithms. Will the promise of big data survive the dynamics of public decision making?},
journal = {Government Information Quarterly},
volume = {36},
number = {1},
pages = {27-38},
year = {2019},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X17304951},
author = {H.G. {van der Voort} and A.J. Klievink and M. Arnaboldi and A.J. Meijer},
abstract = {Big data promises to transform public decision-making for the better by making it more responsive to actual needs and policy effects. However, much recent work on big data in public decision-making assumes a rational view of decision-making, which has been much criticized in the public administration debate. In this paper, we apply this view, and a more political one, to the context of big data and offer a qualitative study. We question the impact of big data on decision-making, realizing that big data – including its new methods and functions – must inevitably encounter existing political and managerial institutions. By studying two illustrative cases of big data use processes, we explore how these two worlds meet. Specifically, we look at the interaction between data analysts and decision makers. In this we distinguish between a rational view and a political view, and between an information logic and a decision logic. We find that big data provides ample opportunities for both analysts and decision makers to do a better job, but this doesn't necessarily imply better decision-making, because big data also provides opportunities for actors to pursue their own interests. Big data enables both data analysts and decision makers to act as autonomous agents rather than as links in a functional chain. Therefore, big data's impact cannot be interpreted only in terms of its functional promise; it must also be acknowledged as a phenomenon set to impact our policymaking institutions, including their legitimacy.}
}
@article{MIKALEF2020103169,
title = {Exploring the relationship between big data analytics capability and competitive performance: The mediating roles of dynamic and operational capabilities},
journal = {Information & Management},
volume = {57},
number = {2},
pages = {103169},
year = {2020},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0378720618301022},
author = {Patrick Mikalef and John Krogstie and Ilias O. Pappas and Paul Pavlou},
keywords = {Big data analytics, Dynamic capabilities, Operational capabilities, Business value, Resource-based view},
abstract = {A central question for information systems (IS) researchers and practitioners is if, and how, big data can help attain a competitive advantage. To address this question, this study draws on the resource-based view, dynamic capabilities view, and on recent literature on big data analytics, and examines the indirect relationship between a firm’s big data analytics capability (BDAC) and competitive performance. The study extends existing research by proposing that BDACs enable firms to generate insight that can help strengthen their dynamic capabilities, which, in turn, positively impact marketing and technological capabilities. To test our proposed research model, we used survey data from 202 chief information officers and IT managers working in Norwegian firms. By means of partial least squares structural equation modeling, results show that a strong BDAC can help firms build a competitive advantage. This effect is not direct but fully mediated by dynamic capabilities, which exerts a positive and significant effect on two types of operational capabilities: marketing and technological capabilities. The findings suggest that IS researchers should look beyond direct effects of big data investments and shift their attention on how a BDAC can be leveraged to enable and support organizational capabilities.}
}
@article{BALTI2020101136,
title = {A review of drought monitoring with big data: Issues, methods, challenges and research directions},
journal = {Ecological Informatics},
volume = {60},
pages = {101136},
year = {2020},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2020.101136},
url = {https://www.sciencedirect.com/science/article/pii/S1574954120300868},
author = {Hanen Balti and Ali {Ben Abbes} and Nedra Mellouli and Imed Riadh Farah and Yanfang Sang and Myriam Lamolle},
keywords = {Drought monitoring, Artificial intelligence, Big data, Machine learning, Statistical approach, Remote sensing},
abstract = {Over recent years, the frequency and intensity of droughts have increased and there has been a large drying trend over many parts of the world. Consequently, drought monitoring using big data analytic has gained an explosive interest. Droughts stand among the most damaging natural disasters. It threatens agricultural production, ecological environment, and socio-economic development. For this reason, early warning, accurate evaluation, and efficient prediction are an emergency especially for the nations that are the most menaced by this danger. There are numerous emerging studies addressing big data and its applications in drought monitoring. In fact, big data handle data heterogeneity which is an additive value for the prediction of drought, it offers a view of the different dimensions such as the spatial distribution, the temporal distribution and the severity detection of this phenomenon. Big data analytic and drought are introduced and reviewed in this paper. Besides, this review includes different studies, researches and applications of big data to drought monitoring. Challenges related to data life cycle such as data challenges, data processing challenges and data infrastructure management challenges are also discussed. Finally, we conclude that big data analytic can be beneficial in drought monitoring but there is a need for statistical and artificial intelligence-based approaches.}
}
@article{LV2020101937,
title = {Achieving secure big data collection based on trust evaluation and true data discovery},
journal = {Computers & Security},
volume = {96},
pages = {101937},
year = {2020},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.101937},
url = {https://www.sciencedirect.com/science/article/pii/S0167404820302133},
author = {Denglong Lv and Shibing Zhu},
keywords = {Big data collection, Trust evaluation, Trust model, True data discovery, Wireless sensor network},
abstract = {Data collection is an important process in the life cycle of big data processing. It is the key part that must be completed first in all kinds of data applications, which determines the results of data analysis and application service quality. However, untrusted data sources and transmission links expose the data collection process to attacks and malicious threats such as counterfeiting, replay, and denial of service, and ultimately lead to untrustworthy data. In order to cope with the threat of data collection process and ensure data quality, this paper proposes trust evaluation scheme for data security collection based on wireless sensor network, one of the data collection applications, including direct trust, recommendation trust, link trust, and backhaul trust. Meanwhile, in order to realize the dynamic update of the trust of the data sources, a true data discovery and trust dynamic update mechanism based on ω-FCM (Weight Fuzzy C-Mean) algorithm is proposed. The results of a large number of simulation experiments show that the proposed scheme, model and algorithm can effectively evaluate the trust of data sources and ensure the authenticity of the collected data.}
}
@article{MOHARM2019100945,
title = {State of the art in big data applications in microgrid: A review},
journal = {Advanced Engineering Informatics},
volume = {42},
pages = {100945},
year = {2019},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2019.100945},
url = {https://www.sciencedirect.com/science/article/pii/S147403461830702X},
author = {Karim Moharm},
keywords = {Big data, Microgrid},
abstract = {The prospering Big data era is emerging in the power grid. Multiple world-wide studies are emphasizing the big data applications in the microgrid due to the huge amount of produced data. Big data analytics can impact the design and applications towards safer, better, more profitable, and effective power grid. This paper presents the recognition and challenges of the big data and the microgrid. The construction of big data analytics is introduced. The data sources, big data opportunities, and enhancement areas in the microgrid like stability improvement, asset management, renewable energy prediction, and decision-making support are summarized. Diverse case studies are presented including different planning, operation control, decision making, load forecasting, data attacks detection, and maintenance aspects of the microgrid. Finally, the open challenges of big data in the microgrid are discussed.}
}
@article{HUGHES2020120300,
title = {Sowing the seeds of value? Persuasive practices and the embedding of big data analytics},
journal = {Technological Forecasting and Social Change},
volume = {161},
pages = {120300},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120300},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520311264},
author = {Jeffrey Hughes and Kirstie Ball},
keywords = {Big data analytics, Persuasion, Practice, Capabilities, Value},
abstract = {This paper draws on data from three organisational case studies and expert interviews to propose that persuasive practices are the precursors and enablers of analytical capability development. A bundle of seven practices was identified and observed to bridge multiple gaps between technical and non-technical colleagues on big data analytics (BDA) projects. The deployment of these practices varied according to the level of BDA maturity and featured a host of socio-material elements. This paper complements existing technical case studies with a fine-grained qualitative account of the managerial and human elements of BDA implementation. Effective deployment of persuasive practices potentially both embeds the benefits and mitigates the risks of BDA, sowing the seeds of many different forms of value.}
}
@article{SU2020138984,
title = {Carbon emissions and environmental management based on Big Data and Streaming Data: A bibliometric analysis},
journal = {Science of The Total Environment},
volume = {733},
pages = {138984},
year = {2020},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.138984},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720325018},
author = {Yuan Su and Yanni Yu and Ning Zhang},
keywords = {Big data, Streaming data, Carbon emission, Environmental management, Bibliometric analysis, Net-work analysis},
abstract = {Climate change and environmental management are issues of global concern. The advent of the era of Big Data has created a new research platform for the assessment of environmental governance and policies. However, little is known about Big Data application to climate change and environmental management research. This paper adopts bibliometric analysis in conjunction with network analysis to systematically evaluate the publications on carbon emissions and environmental management based on Big Data and Streaming Data using R package and VOSviewer software. The analysis involves 274 articles after rigorous screening and includes citation analysis, co-citation analysis, and co-word analysis. Main findings include (1) Carbon emissions and environmental management based on big data and streaming data is an emerging multidisciplinary research topic, which has been applied in the fields of computer science, supply chain design, transportation, carbon price assessment, environmental policy evaluation, and CO2 emissions reduction. (2) This field has attracted the attention of nations which are major contributors to the world economy. In particular, European and American scholars have made the main contributions to this topic, and Chinese researchers have also had great impact. (3) The research content of this topic is primarily divided into four categories, including empirical studies of specific industries, air pollution governance, technological innovation, and low-carbon transportation. Our findings suggest that future research should bring greater depth of practical and modeling analysis to environmental policy assessment based on Big Data.}
}
@article{HAJLI2020135,
title = {Understanding market agility for new product success with big data analytics},
journal = {Industrial Marketing Management},
volume = {86},
pages = {135-143},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2019.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0019850118304735},
author = {Nick Hajli and Mina Tajvidi and Ayantunji Gbadamosi and Waqar Nadeem},
keywords = {Big data analytics, Customer agility, Effective use of data, New product success},
abstract = {The complexity that characterises the dynamic nature of the various environmental factors makes it very compelling for firms to be capable of addressing the changing customers' needs. The current study examines the role of big data in new product success. We develop a qualitative research with case study approach to look at this. Specifically, we look at multiple cases to get in-depth understanding of customer agility for new product success with big data analytics. The findings of the study provide insight into the role of customer agility in new product success. This study unpacks the interconnectedness of the effective use of data aggregation tools, the effectiveness of data analysis tools and customer agility. It also explores the link between all of these factors and new product success. The study is reasonably telling in that it shows that the effective use of data aggregation and data analysis tools results in customer agility which in itself explains how an organisation senses and responds speedily to opportunities for innovation in the competitive marketing environment. The current study provides significant theoretical contributions by providing evidence for the role of big data analytics, big data aggregation tools, customer agility, organisational slack and environmental turbulence in new product success.}
}
@article{NEILSON201935,
title = {Systematic Review of the Literature on Big Data in the Transportation Domain: Concepts and Applications},
journal = {Big Data Research},
volume = {17},
pages = {35-44},
year = {2019},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214579617303866},
author = {Alex Neilson and  Indratmo and Ben Daniel and Stevanus Tjandra},
keywords = {Big Data, Smart city, Intelligent transportation system, Connected vehicle, Road traffic safety, Vision Zero},
abstract = {Research in Big Data and analytics offers tremendous opportunities to utilize evidence in making decisions in many application domains. To what extent can the paradigms of Big Data and analytics be used in the domain of transport? This article reports on an outcome of a systematic review of published articles in the last five years that discuss Big Data concepts and applications in the transportation domain. The goal is to explore and understand the current research, opportunities, and challenges relating to the utilization of Big Data and analytics in transportation. The review shows the potential of Big Data and analytics to garner insights and improve transportation systems through the analysis of various forms of data obtained from traffic monitoring systems, connected vehicles, crowdsourcing, and social media. We discuss some platforms and software architecture for the transport domain, along with a wide array of storage, processing, and analytical techniques, and describe challenges associated with the implementation of Big Data and analytics. This review contributes broadly to the various ways in which cities can utilize Big Data in transportation to guide the creation of sustainable and safer traffic systems. Since research in Big Data and transportation is, by and large, at infancy, this article does not prescribe recommendations to the various challenges identified, which also constitutes the limitation of the article.}
}
@article{YADEGARIDEHKORDI2018199,
title = {Influence of big data adoption on manufacturing companies' performance: An integrated DEMATEL-ANFIS approach},
journal = {Technological Forecasting and Social Change},
volume = {137},
pages = {199-210},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.07.043},
url = {https://www.sciencedirect.com/science/article/pii/S0040162518304141},
author = {Elaheh Yadegaridehkordi and Mehdi Hourmand and Mehrbakhsh Nilashi and Liyana Shuib and Ali Ahani and Othman Ibrahim},
keywords = {Big data, Firm performance, Manufacturing companies, DEMATEL, ANFIS},
abstract = {Big Data is one of the recent technological advances with the strong applicability in almost every industry, including manufacturing. However, despite business opportunities offered by this technology, its adoption is still in early stage in many industries. Thus, this study aimed to identify and rank the significant factors influencing adoption of big data and in turn to predict the influence of big data adoption on manufacturing companies' performance using a hybrid approach of decision-making trial and evaluation laboratory (DEMATEL)- adaptive neuro-fuzzy inference systems (ANFIS). This study identified the critical adoption factors from literature review and categorized them into technological, organizational and environmental dimensions. Data was collected from 234 industrial managers who were involved in the decision-making process regarding IT procurement in Malaysian manufacturing companies. Research results showed that technological factors (perceived benefits, complexity, technology resources, big data quality and integration) have the highest influence on the big data adoption and firms' performance. This study is one of the pioneers in using DEMATEL-ANFIS approach in the big data adoption context. In addition to the academic contribution, findings of this study can hopefully assist manufacturing industries, big data service providers, and governments to precisely focus on vital factors found in this study in order to improve firm performance by adopting big data.}
}
@incollection{DAS2020127,
title = {Chapter 7 - A Framework Development on Big Data Analytics for Terahertz Healthcare},
editor = {Amit Banerjee and Basabi Chakraborty and Hiroshi Inokawa and Jitendra {Nath Roy}},
booktitle = {Terahertz Biomedical and Healthcare Technologies},
publisher = {Elsevier},
pages = {127-143},
year = {2020},
isbn = {978-0-12-818556-8},
doi = {https://doi.org/10.1016/B978-0-12-818556-8.00007-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128185568000070},
author = {Debashis Das and Chinmay Chakraborty and Sourav Banerjee},
keywords = {3D imaging, Big data, Genomic expression, Healthcare technology, Medical sensors, Personal medical information, Radiology images},
abstract = {This chapter is mainly focused on the development of big data analytics in terahertz healthcare technology. In today's world, “big data” is a very familiar term, but the way it is interpreted is modified day by day. Healthcare is one aspect in which big data can be utilized to improve the overall system of healthcare, as, in the context of healthcare, the three primary “V's” of big data definition, volume, variety, and velocity, are very well suited. According to big data analysts, error-free analysis and outcome are ensured from big data, but this is really difficult for medical data due to the issues regarding the data quality. The final V related to big data is value, i.e., how much leverage the data can provide. Thus we can conclude that healthcare is a much preferred area for expert big data analysis. But there are several challenges to be faced in every aspect, starting from data collection to storage, analysis, prediction, etc. The main challenge is the unstructured nature of the data and the organizations from where the data are collected, following no standardized rules, which forms a big gap in the processing of information. Also, a huge investment is required for resources such as high-level expertise, knowledge, technologies used for data analytics, common data warehouses (for obtaining homogeneous data), etc. Despite having so many obstructions, big data analytics has already started growing rapidly in the healthcare sector.}
}
@article{TSAI2019306,
title = {Big Data in Cancer Research: Real-World Resources for Precision Oncology to Improve Cancer Care Delivery},
journal = {Seminars in Radiation Oncology},
volume = {29},
number = {4},
pages = {306-310},
year = {2019},
note = {Big Data in Radiation Oncology},
issn = {1053-4296},
doi = {https://doi.org/10.1016/j.semradonc.2019.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1053429619300347},
author = {Chiaojung Jillian Tsai and Nadeem Riaz and Scarlett Lin Gomez},
abstract = {In oncology, the term “big data” broadly describes the rapid acquisition and generation of massive amounts of information, typically from population cancer registries, electronic health records, or large-scale genetic sequencing studies. The challenge of using big data in cancer research lies in interdisciplinary collaboration and information processing to unify diverse data sources and provide valid analytics to harness meaningful information. This article provides an overview of how big data approaches can be applied in cancer research, and how they can be used to translate information into new ways to ultimately make informed decisions that improve cancer care and delivery.}
}
@article{ATITALLAH2020100303,
title = {Leveraging Deep Learning and IoT big data analytics to support the smart cities development: Review and future directions},
journal = {Computer Science Review},
volume = {38},
pages = {100303},
year = {2020},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2020.100303},
url = {https://www.sciencedirect.com/science/article/pii/S1574013720304032},
author = {Safa Ben Atitallah and Maha Driss and Wadii Boulila and Henda Ben Ghézala},
keywords = {Internet of Things, Deep Learning, Smart city, Big data analytics, Review},
abstract = {The rapid growth of urban populations worldwide imposes new challenges on citizens’ daily lives, including environmental pollution, public security, road congestion, etc. New technologies have been developed to manage this rapid growth by developing smarter cities. Integrating the Internet of Things (IoT) in citizens’ lives enables the innovation of new intelligent services and applications that serve sectors around the city, including healthcare, surveillance, agriculture, etc. IoT devices and sensors generate large amounts of data that can be analyzed to gain valuable information and insights that help to enhance citizens’ quality of life. Deep Learning (DL), a new area of Artificial Intelligence (AI), has recently demonstrated the potential for increasing the efficiency and performance of IoT big data analytics. In this survey, we provide a review of the literature regarding the use of IoT and DL to develop smart cities. We begin by defining the IoT and listing the characteristics of IoT-generated big data. Then, we present the different computing infrastructures used for IoT big data analytics, which include cloud, fog, and edge computing. After that, we survey popular DL models and review the recent research that employs both IoT and DL to develop smart applications and services for smart cities. Finally, we outline the current challenges and issues faced during the development of smart city services.}
}
@article{SHAMIM2020120315,
title = {Big data analytics capability and decision making performance in emerging market firms: The role of contractual and relational governance mechanisms},
journal = {Technological Forecasting and Social Change},
volume = {161},
pages = {120315},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120315},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520311410},
author = {Saqib Shamim and Jing Zeng and Zaheer Khan and Najam Ul Zia},
keywords = {Big data, Contractual governance, Relational governance, Big data analytics capability, Culture, Decision-making performance, Emerging markets},
abstract = {This study examines the role of big data contractual and relational governance in big data decision-making performance of firms based in China. It investigates the mediation of big data analytics (BDA) capability in the association of contractual and relational governance with decision-making performance. Furthermore, moderating role of data-driven culture in the relationship of BDA capability and decision-making performance is examined. Data are collected from 108 Chinese firms engaged in big data-related activities. Structural equation modeling is employed to test the hypotheses. This study contributes towards the literature on big data management and governance mechanisms, by establishing the relationship of decision-making performance with big data contractual and relational governance directly and through the mediation of BDA capabilities. It also contributes towards knowledge based dynamic capabilities (KBDCs) view of firms, arguing that dynamic capabilities such as BDA capabilities can be influenced through knowledge sources and activities. We add to the discussions on whether contractual and relational governance are alternatives or they complement each other, by establishing the moderating role of big data relational governance in the relationship of contractual governance and decision-making performance. Finally, we argue that social capital can enhance KBDCs through contractual and relational governance in big data context.}
}
@article{SHAMIM2019103135,
title = {Role of big data management in enhancing big data decision-making capability and quality among Chinese firms: A dynamic capabilities view},
journal = {Information & Management},
volume = {56},
number = {6},
pages = {103135},
year = {2019},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0378720618302854},
author = {Saqib Shamim and Jing Zeng and Syed Muhammad Shariq and Zaheer Khan},
keywords = {Big data management, Dynamic capabilities, Big data decision-making capability, Decision-making quality, China},
abstract = {This study examines the antecedents and influence of big data decision-making capabilities on decision-making quality among Chinese firms. We propose that such capabilities are influenced by big data management challenges such as leadership, talent management, technology, and organisational culture. By using primary data from 108 Chinese firms and utilising partial least squares, we tested the antecedents of big data decision-making capability and its impact on decision-making quality. Findings suggest that big data management challenges are the key antecedents of big data decision-making capability. Furthermore, the latter is vital for big data decision-making quality.}
}
@article{SILVA2020111,
title = {Ion beam analysis and big data: How data science can support next-generation instrumentation},
journal = {Nuclear Instruments and Methods in Physics Research Section B: Beam Interactions with Materials and Atoms},
volume = {478},
pages = {111-115},
year = {2020},
issn = {0168-583X},
doi = {https://doi.org/10.1016/j.nimb.2020.05.027},
url = {https://www.sciencedirect.com/science/article/pii/S0168583X2030272X},
author = {Tiago F. Silva and Cleber L. Rodrigues and Manfredo H. Tabacniks and Hugo D.C. Pereira and Thiago B. Saramela and Renato O. Guimarães},
keywords = {Ion beam analysis, Big data, Data quality assurance, Artificial intelligence},
abstract = {With a growing demand for accurate ion beam analysis on a large number of samples, it becomes an issue of how to ensure the quality standards and consistency over hundreds or thousands of samples. In this sense, a virtual assistant that checks the data quality, emitting certificates of quality, is highly desired. Even the processing of a massive number of spectra is a problem regarding the consistency of the analysis. In this work, we report the design and first results of a virtual layer under implementation in our laboratory. It consists of a series of systems running in the cloud that perform the mentioned tasks and serves as a virtual assistant for member staff and users. We aim to bring the concept of the Internet of Things and artificial intelligence closer to the laboratory to support a new generation of instrumentation.}
}
@article{KWON2014387,
title = {Data quality management, data usage experience and acquisition intention of big data analytics},
journal = {International Journal of Information Management},
volume = {34},
number = {3},
pages = {387-394},
year = {2014},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2014.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0268401214000127},
author = {Ohbyung Kwon and Namyeon Lee and Bongsik Shin},
keywords = {Big data analytics, Resource-based view, Data quality management, IT capability, Data usage},
abstract = {Big data analytics associated with database searching, mining, and analysis can be seen as an innovative IT capability that can improve firm performance. Even though some leading companies are actively adopting big data analytics to strengthen market competition and to open up new business opportunities, many firms are still in the early stage of the adoption curve due to lack of understanding of and experience with big data. Hence, it is interesting and timely to understand issues relevant to big data adoption. In this study, a research model is proposed to explain the acquisition intention of big data analytics mainly from the theoretical perspectives of data quality management and data usage experience. Our empirical investigation reveals that a firm's intention for big data analytics can be positively affected by its competence in maintaining the quality of corporate data. Moreover, a firm's favorable experience (i.e., benefit perceptions) in utilizing external source data could encourage future acquisition of big data analytics. Surprisingly, a firm's favorable experience (i.e., benefit perceptions) in utilizing internal source data could hamper its adoption intention for big data analytics.}
}
@article{LIU2016134,
title = {Rethinking big data: A review on the data quality and usage issues},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {115},
pages = {134-142},
year = {2016},
note = {Theme issue 'State-of-the-art in photogrammetry, remote sensing and spatial information science'},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2015.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0924271615002567},
author = {Jianzheng Liu and Jie Li and Weifeng Li and Jiansheng Wu},
keywords = {Big data, Data quality and error, Data ethnics, Spatial information sciences},
abstract = {The recent explosive publications of big data studies have well documented the rise of big data and its ongoing prevalence. Different types of “big data” have emerged and have greatly enriched spatial information sciences and related fields in terms of breadth and granularity. Studies that were difficult to conduct in the past time due to data availability can now be carried out. However, big data brings lots of “big errors” in data quality and data usage, which cannot be used as a substitute for sound research design and solid theories. We indicated and summarized the problems faced by current big data studies with regard to data collection, processing and analysis: inauthentic data collection, information incompleteness and noise of big data, unrepresentativeness, consistency and reliability, and ethical issues. Cases of empirical studies are provided as evidences for each problem. We propose that big data research should closely follow good scientific practice to provide reliable and scientific “stories”, as well as explore and develop techniques and methods to mitigate or rectify those ‘big-errors’ brought by big data.}
}
@article{LIN201949,
title = {Strategic orientations, developmental culture, and big data capability},
journal = {Journal of Business Research},
volume = {105},
pages = {49-60},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2019.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S0148296319304333},
author = {Canchu Lin and Anand Kunnathur},
keywords = {Big data capability, Customer orientation, Entrepreneurial orientation, Technology orientation, And developmental culture},
abstract = {Prior research articulated the importance of developing a big data analytics capability but did not show how to cultivate this development. Drawing on the literature on this topic, this study develops the concept of Big Data capability, which enhances our understanding of Big Data practice beyond that captured in previous literature on the concept of big data analytics capability. This study further highlights the strategic implications of the concept by testing its relationship to three strategic orientations and one aspect of organizational culture. Findings show that customer, entrepreneurial, and technology orientations, and developmental culture are important contributors to the development of Big Data capability.}
}
@article{YAO20181,
title = {Big data quality prediction in the process industry: A distributed parallel modeling framework},
journal = {Journal of Process Control},
volume = {68},
pages = {1-13},
year = {2018},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2018.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0959152418300660},
author = {Le Yao and Zhiqiang Ge},
keywords = {Distributed modeling, MapReduce framework, Parallel computing, Quality prediction, Big data analytics},
abstract = {With the ever increasing data collected from the process, the era of big data has arrived in the process industry. Therefore, the computational effort for data modeling and analytics in standalone modes has become increasingly demanding, particularly for large-scale processes. In this paper, a distributed parallel process modeling approach is presented based on a MapReduce framework for big data quality prediction. Firstly, the architecture for distributed parallel data modeling is formulated under the MapReduce framework. Secondly, a big data quality prediction scheme is developed based on the distributed parallel data modeling approach. As an example, the basic Semi-Supervised Probabilistic Principal Component Regression (SSPPCR) model is deployed to concurrently train a set of local models with split datasets. Meanwhile, Bayesian rule is utilized in a MapReduce way to integrate local models based on their predictive abilities. Two case studies demonstrate the effectiveness of the proposed method for big data quality prediction.}
}
@article{GARCIAGIL2019135,
title = {Enabling Smart Data: Noise filtering in Big Data classification},
journal = {Information Sciences},
volume = {479},
pages = {135-152},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518309460},
author = {Diego García-Gil and Julián Luengo and Salvador García and Francisco Herrera},
keywords = {Big Data, Smart Data, Classification, Class noise, Label noise.},
abstract = {In any knowledge discovery process the value of extracted knowledge is directly related to the quality of the data used. Big Data problems, generated by massive growth in the scale of data observed in recent years, also follow the same dictate. A common problem affecting data quality is the presence of noise, particularly in classification problems, where label noise refers to the incorrect labeling of training instances, and is known to be a very disruptive feature of data. However, in this Big Data era, the massive growth in the scale of the data poses a challenge to traditional proposals created to tackle noise, as they have difficulties coping with such a large amount of data. New algorithms need to be proposed to treat the noise in Big Data problems, providing high quality and clean data, also known as Smart Data. In this paper, two Big Data preprocessing approaches to remove noisy examples are proposed: an homogeneous ensemble and an heterogeneous ensemble filter, with special emphasis in their scalability and performance traits. The obtained results show that these proposals enable the practitioner to efficiently obtain a Smart Dataset from any Big Data classification problem.}
}
@article{CORTEREAL2020103141,
title = {Leveraging internet of things and big data analytics initiatives in European and American firms: Is data quality a way to extract business value?},
journal = {Information & Management},
volume = {57},
number = {1},
pages = {103141},
year = {2020},
note = {Big data and business analytics: A research agenda for realizing business value},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617308662},
author = {Nadine Côrte-Real and Pedro Ruivo and Tiago Oliveira},
keywords = {Big data analytics, Internet of things, Strategic management, Knowledge-based theory, Dynamics capability theory},
abstract = {Big data analytics (BDA) and the Internet of Things (IoT) tools are considered crucial investments for firms to distinguish themselves among competitors. Drawing on a strategic management perspective, this study proposes that BDA and IoT capabilities can create significant value in business processes if supported by a good level of data quality, which will lead to a better competitive advantage. Responses are collected from 618 European and American firms that use IoT and BDA applications. Partial least squares results reveal that better data quality is needed to unlock the value of IoT and BDA capabilities.}
}
@article{LV2020103,
title = {Analysis of healthcare big data},
journal = {Future Generation Computer Systems},
volume = {109},
pages = {103-110},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.03.039},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20304829},
author = {Zhihan Lv and Liang Qiao},
keywords = {Big data, Health care, Privacy security risk, Privacy measures},
abstract = {In order to explore the development of healthcare in China and the privacy and security risk factors in medical data under the background of big data, the development status of China’s healthcare sector is analyzed. The questionnaire is used to analyze the privacy and security risk factors of healthcare big data and protection measures are put forward based on the data privacy and security risk factors in the context of cloud services in the literature. The results show that in recent years, the number of health institutions, the number of medical personnel, the assets of medical institutions, the per capita hospitalization cost, and the insured population all show a trend of increasing year by year; while in 2017, the crude mortality rate of malignant tumor patients was the highest in China, and the mortality rate of rural patients was higher than that of urban patients. The results of the questionnaire show that the probability of data analysis, medical treatment process, disease diagnosis process, lack of protective measures, and imperfect access system is all greater than 0.8 when medical care big data is oriented to cloud services. Based on this, two levels of privacy protection measures are proposed: technology and management. It indicates that medical institutions need to pay attention to data privacy protection and grasp the use of digital medical data to provide decision support for subsequent medical data analysis.}
}
@article{GILL202051,
title = {Big Data Everywhere: The Impact of Data Disjunction in the Direct-to-Consumer Testing Model},
journal = {Clinics in Laboratory Medicine},
volume = {40},
number = {1},
pages = {51-59},
year = {2020},
note = {Direct-to-Consumer Testing: The Role of Laboratory Medicine},
issn = {0272-2712},
doi = {https://doi.org/10.1016/j.cll.2019.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0272271219300927},
author = {Emily L. Gill and Stephen R. Master},
keywords = {Big Data, Laboratory medicine, Machine learning, Direct-to-consumer testing, DTC, Harmonization}
}
@article{ARBEX2020102671,
title = {Estimating the influence of crowding and travel time variability on accessibility to jobs in a large public transport network using smart card big data},
journal = {Journal of Transport Geography},
volume = {85},
pages = {102671},
year = {2020},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2020.102671},
url = {https://www.sciencedirect.com/science/article/pii/S0966692319300092},
author = {Renato Arbex and Claudio B. Cunha},
keywords = {Public transport, Accessibility, Smart card data, In-vehicle crowding, Travel time reliability},
abstract = {Accessibility metrics are gaining momentum in public transportation planning and policy-making. However, critical user experience issues such as crowding discomfort and travel time unreliability are still not considered in those accessibility indicators. This paper aims to apply a methodology to build spatiotemporal crowding data and estimate travel time variability in a congested public transport network to improve accessibility calculations. It relies on using multiple big data sources available in most transit systems such as smart card and automatic vehicle location (AVL) data. São Paulo, Brazil, is used as a case study to show the impact of crowding and travel time variability on accessibility to jobs. Our results evidence a population-weighted average reduction of 56.8% in accessibility to jobs in a regular workday morning peak due to crowding discomfort, as well as reductions of 6.2% due to travel time unreliability and 59.2% when both are combined. The findings of this study can be of invaluable help to public transport planners and policymakers, as they show the importance of including both aspects in accessibility indicators for better decision making. Despite some limitations due to data quality and consistency throughout the study period, the proposed approach offers a new way to leverage big data in public transport to enhance policy decisions.}
}
@article{VIEIRA2020125,
title = {Are Simulation Tools Ready For Big Data? Computational Experiments with Supply Chain Models Developed in Simio},
journal = {Procedia Manufacturing},
volume = {42},
pages = {125-131},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.093},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920306582},
author = {António A.C. Vieira and Luís Dias and Maribel Y. Santos and Guilherme A.B. Pereira and José Oliveira},
keywords = {Simulation, Supply Chain, Big Data, Industry 4.0},
abstract = {The need and potential benefits for the combined use of Simulation and Big Data in Supply Chains (SCs) has been widely recognized. Having worked on such project, some simulation experiments of the modelled SC system were conducted in SIMIO. Different circumstances were tested, including running the model based on the stored data, on statistical distributions and considering risk situations. Thus, this paper aimed to evaluate such experiments, to evaluate the performance of simulations in these contexts. After analyzing the obtained results, it was found that whilst running the model based on the real data required considerable amounts of computer memory, running the model based on statistical distributions reduced such values, albeit required considerable higher time to run a single replication. In all the tested experiments, the simulation took considerable time to run and was not smooth, which can reduce the stakeholders’ interest in the developed tool, despite its benefits for the decision-making process. For future researches, it would be beneficial to test other simulation tools and other strategies and compare those results to the ones provided in this paper.}
}
@article{BAIG2019102095,
title = {Big data adoption: State of the art and research challenges},
journal = {Information Processing & Management},
volume = {56},
number = {6},
pages = {102095},
year = {2019},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2019.102095},
url = {https://www.sciencedirect.com/science/article/pii/S0306457319301773},
author = {Maria Ijaz Baig and Liyana Shuib and Elaheh Yadegaridehkordi},
keywords = {Big data adoption, Technology–Organization–Environment, Diffusion of Innovations},
abstract = {Big data adoption is a process through which businesses find innovative ways to enhance productivity and predict risk to satisfy customers need more efficiently. Despite the increase in demand and importance of big data adoption, there is still a lack of comprehensive review and classification of the existing studies in this area. This research aims to gain a comprehensive understanding of the current state-of-the-art by highlighting theoretical models, the influence factors, and the research challenges of big data adoption. By adopting a systematic selection process, twenty studies were identified in the domain of big data adoption and were reviewed in order to extract relevant information that answers a set of research questions. According to the findings, Technology–Organization–Environment and Diffusion of Innovations are the most popular theoretical models used for big data adoption in various domains. This research also revealed forty-two factors in technology, organization, environment, and innovation that have a significant influence on big data adoption. Finally, challenges found in the current research about big data adoption are represented, and future research directions are recommended. This study is helpful for researchers and stakeholders to take initiatives that will alleviate the challenges and facilitate big data adoption in various fields.}
}
@article{DAISSAOUI2020161,
title = {IoT and Big Data Analytics for Smart Buildings: A Survey},
journal = {Procedia Computer Science},
volume = {170},
pages = {161-168},
year = {2020},
note = {The 11th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 3rd International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920304506},
author = {Abdellah Daissaoui and Azedine Boulmakoul and Lamia Karim and Ahmed Lbath},
keywords = {Smart buildings, IoT, Big data analytics, Reactif systems, Complex event processing},
abstract = {The processes of digital transformation have involved a variety of socio-technical activities, with the objective of increasing productivity, safety and quality of execution, sustainable development, collaborative working and solutions for the sustainable smart city. The major digital trends, changing the building sector and revealing new trends of understanding information technologies to integrate in this sector. Current smart building management systems incorporate a variety of sensors, actuators and dedicated networks. Their objectives are to observe the condition of specific areas and apply appropriate rules to preserve or improve comfort while saving energy. In this paper, we propose a review of related works to IoT, Big Data Analytics in smart buildings.}
}
@article{HAZEN201472,
title = {Data quality for data science, predictive analytics, and big data in supply chain management: An introduction to the problem and suggestions for research and applications},
journal = {International Journal of Production Economics},
volume = {154},
pages = {72-80},
year = {2014},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2014.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0925527314001339},
author = {Benjamin T. Hazen and Christopher A. Boone and Jeremy D. Ezell and L. Allison Jones-Farmer},
keywords = {Data quality, Statistical process control, Knowledge-based view, Organizational information processing view, Systems theory},
abstract = {Today׳s supply chain professionals are inundated with data, motivating new ways of thinking about how data are produced, organized, and analyzed. This has provided an impetus for organizations to adopt and perfect data analytic functions (e.g. data science, predictive analytics, and big data) in order to enhance supply chain processes and, ultimately, performance. However, management decisions informed by the use of these data analytic methods are only as good as the data on which they are based. In this paper, we introduce the data quality problem in the context of supply chain management (SCM) and propose methods for monitoring and controlling data quality. In addition to advocating for the importance of addressing data quality in supply chain research and practice, we also highlight interdisciplinary research topics based on complementary theory.}
}
@article{ALIC2019243,
title = {BIGSEA: A Big Data analytics platform for public transportation information},
journal = {Future Generation Computer Systems},
volume = {96},
pages = {243-269},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18304448},
author = {Andy S. Alic and Jussara Almeida and Giovanni Aloisio and Nazareno Andrade and Nuno Antunes and Danilo Ardagna and Rosa M. Badia and Tania Basso and Ignacio Blanquer and Tarciso Braz and Andrey Brito and Donatello Elia and Sandro Fiore and Dorgival Guedes and Marco Lattuada and Daniele Lezzi and Matheus Maciel and Wagner Meira and Demetrio Mestre and Regina Moraes and Fabio Morais and Carlos Eduardo Pires and Nádia P. Kozievitch and Walter dos Santos and Paulo Silva and Marco Vieira},
abstract = {Analysis of public transportation data in large cities is a challenging problem. Managing data ingestion, data storage, data quality enhancement, modelling and analysis requires intensive computing and a non-trivial amount of resources. In EUBra-BIGSEA (Europe–Brazil Collaboration of Big Data Scientific Research Through Cloud-Centric Applications) we address such problems in a comprehensive and integrated way. EUBra-BIGSEA provides a platform for building up data analytic workflows on top of elastic cloud services without requiring skills related to either programming or cloud services. The approach combines cloud orchestration, Quality of Service and automatic parallelisation on a platform that includes a toolbox for implementing privacy guarantees and data quality enhancement as well as advanced services for sentiment analysis, traffic jam estimation and trip recommendation based on estimated crowdedness. All developments are available under Open Source licenses (http://github.org/eubr-bigsea, https://hub.docker.com/u/eubrabigsea/).}
}
@article{ZHANG2019814,
title = {Big data driven decision-making for batch-based production systems},
journal = {Procedia CIRP},
volume = {83},
pages = {814-818},
year = {2019},
note = {11th CIRP Conference on Industrial Product-Service Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.05.023},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119310169},
author = {Yongheng Zhang and Rui Zhang and Yizhong Wang and Hongfei Guo and Ray Y Zhong and Ting Qu and Zhiwu Li},
keywords = {Big data, Smart Product-Service System, Sales predict, Economic batch quantity, production plan},
abstract = {The era of big data has brought new challenges to chemical enterprises. In order to maximize the benefits, enterprises are considering to implement intelligent service technology into traditional production systems to improve the level of intelligence in business. This paper proposes a service framework based on big data driven prediction, which includes information perception layer, information application layer and big data service layer. In this paper, the composition of big data service layer is described in detail, and a sales predicting method based on neural network is introduced. The salability of products is divided, and the qualitative economic production volume mechanism is finally given. Based on the framework, an intelligent service system for enterprises with the characteristics of mass production is implemented. Experimental results show that the big data service framework can support chemical enterprises to make decisions to reduce costs, and provides an effective method for Smart Product Service System (PSS).}
}
@article{ELIA2020508,
title = {A multi-dimension framework for value creation through big data},
journal = {Industrial Marketing Management},
volume = {90},
pages = {508-522},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2019.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0019850118307600},
author = {Gianluca Elia and Gloria Polimeno and Gianluca Solazzo and Giuseppina Passiante},
keywords = {Big data analytics, Cognitive computing, Framework, Model, Systematic literature review, Value creation},
abstract = {Big Data represents a promising area for value creation and frontier research. The potential to extract actionable insights from Big Data has gained increasing attention of both academics and practitioners operating in several industries. Marketing domain has become from the start a field for experiments with Big Data approaches, even if the adoption of Big Data solutions does not always generate effective value for the adopters. Therefore, the gap existing between the potential of value creation embedded in the Big Data paradigm and the current limited exploitation of this value represents an area of investigation that this paper aims to explore. In particular, by following a systematic literature review, this study aims at presenting a framework that outlines the multiple value directions that the Big Data paradigm can generate for the adopting organizations. Eleven distinct value directions have been identified and then grouped in five dimensions (Informational, Transactional, Transformational, Strategic, Infrastructural Value), which constitute the pillars of the proposed framework. Finally, the framework has been also preliminarily applied in three case studies conducted within three Italian based companies operating in different industries (e-commerce, fast-moving consumer goods, and banking) in the final aim to see its applicability in real business scenarios.}
}
@article{VIEIRA2020101985,
title = {On the use of simulation as a Big Data semantic validator for supply chain management},
journal = {Simulation Modelling Practice and Theory},
volume = {98},
pages = {101985},
year = {2020},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2019.101985},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X19301182},
author = {António AC Vieira and Luís MS Dias and Maribel Y Santos and Guilherme AB Pereira and José A Oliveira},
keywords = {Simulation, Big Data, Data issues, Semantic validation, Supply chain management, Industry 4.0},
abstract = {Simulation stands out as an appropriate method for the Supply Chain Management (SCM) field. Nevertheless, to produce accurate simulations of Supply Chains (SCs), several business processes must be considered. Thus, when using real data in these simulation models, Big Data concepts and technologies become necessary, as the involved data sources generate data at increasing volume, velocity and variety, in what is known as a Big Data context. While developing such solution, several data issues were found, with simulation proving to be more efficient than traditional data profiling techniques in identifying them. Thus, this paper proposes the use of simulation as a semantic validator of the data, proposed a classification for such issues and quantified their impact in the volume of data used in the final achieved solution. This paper concluded that, while SC simulations using Big Data concepts and technologies are within the grasp of organizations, their data models still require considerable improvements, in order to produce perfect mimics of their SCs. In fact, it was also found that simulation can help in identifying and bypassing some of these issues.}
}
@article{JIN202024,
title = {Big Data in food safety- A review},
journal = {Current Opinion in Food Science},
volume = {36},
pages = {24-32},
year = {2020},
note = {Food Safety},
issn = {2214-7993},
doi = {https://doi.org/10.1016/j.cofs.2020.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S2214799320301260},
author = {Cangyu Jin and Yamine Bouzembrak and Jiehong Zhou and Qiao Liang and Leonieke M. {van den Bulk} and Anand Gavai and Ningjing Liu and Lukas J. {van den Heuvel} and Wouter Hoenderdaal and Hans J.P. Marvin},
abstract = {The massive rise of Big Data generated from smartphones, social media, Internet of Things (IoT), and multimedia, has produced an overwhelming flow of data in either structured or unstructured format. Big Data technologies are being developed and implemented in the food supply chain that gather and analyse these data. Such technologies demand new approaches in data collection, storage, processing and knowledge extraction. In this article, an overview of the recent developments in Big Data applications in food safety are presented. This review shows that the use of Big Data in food safety remains in its infancy but it is influencing the entire food supply chain. Big Data analysis is used to provide predictive insights in several steps in the food supply chain, support supply chain actors in taking real time decisions, and design the monitoring and sampling strategies. Lastly, the main research challenges that require research efforts are introduced.}
}
@article{LOZADA2019e02541,
title = {Big data analytics capability and co-innovation: An empirical study},
journal = {Heliyon},
volume = {5},
number = {10},
pages = {e02541},
year = {2019},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2019.e02541},
url = {https://www.sciencedirect.com/science/article/pii/S2405844019362012},
author = {Nelson Lozada and Jose Arias-Pérez and Geovanny Perdomo-Charry},
keywords = {Business, Economics, Information science, Big data analytics capabilities, Co-innovation, Big data, Co-creation},
abstract = {There are numerous emerging studies addressing big data and its application in different organizational aspects, especially regarding its impact on the business innovation process. This study in particular aims at analyzing the existing relationship between Big Data Analytics Capabilities and Co-innovation. To test the hypothesis model, structural equations by the partial least squares method were used in a sample of 112 Colombian firms. The main findings allow to positively relate Big Data Analytics Capabilities with better and more agile processes of product and service co-creation and with more robust collaboration networks with stakeholders internal and external to the firm.}
}
@article{GHOLIZADEH2020120640,
title = {A robust fuzzy stochastic programming for sustainable procurement and logistics under hybrid uncertainty using big data},
journal = {Journal of Cleaner Production},
volume = {258},
pages = {120640},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.120640},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620306879},
author = {Hadi Gholizadeh and Hamed Fazlollahtabar and Mohammad Khalilzadeh},
keywords = {logistics, Robust fuzzy stochastic programming, Sustainable procurement, Big data, Hybrid uncertainty, ε-Constraint},
abstract = {Today, in many organizations, the debate about the difference in core capabilities has become an important factor for market competition. Companies, based on the field of activity, decide to strengthen some of their capabilities, capacities, and expertise. Therefore, the focus of an organization on the strengths and efforts to develop its sustainability will lead to a competitive advantage in the marketplace. Due to changes in environmental factors, organizations have focused on carbon emissions in procurement and transportation that have the highest carbon footprint. This paper proposes a multi-objective, eco-sustainability model for a supply chain. The objectives are to minimize overall costs, maximize the efficiency of transportation vehicles and minimize information fraud in the process of information sharing within supply chain elements. Big data is considered in the amount of information exchanged between customers and other elements of the proposed supply chain; since there are frauds in information sharing then using big data 5Vs the model is adapted to control the cost of information loss leading to customer dissatisfaction. Since uncertainty is inevitable in the real environments, in this research hybrid uncertainty is considered. Because two sources of uncertainty are considered in most of the parameters, thus it is necessary to robustify the decision-making process. The model is a mixed integer nonlinear program including big data for an optimal sustainable procurement and transportation decision. A heuristic method is used to solve the big data problem that makes use of a robust fuzzy stochastic programming approach. The proposed model can prevent disturbances by using a scenario-based stochastic programming approach. An effective hybrid robust fuzzy stochastic method is also employed for controlling uncertainty in parameters and risk taking out of outbound decisions. To solve the multi-objective model, augmented ε-constraint method is utilized. The model performance is investigated in a comprehensive computational study.}
}
@article{JHA2020113382,
title = {A note on big data analytics capability development in supply chain},
journal = {Decision Support Systems},
volume = {138},
pages = {113382},
year = {2020},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113382},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620301378},
author = {Ashish Kumar Jha and Maher A.N. Agi and Eric W.T. Ngai},
keywords = {Big data, Analytics, Capability development, Qualitative study, Supply chain},
abstract = {Big data analytics (BDA) are gaining importance in all aspects of business management. This is driven by both the presence of large-scale data and management's desire to root decisions in data. Extant research demonstrates that supply chain and operations management functions are among the biggest sources and users of data in the company. Therefore, their decision-making processes would benefit from increased use of BDA technologies. However, there is still a lack of understanding of what determines a company's ability to build BDA capability to gain a competitive advantage. In this study, we attempt to answer this fundamental question by identifying the factors that assist a company in or inhibit it from building its BDA capability and maximizing its gains through BDA technologies. We base our findings on a qualitative analysis of data collected from field visits, interviews with senior management, and secondary resources. We find that, in addition to technical capacity, competitive landscape and intra-firm power dynamics play an important role in building BDA capability and using BDA technologies.}
}
@article{KEZUNOVIC2020106788,
title = {Big data analytics for future electricity grids},
journal = {Electric Power Systems Research},
volume = {189},
pages = {106788},
year = {2020},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2020.106788},
url = {https://www.sciencedirect.com/science/article/pii/S0378779620305915},
author = {Mladen Kezunovic and Pierre Pinson and Zoran Obradovic and Santiago Grijalva and Tao Hong and Ricardo Bessa},
keywords = {Electricity grids, Analytics, Big data, Decision-making},
abstract = {This paper provides a survey of big data analytics applications and associated implementation issues. The emphasis is placed on applications that are novel and have demonstrated value to the industry, as illustrated using field data and practical applications. The paper reflects on the lessons learned from initial implementations, as well as ideas that are yet to be explored. The various data science trends treated in the literature are outlined, while experiences from applying them in the electricity grid setting are emphasized to pave the way for future applications. The paper ends with opportunities and challenges, as well as implementation goals and strategies for achieving impactful outcomes.}
}
@article{CUI2020101861,
title = {Manufacturing big data ecosystem: A systematic literature review},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {62},
pages = {101861},
year = {2020},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2019.101861},
url = {https://www.sciencedirect.com/science/article/pii/S0736584519300559},
author = {Yesheng Cui and Sami Kara and Ka C. Chan},
keywords = {Smart manufacturing, Big data, Cloud computing, Cloud manufacturing, Internet of things, NoSQL},
abstract = {Advanced manufacturing is one of the core national strategies in the US (AMP), Germany (Industry 4.0) and China (Made-in China 2025). The emergence of the concept of Cyber Physical System (CPS) and big data imperatively enable manufacturing to become smarter and more competitive among nations. Many researchers have proposed new solutions with big data enabling tools for manufacturing applications in three directions: product, production and business. Big data has been a fast-changing research area with many new opportunities for applications in manufacturing. This paper presents a systematic literature review of the state-of-the-art of big data in manufacturing. Six key drivers of big data applications in manufacturing have been identified. The key drivers are system integration, data, prediction, sustainability, resource sharing and hardware. Based on the requirements of manufacturing, nine essential components of big data ecosystem are captured. They are data ingestion, storage, computing, analytics, visualization, management, workflow, infrastructure and security. Several research domains are identified that are driven by available capabilities of big data ecosystem. Five future directions of big data applications in manufacturing are presented from modelling and simulation to real-time big data analytics and cybersecurity.}
}
@article{NORTHCOTT202096,
title = {Big data and prediction: Four case studies},
journal = {Studies in History and Philosophy of Science Part A},
volume = {81},
pages = {96-104},
year = {2020},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2019.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0039368119300652},
author = {Robert Northcott},
keywords = {Big data, Prediction, Case studies, Explanation, Elections, Weather},
abstract = {Has the rise of data-intensive science, or ‘big data’, revolutionized our ability to predict? Does it imply a new priority for prediction over causal understanding, and a diminished role for theory and human experts? I examine four important cases where prediction is desirable: political elections, the weather, GDP, and the results of interventions suggested by economic experiments. These cases suggest caution. Although big data methods are indeed very useful sometimes, in this paper's cases they improve predictions either limitedly or not at all, and their prospects of doing so in the future are limited too.}
}
@article{SILVA2019532,
title = {Risk Analysis of Using Big Data in Computer Sciences},
journal = {Procedia Computer Science},
volume = {160},
pages = {532-537},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919317521},
author = {Jesus Silva and Omar Bonerge {Pineda Lezama} and Ligia Romero and Darwin Solano and Claudia Fernández},
keywords = {Data management, data quality, decision making, data analysis},
abstract = {Today, as technologies mature and people are encouraged to contribute data to organizations’ databases, more transactions are being captured than ever before. Meanwhile, improvements in data storage technologies have made the cost of evaluating, selecting, and destroying legacy data considerably greater than simply letting it accumulate. On the one hand, the excess of stored data has considerably increased the opportunities to interrelate and analyze them, while the moderate enthusiasm generated by data warehousing and data mining in the 1990s has been replaced by a rampant euphoria about big data and data analytics. But, is this as wonderful as seems? This paper presents a risk analysis of Big Data and Big Data Analytics based on a review of quality factors.}
}
@article{TRIPATHI20201245,
title = {Big-data driven approaches in materials science: A survey},
journal = {Materials Today: Proceedings},
volume = {26},
pages = {1245-1249},
year = {2020},
note = {10th International Conference of Materials Processing and Characterization},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.02.249},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320310026},
author = {Manwendra K. Tripathi and Randhir Kumar and Rakesh Tripathi},
keywords = {Material science, Big data, Machine learning, Data analytics, Predictive Algorithms},
abstract = {The data volume is growing rapidly in material science. Every year data volume is getting double in many context of material science. The growing rate of data in material science is demanding for new computational infrastructures that can speed-up material discovery and deployment. In this survey, we are focusing on the challenges in material science due to growing data rate, and how Big Data technology can play a major role in research of material science. This survey includes various disciplines that can be used with Big Data to provide better analysis in the material science research.}
}
@article{LEEPOST2019113135,
title = {Numerical, secondary Big Data quality issues, quality threshold establishment, & guidelines for journal policy development},
journal = {Decision Support Systems},
volume = {126},
pages = {113135},
year = {2019},
note = {Perspectives on Numerical Data Quality in IS Research},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113135},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619301642},
author = {Anita Lee-Post and Ram Pakath},
keywords = {Data quality, Big data, Secondary data, Numerical data, Quality threshold},
abstract = {An IS researcher may obtain Big Data from primary or secondary data sources. Sometimes, acquiring primary Big Data is infeasible due to availability, accessibility, cost, time, and/or complexity considerations. In this paper, we focus on Big Data-based IS research and discuss ways in which one may, post hoc, establish quality thresholds for numerical Big Data obtained from secondary sources. We also present guidelines for developing journal policies aimed at ensuring the veracity and verifiability of such data when used for research purposes.}
}
@article{GHALLAB2020131,
title = {Detection outliers on internet of things using big data technology},
journal = {Egyptian Informatics Journal},
volume = {21},
number = {3},
pages = {131-138},
year = {2020},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2019.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1110866519301616},
author = {Haitham Ghallab and Hanan Fahmy and Mona Nasr},
keywords = {Internet of things, IoT, Big data, Data quality, Outliers Detection, DBSCAN, RDDs},
abstract = {Internet of Things (IoT) is a fundamental concept of a new technology that will be promising and significant in various fields. IoT is a vision that allows things or objects equipped with sensors, actuators, and processors to talk and communicate with each other over the internet to achieve a meaningful goal. Unfortunately, one of the major challenges that affect IoT is data quality and uncertainty, as data volume increases noise, inconsistency and redundancy increases within data and causes paramount issues for IoT technologies. And since IoT is considered to be a massive quantity of heterogeneous networked embedded devices that generate big data, then it is very complex to compute and analyze such massive data. So this paper introduces a new model named NRDD-DBSCAN based on DBSCAN algorithm and using resilient distributed datasets (RDDs) to detect outliers that affect the data quality of IoT technologies. NRDD-DBSCAN has been applied on three different datasets of N-dimensions (2-D, 3-D, and 25-D) and the results were promising. Finally, comparisons have been made between NRDD-DBSCAN and previous models such as RDD-DBSCAN model and DBSCAN algorithm, and these comparisons proved that NRDD-DBSCAN solved the low dimensionality issue of RDD-DBSCAN model and also solved the fact that DBSCAN algorithm cannot handle IoT data. So the conclusion is that NRDD-DBSCAN proposed model can detect the outliers that exist in the datasets of N-dimensions by using resilient distributed datasets (RDDs), and NRDD-DBSCAN can enhance the quality of data exists in IoT applications and technologies.}
}
@article{MIKALEF2020103361,
title = {The role of information governance in big data analytics driven innovation},
journal = {Information & Management},
volume = {57},
number = {7},
pages = {103361},
year = {2020},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2020.103361},
url = {https://www.sciencedirect.com/science/article/pii/S0378720620302998},
author = {Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie},
keywords = {Big data analytics capabilities, Information governance, Incremental innovation, Radical innovation, Environmental uncertainty, FIMIX-PLS},
abstract = {The age of big data analytics is now here, with companies increasingly investing in big data initiatives to foster innovation and outperform competition. Nevertheless, while researchers and practitioners started to examine the shifts that these technologies entail and their overall business value, it is still unclear whether and under what conditions they drive innovation. To address this gap, this study draws on the resource-based view (RBV) of the firm and information governance theory to explore the interplay between a firm’s big data analytics capabilities (BDACs) and their information governance practices in shaping innovation capabilities. We argue that a firm’s BDAC helps enhance two distinct types of innovative capabilities, incremental and radical capabilities, and that information governance positively moderates this relationship. To examine our research model, we analyzed survey data collected from 175 IT and business managers. Results from partial least squares structural equation modelling analysis reveal that BDACs have a positive and significant effect on both incremental and radical innovative capabilities. Our analysis also highlights the important role of information governance, as it positively moderates the relationship between BDAC’s and a firm’s radical innovative capability, while there is a nonsignificant moderating effect for incremental innovation capabilities. Finally, we examine the effect of environmental uncertainty conditions in our model and find that information governance and BDACs have amplified effects under conditions of high environmental dynamism.}
}
@article{ABOELMAGED2020102234,
title = {Influencing models and determinants in big data analytics research: A bibliometric analysis},
journal = {Information Processing & Management},
volume = {57},
number = {4},
pages = {102234},
year = {2020},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2020.102234},
url = {https://www.sciencedirect.com/science/article/pii/S0306457319313366},
author = {Mohamed Aboelmaged and Samar Mouakket},
keywords = {Big data analytics, Technology adoption, Literature review, Bibliometric analysis, Theoretical models, Adoption frameworks},
abstract = {Incorporating big data analytics into a particular context brings various challenges that rest on the model or framework through which individuals or organisations adopt big data to achieve their objectives. Although these models have recently triggered scholars’ attention in various domains, in-depth knowledge of using each of these models in big data research is still blurred. This study enriches our knowledge on emerging models and theories that shape big data analytics adoption (BDAD) research through a bibliometric analysis of 229 studies (143 journal articles and 86 conference papers) published in indexed sources between 2013 and 2019. As a result, twenty models on BDAD have emerged (e.g., “Dynamic Capabilities”, “Resource-Based View”, “Technology Acceptance Model”, “Diffusion of Innovation”, etc.). The analysis reveals that BDAD research to demonstrate attributes suggestive of a topic at an initial stage of development as it is broadly dispersed across different domains employs a wide range of models, some of which overlap. Most of the applied models are generic in nature focusing on variance-based relationships and snapshot prediction with little consensus. There is a conspicuous dearth of process models, firm-level analysis and cultural orientation in contemporary BDAD research. Insights of this bibliometric study could guide rigorous big data research and practice in various contexts. The study concludes with research implications and limitations that offer promising prospects for forthcoming research.}
}
@article{SFAXI2020101862,
title = {DECIDE: An Agile event-and-data driven design methodology for decisional Big Data projects},
journal = {Data & Knowledge Engineering},
volume = {130},
pages = {101862},
year = {2020},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2020.101862},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X19303830},
author = {Lilia Sfaxi and Mohamed Mehdi Ben Aissa},
keywords = {Big Data, Methodology, Decisional systems, Agile, Data governance, Data quality},
abstract = {Decision making is the lifeblood of the enterprise — from the mundane to the strategically critical. However, the increasing deluge of data makes it more important than ever to understand and use it effectively in every context. Being “data driven” is more aspiration than reality in most organizations due to the complexity, volume, variability and velocity of data streams from every customer and employee interaction. The purpose of this paper is to provide a flexible and adaptable methodology for governing, managing and applying data throughout the enterprise, called DECIDE.}
}
@incollection{BIRKIN2020303,
title = {Big Data},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {303-311},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10616-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008102295510616X},
author = {Mark Birkin},
keywords = {Administrative data, Crowdsourcing, Data ethics, Data quality, Data sharing, Representation and bias, Social media, Spatial analysis, Value, Variety, Velocity, Volume, Volunteered geographical information},
abstract = {Big Data are deeply impactful for research in providing a diverse and rich array of novel sources for academic enquiry. Geographers are benefitting from varied data types including administrative data, social media, volunteered geographic information, and consumer data. The article presents examples and illustrations associated with each of these data types. An additional benefit of Big Data analytics is that it brings geographers into closer contact with real world partners and policy problems. Big Data also attract challenges including representational bias, variable data quality, difficulties of access and ownership, and ethical and legal restrictions in their use. Hence we conclude that Big Data are ripe for fruitful exploitation, but careful investments will be necessary if opportunities are to be realized to the full.}
}
@article{SURBAKTI2020103146,
title = {Factors influencing effective use of big data: A research framework},
journal = {Information & Management},
volume = {57},
number = {1},
pages = {103146},
year = {2020},
note = {Big data and business analytics: A research agenda for realizing business value},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617308649},
author = {Feliks P. Sejahtera Surbakti and Wei Wang and Marta Indulska and Shazia Sadiq},
keywords = {Big data, Effective use, Factors, Framework},
abstract = {Information systems (IS) research has explored “effective use” in a variety of contexts. However, it is yet to specifically consider it in the context of the unique characteristics of big data. Yet, organizations have a high appetite for big data, and there is growing evidence that investments in big data solutions do not always lead to the derivation of intended value. Accordingly, there is a need for rigorous academic guidance on what factors enable effective use of big data. With this paper, we aim to guide IS researchers such that the expansion of the body of knowledge on the effective use of big data can proceed in a structured and systematic manner and can subsequently lead to empirically driven guidance for organizations. Namely, with this paper, we cast a wide net to understand and consolidate from literature the potential factors that can influence the effective use of big data, so they may be further studied. To do so, we first conduct a systematic literature review. Our review identifies 41 factors, which we categorize into 7 themes, namely data quality; data privacy and security and governance; perceived organizational benefit; process management; people aspects; systems, tools, and technologies; and organizational aspects. To explore the existence of these themes in practice, we then analyze 45 published case studies that document insights into how specific companies use big data successfully. Finally, we propose a framework for the study of effective use of big data as a basis for future research. Our contributions aim to guide researchers in establishing the relevance and relationships within the identified themes and factors and are a step toward developing a deeper understanding of effective use of big data.}
}
@article{RIDZUAN2019731,
title = {A Review on Data Cleansing Methods for Big Data},
journal = {Procedia Computer Science},
volume = {161},
pages = {731-738},
year = {2019},
note = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.177},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919318885},
author = {Fakhitah Ridzuan and Wan Mohd Nazmee {Wan Zainon}},
keywords = {data cleansing, big data, data quality},
abstract = {Massive amounts of data are available for the organization which will influence their business decision. Data collected from the various resources are dirty and this will affect the accuracy of prediction result. Data cleansing offers a better data quality which will be a great help for the organization to make sure their data is ready for the analyzing phase. However, the amount of data collected by the organizations has been increasing every year, which is making most of the existing methods no longer suitable for big data. Data cleansing process mainly consists of identifying the errors, detecting the errors and corrects them. Despite the data need to be analyzed quickly, the data cleansing process is complex and time-consuming in order to make sure the cleansed data have a better quality of data. The importance of domain expert in data cleansing process is undeniable as verification and validation are the main concerns on the cleansed data. This paper reviews the data cleansing process, the challenge of data cleansing for big data and the available data cleansing methods.}
}
@article{RAHUL2020364,
title = {Data Life Cycle Management in Big Data Analytics},
journal = {Procedia Computer Science},
volume = {173},
pages = {364-371},
year = {2020},
note = {International Conference on Smart Sustainable Intelligent Computing and Applications under ICITETM2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.06.042},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920315465},
author = {Kumar Rahul and Rohitash Kumar Banyal},
keywords = {Data life cycle, Data creation, Data usability, Healthcare, Big data},
abstract = {Data life cycle management is very much useful for any enterprise or application where data is being used and processed for producing results. Data’s appearance for a certain period time ensures accessibility and usability in the system. Data generated through different sources and it is available in various forms for accessibility. A big data-based application such as the healthcare sector generates lots of data through sensors and other electronic devices which can be further classified into a model for report generations and predictions for various purposes for the benefits of patients and hospitals as well. The data life cycle presents the entire data process in the system. The lifecycle of data starts from creation, store, usability, sharing, and archive and destroy in the system and applications. It defines the data flow in an organization. For the successful implementation of the model, there is a need to maintain the life cycle of data under a secured system of data management. This paper deals with the data life cycle with different steps and various works are done for data management in different sectors and benefits of the data life cycle for industrial and healthcare applications including challenges, conclusions, and future scope.}
}
@article{LI2020100608,
title = {Network analysis of big data research in tourism},
journal = {Tourism Management Perspectives},
volume = {33},
pages = {100608},
year = {2020},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2019.100608},
url = {https://www.sciencedirect.com/science/article/pii/S2211973619301400},
author = {Xin Li and Rob Law},
keywords = {Big data, Tourism studies, Co-citation analysis, Network analysis, Research trends},
abstract = {This study aims to provide a comprehensive network analysis to understand the current state of big data research in tourism by investigating multi-disciplinary contributions relevant to big data. A comprehensive network analytical method, which includes co-citation, clustering and trend analysis, is applied to systematically analyse publications from 2008 to 2017. Two unique data sets from Web of Science are collected. The first data set focuses on big data research in tourism and hospitality. The second data set involves other disciplines, such as computer science, for a comparison with tourism. Results suggest that applications of social media and user-generated content are gaining momentum, whereas theory-based studies on big data in tourism remain limited. Tourism and other relevant domains have similar concerns with the challenges involved in big data, such as privacy, data quality and appropriate data use. This comparative network analysis has implications for future big data research in tourism.}
}
@article{WANG2020120175,
title = {Tension in big data using machine learning: Analysis and applications},
journal = {Technological Forecasting and Social Change},
volume = {158},
pages = {120175},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120175},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520310015},
author = {Huamao Wang and Yumei Yao and Said Salhi},
keywords = {Big data, Machine learning, Data size, Prediction accuracy, Social media},
abstract = {The access of machine learning techniques in popular programming languages and the exponentially expanding big data from social media, news, surveys, and markets provide exciting challenges and invaluable opportunities for organizations and individuals to explore implicit information for decision making. Nevertheless, the users of machine learning usually find that these sophisticated techniques could incur a high level of tensions caused by the selection of the appropriate size of the training data set among other factors. In this paper, we provide a systematic way of resolving such tensions by examining practical examples of predicting popularity and sentiment of posts on Twitter and Facebook, blogs on Mashable, news on Google and Yahoo, the US house survey, and Bitcoin prices. Interesting results show that for the case of big data, using around 20% of the full sample often leads to a better prediction accuracy than opting for the full sample. Our conclusion is found to be consistent across a series of experiments. The managerial implication is that using more is not necessarily the best and users need to be cautious about such an important sensitivity as the simplistic approach may easily lead to inferior solutions with potentially detrimental consequences.}
}
@article{MIKALEF2019261,
title = {Big data analytics and firm performance: Findings from a mixed-method approach},
journal = {Journal of Business Research},
volume = {98},
pages = {261-276},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2019.01.044},
url = {https://www.sciencedirect.com/science/article/pii/S014829631930061X},
author = {Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie},
keywords = {Big data analytics, Complexity theory, fsQCA, Business value, Mixed-method, Environmental uncertainty},
abstract = {Big data analytics has been widely regarded as a breakthrough technological development in academic and business communities. Despite the growing number of firms that are launching big data initiatives, there is still limited understanding on how firms translate the potential of such technologies into business value. The literature argues that to leverage big data analytics and realize performance gains, firms must develop strong big data analytics capabilities. Nevertheless, most studies operate under the assumption that there is limited heterogeneity in the way firms build their big data analytics capabilities and that related resources are of similar importance regardless of context. This paper draws on complexity theory and investigates the configurations of resources and contextual factors that lead to performance gains from big data analytics investments. Our empirical investigation followed a mixed methods approach using survey data from 175 chief information officers and IT managers working in Greek firms, and three case studies to show that depending on the context, big data analytics resources differ in significance when considering performance gains. Applying a fuzzy-set qualitative comparative analysis (fsQCA) method on the quantitative data, we show that there are four different patterns of elements surrounding big data analytics that lead to high performance. Outcomes of the three case studies highlight the inter-relationships between these elements and outline challenges that organizations face when orchestrating big data analytics resources.}
}
@article{MATHIS2020582,
title = {Making Sense of Big Data to Improve Perioperative Care: Learning Health Systems and the Multicenter Perioperative Outcomes Group},
journal = {Journal of Cardiothoracic and Vascular Anesthesia},
volume = {34},
number = {3},
pages = {582-585},
year = {2020},
issn = {1053-0770},
doi = {https://doi.org/10.1053/j.jvca.2019.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S1053077019311590},
author = {Michael R. Mathis and Timur Z. Dubovoy and Matthew D. Caldwell and Milo C. Engoren}
}
@article{GAO2020668,
title = {Big data analytics for smart factories of the future},
journal = {CIRP Annals},
volume = {69},
number = {2},
pages = {668-692},
year = {2020},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2020.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0007850620301359},
author = {Robert X. Gao and Lihui Wang and Moneer Helu and Roberto Teti},
keywords = {Digital manufacturing system, Information, Learning},
abstract = {Continued advancement of sensors has led to an ever-increasing amount of data of various physical nature to be acquired from production lines. As rich information relevant to the machines and processes are embedded within these “big data”, how to effectively and efficiently discover patterns in the big data to enhance productivity and economy has become both a challenge and an opportunity. This paper discusses essential elements of and promising solutions enabled by data science that are critical to processing data of high volume, velocity, variety, and low veracity, towards the creation of added-value in smart factories of the future.}
}
@article{ZHANG2020483,
title = {Linking big data analytical intelligence to customer relationship management performance},
journal = {Industrial Marketing Management},
volume = {91},
pages = {483-494},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2020.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0019850120308762},
author = {Chubing Zhang and Xinchun Wang and Annie Peng Cui and Shenghao Han},
keywords = {Big data, Data-driven culture, Competitive pressures, Mass customization, Marketing capability, CRM performance},
abstract = {This study investigates the driving forces of a firm's assimilation of big data analytical intelligence (BDAI) and how the assimilation of BDAI improve customer relationship management (CRM) performance. Drawing on the resource-based view, this study argues that a firm's data-driven culture and the competitive pressure it faces in the industry motivate a firm's assimilation of BDAI. As a firm resource, BDAI enables an organization to develop superior mass-customization capability, which in turn positively influences its CRM performance. In addition, this study proposes that a firm's marketing capability can moderate the impact of BDAI assimilation on its mass-customization capability. Using survey data collected from 147 business-to-business companies, this study finds support for most of the hypotheses. The findings of this study uncover compelling insights about the dynamics involved in the process of using BDAI to improve CRM performance.}
}
@article{HAUSLADEN2020101721,
title = {Towards a maturity model for big data analytics in airline network planning},
journal = {Journal of Air Transport Management},
volume = {82},
pages = {101721},
year = {2020},
issn = {0969-6997},
doi = {https://doi.org/10.1016/j.jairtraman.2019.101721},
url = {https://www.sciencedirect.com/science/article/pii/S0969699718304988},
author = {Iris Hausladen and Maximilian Schosser},
keywords = {Maturity model, Network planning, Big data analytics, Airlines, Case study},
abstract = {The evaluation, acquisition and use of newly available big data sources has become a major strategic and organizational challenge for airline network planners. We address this challenge by developing a maturity model for big data readiness for airline network planning. The development of the maturity model is grounded in literature, expert interviews and case study research involving nine airlines. Four airline business models are represented, namely full-service carriers, low-cost airlines, scheduled charter airlines and cargo airlines. The maturity model has been well received with seven change requests in the model development phase. The revised version has been evaluated as exhaustive and useful by airline network planners. The self-assessment of airlines revealed low to medium maturity for most domains. Organizational factors show the lowest average maturity, IT architecture the highest. Full-service carriers seem to be more mature than airlines with different business models.}
}
@article{SHAH2020106970,
title = {Feature engineering in big data analytics for IoT-enabled smart manufacturing – Comparison between deep learning and statistical learning},
journal = {Computers & Chemical Engineering},
volume = {141},
pages = {106970},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.106970},
url = {https://www.sciencedirect.com/science/article/pii/S0098135420300363},
author = {Devarshi Shah and Jin Wang and Q. Peter He},
keywords = {Internet-of-Things, Smart manufacturing, Big data, Data analytics, Feature engineering, Deep learning, Statistical learning},
abstract = {As IoT-enabled manufacturing is still in its infancy, there are several key research gaps that need to be addressed. These gaps include the understanding of the characteristics of the big data generated from industrial IoT sensors, the challenges they present to process data analytics, as well as the specific opportunities that the IoT big data could bring to advance manufacturing. In this paper, we use an inhouse-developed IoT-enabled manufacturing testbed to study the characteristics of the big data generated from the testbed. Since the quality of the data usually has the most impact on process modeling, data veracity is often the most challenging characteristic of big data. To address that, we explore the role of feature engineering in developing effective machine learning models for predicting key process variables. We compare complex deep learning approaches to a simple statistical learning approach, with different level or extent of feature engineering, to explore their pros and cons for potential industrial IoT-enabled manufacturing applications.}
}
@article{HUANG2019592,
title = {Challenges, opportunities and paradigm of applying big data to production safety management: From a theoretical perspective},
journal = {Journal of Cleaner Production},
volume = {231},
pages = {592-599},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.05.245},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619317810},
author = {Lang Huang and Chao Wu and Bing Wang},
keywords = {Big data, Production safety management, Big-data-driven, Challenges, Opportunities},
abstract = {Big data has caused the scientific community to re-examine the scientific research methodologies and has triggered a revolution in scientific thinking. As a branch of scientific research, production safety management is also exploring methods to take advantage of big data. This research aims to provide a theoretical basis for promoting the application of big data in production safety management. First, four different types of production safety management paradigms were identified, namely small-data-based, static-oriented, interpretation-based and causal-oriented paradigm, and the challenges to these paradigms in the presence of big data were introduced. Second, the opportunities of employing big data in production safety management were identified from four aspects, including better predict the future production safety phenomena, promote production safety management highlight relevance, achieve the balance between deductive and inductive approaches and promote the interdisciplinary development of production safety management. Third, the paradigm shifting trend of production safety management was concluded, and the discipline foundation of the new paradigm was considered as the integration of data science, production management and safety science. Fourth, a new big-data-driven production safety management paradigm was developed, which consists of the logical line of production safety management, the macro-meso-micro data spectrum, the key big data analytics, and the four-dimensional morphology. At last, the strengths (e.g., supporting better-informed safety description, safety inquisition, safety prediction) and future research direction (e.g., theory research focuses on safety-related data mining/capturing/cleansing) of the new paradigm were discussed. The research results not only can provide theoretical and practical basis for big-data-driven production safety management, but also can offer advice to managerial consideration and scholarly investigation.}
}
@article{SELLAMI2020102732,
title = {On the use of big data frameworks for big service composition},
journal = {Journal of Network and Computer Applications},
volume = {166},
pages = {102732},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102732},
url = {https://www.sciencedirect.com/science/article/pii/S108480452030206X},
author = {Mokhtar Sellami and Haithem Mezni and Mohand Said Hacid},
keywords = {Big data, Big service, Big service composition, Quality of big services, Fuzzy RCA, Spark},
abstract = {Over the last years, big data has emerged as a new paradigm for the processing and analysis of massive volumes of data. Big data processing has been combined with service and cloud computing, leading to a new class of services called “Big Services”. In this new model, services can be seen as an abstract layer that hides the complexity of the processed big data. To meet users' complex and heterogeneous needs in the era of big data, service reuse is a natural and efficient means that helps orchestrating available services' operations, to provide customer on-demand big services. However different from traditional Web service composition, composing big services refers to the reuse of, not only existing high-quality services, but also high-quality data sources, while taking into account their security constraints (e.g., data provenance, threat level and data leakage). Moreover, composing heterogeneous and large-scale data-centric services faces several challenges, apart from security risks, such as the big services' high execution time and the incompatibility between providers' policies across multiple domains and clouds. Aiming to solve the above issues, we propose a scalable approach for big service composition, which considers not only the quality of reused services (QoS), but also the quality of their consumed data sources (QoD). Since the correct representation of big services requirements is the first step towards an effective composition, we first propose a quality model for big services and we quantify the data breaches using L-Severity metrics. Then to facilitate processing and mining big services' related information during composition, we exploit the strong mathematical foundation of fuzzy Relational Concept Analysis (fuzzy RCA) to build the big services' repository as a lattice family. We also used fuzzy RCA to cluster services and data sources based on various criteria, including their quality levels, their domains, and the relationships between them. Finally, we define algorithms that parse the lattice family to select and compose high-quality and secure big services in a parallel fashion. The proposed method, which is implemented on top of Spark big data framework, is compared with two existing approaches, and experimental studies proved the effectiveness of our big service composition approach in terms of QoD-aware composition, scalability, and security breaches.}
}
@article{LIU2020123646,
title = {Investment decision and coordination of green agri-food supply chain considering information service based on blockchain and big data},
journal = {Journal of Cleaner Production},
volume = {277},
pages = {123646},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.123646},
url = {https://www.sciencedirect.com/science/article/pii/S095965262033691X},
author = {Pan Liu and Yue Long and Hai-Cao Song and Yan-Dong He},
keywords = {Big data, Blockchain, Agri-food supply chain, Investment decision, Coordination},
abstract = {Researches about the fusion application of Big Data and blockchain have appeared for a long time, many information service providers have launched information service business based on Big Data and blockchain (hereafter, ISBD). However, in the green agri-food area, the ISBD application does not popularized. A vital reason is that many decision makers do not know how to make an optimal investment decision and coordinate chain members after adopting ISBD. The core of this problem is to study the issue of investment decision and coordination in a green agri-food supply chain. To solve this problem, firstly, combining with the status of Chinese agricultural development, we proposed a more suitable supply chain structure in the fusion application environment of Big Data and blockchain. Then, we chose a green agri-food supply chain with one producer and one retailer as research object and revised the demand function. Afterwards, considering the changes of agri-food freshness and greenness, we built and analysed the benefit models of producer and retailer before and after using ISBD, and then a cost-sharing and revenue-sharing contract was put forward to coordinate the supply chain. Findings: 1) When the total investment cost payed by producer and retailer is in a certain range, using ISBD will help chain members gain more benefits. 2) If chain members want to gain more benefits after using ISBD, they should try their best to optimize costs by extracting valuable information. Results can offer a theoretical guidance for producer and retailer in investing in ISBD, pricing decision and supply chain coordination after applying ISBD.}
}
@article{XIANG2020106538,
title = {Dynamic game strategies of a two-stage remanufacturing closed-loop supply chain considering Big Data marketing, technological innovation and overconfidence},
journal = {Computers & Industrial Engineering},
volume = {145},
pages = {106538},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106538},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220302722},
author = {Zehua Xiang and Minli Xu},
keywords = {Supply chain management, Big Data marketing, Technological innovation, Closed-loop supply chain, Overconfidence},
abstract = {In the “Internet+” era, involving third-party Internet recycling platforms (IRPs) has revolutionized the operation models of closed-loop supply chains (CLSCs) in China. This study explores the impact of technological innovation, Big Data marketing and overconfidence on supply chain member decision-making. We propose a two-stage remanufacturing CLSC dynamic model consisting of a manufacturer, an IRP, and a supplier based on differential game theory. By comparing the optimal decisions of each member in three scenarios, we find that the IRP’s overconfident behavior is beneficial to both the manufacturer and the IRP but will damage the supplier's profit. Although a suitable cost-sharing ratio can enable the manufacturer and IRP to achieve a “win–win” situation, an excessive level of confidence will inhibit the incentives of the cost-sharing strategy, negatively affecting the manufacturer's interests. Interestingly, a cost-sharing contract will become inefficient under certain conditions, i.e., highly efficient level of technological innovation, highly efficient Big Data marketing, and a high level of overconfidence, negatively affecting the manufacturer’s interests. Additionally, technological innovation efficiency and marketing efficiency will have different effects on the IRP's recycling price. A cost-sharing contract and the IRP’s overconfidence will prompt the IRP to exert more efforts on technological innovation and Big Data marketing and to significantly reduce the manufacturing costs and recycling costs for all members. Notably, although the IRP’s overconfidence and cost-sharing strategies may damage the supplier’s profit, the total profit of the CLSC increases.}
}
@article{KONG2020123142,
title = {A systematic review of big data-based urban sustainability research: State-of-the-science and future directions},
journal = {Journal of Cleaner Production},
volume = {273},
pages = {123142},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.123142},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620331875},
author = {Lingqiang Kong and Zhifeng Liu and Jianguo Wu},
keywords = {Big data, Social media data, Urban landscape sustainability, Smart city, Urban planning},
abstract = {The future of humanity depends increasingly on the performance of cities. Big data provide new and powerful ways of studying and improving coupled urban environmental, social, and economic systems to achieve urban sustainability. However, the term big data has been defined variably, and its urban applications have so far been sporadic in terms of research topic and location. A comprehensive review of big data-based urban environment, society, and sustainability (UESS) research is much needed. The aim of this study was to summarize the big data-based UESS research using a systematic review approach in combination with bibliometric and thematic analyses. The results showed that the numbers of publications and citations of related articles have been increasing exponentially in recent years. The most frequently used big data in UESS research are human behavior data, and the major analytical methods are of five types: classification, clustering, regression, association rules, and social network analysis. The major research topics of big data-based UESS research include urban mobility, urban land use and planning, environmental sustainability, public health and safety, social equity, tourism, resources and energy utilization, real estate, and retail, accommodation and catering. Big data benefit UESS research by proving a people-oriented perspective, timely and real-time information, and fine-resolution spatial dynamics. In addition, several obstacles were identified to applying big data in UESS research, which are related to data quality and acquisition, data storage and management, data security and privacy, data cleaning and preprocessing, and data analysis and information mining. To move forward, future research should integrate multiple big data sources, develop and utilize new methods such as deep learning and cloud computing, and expand the application fields to focus on the interactions between human activities and urban environments. This review can contribute to understanding the current situation of big data-based UESS research, and provide a reference for studies of this topic in the future.}
}
@article{LIU202053,
title = {Semantic-aware data quality assessment for image big data},
journal = {Future Generation Computer Systems},
volume = {102},
pages = {53-65},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.07.063},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19302304},
author = {Yu Liu and Yangtao Wang and Ke Zhou and Yujuan Yang and Yifei Liu},
keywords = {Semantic-aware, Quality assessment, Image big data, IDSTH, SHR},
abstract = {Data quality (DQ) assessment is essential for realizing the promise of big data by judging the value of data in advance. Relevance, an indispensable dimension of DQ, focusing on “fitness for requirement”, can arouse the user’s interest in exploiting the data source. It has two-level evaluations: (1) the amount of data that meets the user’s requirements; (2) the matching degree of these relevant data. However, there lack works of DQ assessment at dimension of relevance, especially for unstructured image data which focus on semantic similarity. When we try to evaluate semantic relevance between an image data source and a query (requirement), there are three challenges: (1) how to extract semantic information with generalization ability for all image data? (2) how to quantify relevance by fusing the quantity of relevant data and the degree of similarity comprehensively? (3) how to improve assessing efficiency of relevance in a big data scenario by design of an effective architecture? To overcome these challenges, we propose a semantic-aware data quality assessment (SDQA) architecture which includes off-line analysis and on-line assessment. In off-line analysis, for an image data source, we first transform all images into hash codes using our improved Deep Self-taught Hashing (IDSTH) algorithm which can extract semantic features with generalization ability, then construct a graph using hash codes and restricted Hamming distance, next use our designed Semantic Hash Ranking (SHR) algorithm to calculate the importance score (rank) for each node (image), which takes both the quantity of relevant images and the degree of semantic similarity into consideration, and finally rank all images in descending order of score. During on-line assessment, we first convert the user’s query into hash codes using IDSTH model, then retrieve matched images to collate their importance scores, and finally help the user determine whether the image data source is fit for his requirement. The results on public dataset and real-world dataset show effectiveness, superiority and on-line efficiency of our SDQA architecture.}
}
@article{AMANULLAH2020495,
title = {Deep learning and big data technologies for IoT security},
journal = {Computer Communications},
volume = {151},
pages = {495-517},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419315361},
author = {Mohamed Ahzam Amanullah and Riyaz Ahamed Ariyaluran Habeeb and Fariza Hanum Nasaruddin and Abdullah Gani and Ejaz Ahmed and Abdul Salam Mohamed Nainar and Nazihah Md Akim and Muhammad Imran},
keywords = {Deep learning, Big data, IoT security},
abstract = {Technology has become inevitable in human life, especially the growth of Internet of Things (IoT), which enables communication and interaction with various devices. However, IoT has been proven to be vulnerable to security breaches. Therefore, it is necessary to develop fool proof solutions by creating new technologies or combining existing technologies to address the security issues. Deep learning, a branch of machine learning has shown promising results in previous studies for detection of security breaches. Additionally, IoT devices generate large volumes, variety, and veracity of data. Thus, when big data technologies are incorporated, higher performance and better data handling can be achieved. Hence, we have conducted a comprehensive survey on state-of-the-art deep learning, IoT security, and big data technologies. Further, a comparative analysis and the relationship among deep learning, IoT security, and big data technologies have also been discussed. Further, we have derived a thematic taxonomy from the comparative analysis of technical studies of the three aforementioned domains. Finally, we have identified and discussed the challenges in incorporating deep learning for IoT security using big data technologies and have provided directions to future researchers on the IoT security aspects.}
}
@article{SEDLMAYR202081,
title = {Evaluation eines Zukunftsszenarios zur Nutzung von Big-Data-Anwendungen für die Verbesserung der Versorgung von Menschen mit seltenen Erkrankungen},
journal = {Zeitschrift für Evidenz, Fortbildung und Qualität im Gesundheitswesen},
volume = {158-159},
pages = {81-91},
year = {2020},
issn = {1865-9217},
doi = {https://doi.org/10.1016/j.zefq.2020.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S1865921720301744},
author = {Brita Sedlmayr and Andreas Knapp and Michéle Kümmel and Franziska Bathelt and Martin Sedlmayr},
keywords = {Evaluation, Akzeptanz, Nutzen, Barrieren, Befragung, Szenario, Big Data, Seltene Erkrankungen, Evaluation, Acceptance, Benefits, Barriers, Survey, Scenario, Big data, Rare diseases},
abstract = {Zusammenfassung
Hintergrund
In Deutschland leben etwa 4 Millionen Menschen mit einer seltenen Erkrankung. Einzelne Studien belegen bereits, dass mit Hilfe von Big Data Verfahren die Diagnostik verbessert und seltene Erkrankungen effektiver erforscht werden können. Deutschlandweit existiert aber bisher kein konkretes, umfassendes Konzept für den Einsatz von Big Data zur Versorgung von Menschen mit seltenen Erkrankungen. Im Rahmen des BMG-geförderten Projekts „BIDA-SE“ wurde ein erstes Szenario entworfen, wie Big-Data-basierte Anwendungen sinnvoll in der Versorgungspraxis von Menschen mit seltenen Erkrankungen einfließen können.
Methode
Ziel der vorliegenden Studie war es, das entwickelte Szenario hinsichtlich der Akzeptanz, des (klinischen) Nutzens, ökonomischer Implikationen sowie Grenzen und Barrieren für dessen mittelfristige Umsetzung zu evaluieren. Für die Bewertung des Szenarios wurde im Zeitraum Oktober-November 2019 eine Online-Befragung innerhalb Deutschlands mit insgesamt N = 9 Ärzt*innen, N = 69 Patient*innen mit seltenen Erkrankungen/Patientenvertreter*innen, N = 14 IT-Expert*innen und N = 21 Versorgungsforscher*innen durchgeführt. Für die Entwicklung des Online-Fragebogens wurden standardisierte, validierte Fragen bereits erprobter Erhebungsinstrumente eingesetzt und eigene Fragen auf Basis einer vorausgegangenen Literaturanalyse konstruiert. Die Auswertung der Befragung erfolgte primär deskriptiv durch eine Analyse von Häufigkeiten, Mittelwerten und Standardabweichungen.
Ergebnisse
Die Ergebnisse zeigen, dass das entwickelte Szenario von allen befragten Gruppen (Ärzt*innen, Patient*innen/Patientenvertreter*innen, IT-Expert*innen und Versorgungsforscher*innen) mehrheitlich Akzeptanz erfährt. Aus Sicht der Ärzt*innen, Patient*innen/Patientenvertreter*innen und Versorgungsforscher*innen hätte das Szenario das Potential, die Diagnosestellung und Therapieeinleitung zu beschleunigen und die sektorenübergreifende Behandlung zu verbessern. Investitionen in die vorgestellte Anwendung würden sich aus Sicht der Ärzt*innen und Versorgungsforscher*innen rentieren. Zur Finanzierung des vorgestellten Szenarios müsste jedoch eine Anpassung der Vergütungssituation erfolgen. Die von allen Gruppen benannten Grenzen und Barrieren für eine mittelfristige Umsetzung des Szenarios lassen sich in sieben Themenfelder mit Handlungsbedarf gruppieren: (1) Finanzierung und Investition, (2) Datenschutz und Datensicherheit, (3) Standards/Datenquellen/Datenqualität, (4) Technologieakzeptanz, (5) Integration in den Arbeitsalltag, (6) Wissen um Verfügbarkeit sowie (7) Gewohnheiten und Präferenzen/Arztrolle.
Diskussion
Mit der vorliegenden Studie wurde ein erstes fachübergreifendes, praxisnahes Szenario unter Nutzung von Big-Data-basierten Anwendungen hinsichtlich Akzeptanz, Nutzen und Grenzen/Barrieren evaluiert und analysiert, inwiefern dieses Szenario zukünftig im Kontext seltener Erkrankungen implementiert werden kann. Das Szenario erfährt von allen befragten Zielgruppen mehrheitlich eine hohe Akzeptanz und wird mehrheitlich als (klinisch) nützlich bewertet, wenngleich noch rechtliche, organisatorische und technische Barrieren für dessen mittelfristige Umsetzung überwunden werden müssen. Die Evaluationsergebnisse tragen dazu bei, Handlungsempfehlungen abzuleiten, um eine mittelfristige Umsetzung des Szenarios zu gewährleiten und den Zugang zu den Zentren für Seltene Erkrankungen zukünftig zu kanalisieren.
Schlussfolgerung
Auf nationaler Ebene wurden zahlreiche Aktivitäten angestoßen, um die Versorgungssituation von Menschen mit seltenen Erkrankungen zu verbessern. Das im Rahmen des Projekts „BIDA-SE“ entwickelte Szenario ergänzt diese Forschungsaktivitäten und verdeutlicht, wie Big-Data-basierte Anwendungen sinnvoll in der Praxis genutzt werden können, um die Diagnosestellung und Therapie von Menschen mit seltenen Erkrankungen nachhaltig verbessern zu können.
Introduction
In Germany there are about 4 million people living with a rare disease. Studies have shown that big data applications can improve diagnosis of and research on rare diseases more effectively. However, no concrete comprehensive concept for the use of big data in the care of people with rare diseases has so far been established in Germany. As part of the project “BIDA-SE”, which is funded by the German Ministry of Health, a first scenario has been designed to show how big data applications can be usefully incorporated into the care of people with rare diseases.
Methods
The aim of the present study was to evaluate this scenario with regard to acceptance, (clinical) benefits, economic aspects, and limitations and barriers to its implementation. To evaluate the scenario, an online survey was conducted in Germany in October/November 2019 amongst a total of N = 9 physicians, N = 69 patients with rare diseases/patient representatives, N = 14 IT experts and N = 21 health care researchers. The online questionnaire consisted of both standardized, validated questions taken from already tested survey instruments and additional questions which were constructed on the basis of a preceding literature analysis. The evaluation of the survey was primarily descriptive, with a calculation of frequencies, mean values and standard deviations.
Results
The results of the evaluation show that the scenario has been accepted by a majority of all groups surveyed (physicians, patients/patient representatives, IT experts and health care researchers). From the point of view of physicians, patients/patient representatives and health care researchers, the scenario has the potential to accelerate the diagnosis and initiation of therapy and to improve cross-sectoral treatment. From the physician’s and health care researcher’s perspective, investments in the application presented in the scenario would be profitable. Financing the scenario would, however, require adjusting the reimbursement situation. The limitations and barriers identified by all groups for a medium-term implementation of the scenario can be grouped into seven thematic areas where action is needed: (1) financing and investment, (2) data protection and data security, (3) standards/data sources/data quality, (4) acceptance of technology, (5) integration into the daily work routine, (6) knowledge about availability as well as (7) habits and preferences/physician's role.
Discussion
With the present study, a first interdisciplinary, practical scenario using big data applications was evaluated with regard to acceptance, benefits and limitations/barriers. The scenario is widely accepted among the majority of all surveyed target groups and is considered (clinically) useful, although legal, organisational and technical barriers still need to be overcome for its medium-term implementation. The evaluation results contribute to the derivation of recommendations for action to ensure the medium-term implementation of the scenario and to channel access to the Centres for Rare Diseases in the future.
Conclusion
Many activities have been initiated at a national level to improve the health care situation of people with rare diseases. The scenario developed in the “BIDA-SE” project complements these research activities and illustrates how big data applications can be usefully implemented into practice to improve the diagnosis and therapy of people with rare diseases in a sustainable way.}
}
@article{YADEGARIDEHKORDI2020100921,
title = {The impact of big data on firm performance in hotel industry},
journal = {Electronic Commerce Research and Applications},
volume = {40},
pages = {100921},
year = {2020},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2019.100921},
url = {https://www.sciencedirect.com/science/article/pii/S1567422319300985},
author = {Elaheh Yadegaridehkordi and Mehrbakhsh Nilashi and Liyana Shuib and Mohd {Hairul Nizam Bin Md Nasir} and Shahla Asadi and Sarminah Samad and Nor {Fatimah Awang}},
keywords = {Firm performance, Big data, Hotel industry, Fuzzy logic, Structural equation modelling},
abstract = {Big data has increasingly appeared as a frontier of opportunity in enhancing firm performance. However, it still is in early stages of introduction and many enterprises are still un-decisive in its adoption. The aim of this study is to propose a theoretical model based on integration of Human-Organization-Technology fit and Technology-Organization-Environment frameworks to identify the key factors affecting big data adoption and its consequent impact on the firm performance. The significant factors are gained from the literature and the research model is developed. Data was collected from top managers and/or owners of SMEs hotels in Malaysia using online survey questionnaire. Structural Equation Modelling (SEM) is used to assess the developed model and Adaptive Neuro-Fuzzy Inference Systems (ANFIS) technique is used to prioritize adoption factors based on their importance levels. The results showed that relative advantage, management support, IT expertise, and external pressure are the most important factors in the technological, organizational, human, and environmental dimensions. The results further revealed that technology is the most important influential dimension. The outcomes of this study can assist the policy makers, businesses and governments to make well-informed decisions in adopting big data.}
}
@article{MARIANI2020338,
title = {Exploring how consumer goods companies innovate in the digital age: The role of big data analytics companies},
journal = {Journal of Business Research},
volume = {121},
pages = {338-352},
year = {2020},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320305956},
author = {Marcello M. Mariani and Samuel {Fosso Wamba}},
keywords = {Big data analytics, Forecasting, Innovation, Online review crowdsourcing, Consumer goods companies, Digital data},
abstract = {The advent and development of digital technologies have brought about a proliferation of online consumer reviews (OCRs), i.e., real-time customers’ evaluations of products, services, and brands. Increasingly, e-commerce platforms are using them to gain insights from customer feedback. Meanwhile, a new generation of big data analytics (BDA) companies are crowdsourcing large volumes of OCRs by means of controlled ad hoc online experiments and advanced machine learning (ML) techniques to forecast demand and determine the market potential for new products in several industries. We illustrate how this process is taking place for consumer goods companies by exploring the case of UK digital BDA company, SoundOut. Based on an in-depth qualitative analysis, we develop the consumer goods company innovation (CGCI) conceptual framework, which illustrates how digital BDA firms help consumer goods companies to test new products before they are launched on the market, and innovate. Theoretical and managerial implications are discussed.}
}
@article{LIOUTAS2019100297,
title = {Key questions on the use of big data in farming: An activity theory approach},
journal = {NJAS - Wageningen Journal of Life Sciences},
volume = {90-91},
pages = {100297},
year = {2019},
issn = {1573-5214},
doi = {https://doi.org/10.1016/j.njas.2019.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1573521418302197},
author = {Evagelos D. Lioutas and Chrysanthi Charatsari and Giuseppe {La Rocca} and Marcello {De Rosa}},
keywords = {Big data, Smart farming, Value, Farmers, Cyber-physical-social systems, Activity theory},
abstract = {Big data represent a pioneering development in the field of agriculture. By producing intuition, intelligence, and insights, these data have the potential to recast conventional process-driven agriculture, plotting the course for a smarter, data-driven farming. However, many open issues about the use of big data in agriculture remain unanswered. In this work, conceptualizing smart agricultural systems as cyber-physical-social systems, and building upon activity theory, we aim at highlighting some key questions that need to be addressed. To our view, big data constitute a tool reciprocally produced by all the actors involved in the agrifood supply chains. The constant flux of this tool and the intricate nature of the interactions among the actors who share it complicate the translation of big data into value. Moreover, farmers’ limited capacity to deal with data complexity, along with their dual role as producers and users of big data, impedes the institutionalization of this tool at the farm level. Although the approach used left us with more questions than answers, we suggest that unraveling the institutional arrangements that govern value co-creation, capturing the motivations of farmers and other actors, and detailing the direct and indirect effects that big data (and the technologies used to generate them) have in farms are important preconditions for setting forth rules that facilitate the extraction and equal exchange of value from big data.}
}
@article{LI2019393,
title = {Functional Neuroimaging in the New Era of Big Data},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {17},
number = {4},
pages = {393-401},
year = {2019},
note = {Big Data in Brain Science},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S1672022919301603},
author = {Xiang Li and Ning Guo and Quanzheng Li},
keywords = {Big data, Neuroimaging, Machine learning, Health informatics, fMRI},
abstract = {The field of functional neuroimaging has substantially advanced as a big data science in the past decade, thanks to international collaborative projects and community efforts. Here we conducted a literature review on functional neuroimaging, with focus on three general challenges in big data tasks: data collection and sharing, data infrastructure construction, and data analysis methods. The review covers a wide range of literature types including perspectives, database descriptions, methodology developments, and technical details. We show how each of the challenges was proposed and addressed, and how these solutions formed the three core foundations for the functional neuroimaging as a big data science and helped to build the current data-rich and data-driven community. Furthermore, based on our review of recent literature on the upcoming challenges and opportunities toward future scientific discoveries, we envisioned that the functional neuroimaging community needs to advance from the current foundations to better data integration infrastructure, methodology development toward improved learning capability, and multi-discipline translational research framework for this new era of big data.}
}
@article{VIEIRA2020132,
title = {Bypassing Data Issues of a Supply Chain Simulation Model in a Big Data Context},
journal = {Procedia Manufacturing},
volume = {42},
pages = {132-139},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.033},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920305825},
author = {António A.C. Vieira and Luís Dias and Maribel Y. Santos and Guilherme A.B. Pereira and José Oliveira},
keywords = {Simulation, Supply Chain, Big Data, Data issues, Industry 4.0},
abstract = {Supply Chains (SCs) are complex and dynamic networks, where certain events may cause severe problems. To avoid them, simulation can be used, allowing the uncertainty of these systems to be considered. Furthermore, the data that is generated at increasingly high volumes, velocities and varieties by relevant data sources allow, on one hand, the simulation model to capture all the relevant elements. While developing such solution, due to the inherent use of simulation, several data issues were identified and bypassed, so that the incorporated elements comprise a coherent SC simulation model. Thus, the purpose of this paper is to present the main issues that were faced, and discuss how these were bypassed, while working on a SC simulation model in a Big Data context and using real industrial data from an automotive electronics SC. This paper highlights the role of simulation in this task, since it worked as a semantic validator of the data. Moreover, this paper also presents the results that can be obtained from the developed model.}
}
@article{MCNUTT2019326,
title = {Use of Big Data for Quality Assurance in Radiation Therapy},
journal = {Seminars in Radiation Oncology},
volume = {29},
number = {4},
pages = {326-332},
year = {2019},
note = {Big Data in Radiation Oncology},
issn = {1053-4296},
doi = {https://doi.org/10.1016/j.semradonc.2019.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1053429619300384},
author = {Todd R. McNutt and Kevin L. Moore and Binbin Wu and Jean L. Wright},
abstract = {The application of big data to the quality assurance of radiation therapy is multifaceted. Big data can be used to detect anomalies and suboptimal quality metrics through both statistical means and more advanced machine learning and artificial intelligence. The application of these methods to clinical practice is discussed through examples of guideline adherence, contour integrity, treatment delivery mechanics, and treatment plan quality. The ultimate goal is to apply big data methods to direct measures of patient outcomes for care quality. The era of big data and machine learning is maturing and the implementation for quality assurance promises to improve the quality of care for patients.}
}