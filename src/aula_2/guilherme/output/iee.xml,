<?xml version='1.0' encoding='utf-8'?>
<data>
  <row>
    <author>Chen, Bingquan and Nie, Guojian and Jiang, Shixin and Hu, Ning</author>
    <title>Research on the Big Data-based Product Quality Data Package Construction and Application</title>
    <keywords>Information science;Data integration;Companies;Reliability theory;Big Data;Product design;Quality assessment;quality data package;big data;intelligent manufacturing;data fusion</keywords>
    <abstract>In the new environment of intelligent manufacturing, enterprise quality data has increased exponentially. How to manage, utilize, mine and analyze quality data has become a key issue in modern quality management. This article expands the definition of the product quality data package in the intelligent manufacturing environment, and proposes a big data-based product quality data package construction and management solution, gives a quality data fusion method based on business decision, outlines the application of quality data package. Finally, a chip manufacturing company was used to verify the feasibility of the product quality data package construction and management plan.</abstract>
    <year>2022</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/CTISC54888.2022.9849793</doi>
  </row>
  <row>
    <author>Liu, He and Huang, Fupeng and Li, Han and Liu, Weiwei and Wang, Tongxun</author>
    <title>A Big Data Framework for Electric Power Data Quality Assessment</title>
    <keywords>Big Data;Data integrity;Power grids;History;Real-time systems;Sensors;data quality;electric power data;data quality assessment;big data;framework</keywords>
    <abstract>Since a low-quality data may influence the effectiveness and reliability of applications, data quality is required to be guaranteed. Data quality assessment is considered as the foundation of the promotion of data quality, so it is essential to access the data quality before any other data related activities. In the electric power industry, more and more electric power data is continuously accumulated, and many electric power applications have been developed based on these data. In China, the power grid has many special characteristic, traditional big data assessment frameworks cannot be directly applied. Therefore, a big data framework for electric power data quality assessment is proposed. Based on big data techniques, the framework can accumulate both the real-time data and the history data, provide an integrated computation environment for electric power big data assessment, and support the storage of different types of data.</abstract>
    <year>2017</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/WISA.2017.29</doi>
  </row>
  <row>
    <author>Taleb, Ikbal and Serhani, Mohamed Adel</author>
    <title>Big Data Pre-Processing: Closing the Data Quality Enforcement Loop</title>
    <keywords>Big Data;Optimization;Data models;Quality assessment;Big Data;Data Quality Evaluation;Data Quality Rules Discovery;Big Data Pre-Processing</keywords>
    <abstract>In the Big Data Era, data is the core for any governmental, institutional, and private organization. Efforts were geared towards extracting highly valuable insights that cannot happen if data is of poor quality. Therefore, data quality (DQ) is considered as a key element in Big data processing phase. In this stage, low quality data is not penetrated to the Big Data value chain. This paper, addresses the data quality rules discovery (DQR) after the evaluation of quality and prior to Big Data pre-processing. We propose a DQR discovery model to enhance and accurately target the pre-processing activities based on quality requirements. We defined, a set of pre-processing activities associated with data quality dimensions (DQD's) to automatize the DQR generation process. Rules optimization are applied on validated rules to avoid multi-passes pre-processing activities and eliminates duplicate rules. Conducted experiments showed an increased quality scores after applying the discovered and optimized DQR's on data.</abstract>
    <year>2017</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigDataCongress.2017.73</doi>
  </row>
  <row>
    <author>Loetpipatwanich, Sakda and Vichitthamaros, Preecha</author>
    <title>Sakdas: A Python Package for Data Profiling and Data Quality Auditing</title>
    <keywords>Data integrity;Pipelines;Data visualization;Big Data;Syntactics;Software;Python;Data Quality Management;Data Profiling;Data Quality Auditing;Python Package;Data Pipeline</keywords>
    <abstract>Data Profiling and data quality management become a more significant part of data engineering, which an essential part of ensuring that the system delivers quality information to users. In the last decade, data quality was considered to need more managing. Especially in the big data era that the data comes from many sources, many data types, and an enormous amount. Thus it makes the managing of data quality is more difficult and complicated. The traditional system was unable to respond as needed. The data quality managing software for big data was developed but often found in a high-priced, difficult to customize as needed, and mostly provide as GUI, which is challenging to integrate with other systems. From this problem, we have developed an opensource package for data quality managing. By using Python programming language, Which is a programming language that is widely used in the scientific and engineering field today. Because it is a programming language that is easy to read syntax, small, and has many additional packages to integrate. The software developed here is called “Sakdas” this package has been divided into three parts. The first part deals with data profiling provide a set of data analyses to generate a data profile, and this profile will help to define the data quality rules. The second part deals with data quality auditing that users can set their own data quality rules for data quality measurement. The final part deals with data visualizing that provides data profiling and data auditing report to improve the data quality. The results of the profiling and auditing services, the user can specify both the form of a report for self-review. Or in the form of JSON for use in post-process automation.</abstract>
    <year>2020</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/IBDAP50342.2020.9245455</doi>
  </row>
  <row>
    <author>HongJu, Xiao and Fei, Wang and FenMei, Wang and XiuZhen, Wang</author>
    <title>Some key problems of data management in army data engineering based on big data</title>
    <keywords>Data engineering;Data analysis;Data integration;Big Data;Distributed databases;Data models;Uncertainty;army data engineering;data management;data integration;data analysis;representation of data analysis results;data quality</keywords>
    <abstract>This paper analyzed the challenges of data management in army data engineering, such as big data volume, data heterogeneous, high rate of data generation and update, high time requirement of data processing, and widely separated data sources. We discussed the disadvantages of traditional data management technologies to deal with these problems. We also highlighted the key problems of data management in army data engineering including data integration, data analysis, representation of data analysis results, and evaluation of data quality.</abstract>
    <year>2017</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICBDA.2017.8078796</doi>
  </row>
  <row>
    <author>Peng, Zhibin and Chen, Yuefeng and Zhang, Zehong and Qiu, Queling and Han, Xiaoqiang</author>
    <title>Implementation of Water Quality Management Platform for Aquaculture Based on Big Data</title>
    <keywords>Data visualization;Aquaculture;Data mining;Big Data;Neural networks;Data models;Predictive models;aquaculture;big data;water quality warning;data visualization</keywords>
    <abstract>In order to ensure the quality and quantity of aquaculture, aquaculture farmers need to grasp the water quality in time. However, most farmers have to collect water quality data manually at present, and cannot store and reuse that information rapidly. This paper aims to use SpringBoot framework and JPA framework to build a big data platform of acquisition automation and visualization, which realizes the data analysis and display of heterogeneous water quality and breeding information. The platform can make the water quality prediction and real-time warning. Meanwhile, it realizes the management of robots, users and breeding experts. The application of this platform will bring better social benefits to aquaculture farmers.</abstract>
    <year>2020</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/CIBDA50819.2020.00024</doi>
  </row>
  <row>
    <author>Patel, Jayesh</author>
    <title>An Effective and Scalable Data Modeling for Enterprise Big Data Platform</title>
    <keywords>Data models;Big Data;Business;Analytical models;Computational modeling;Lakes;Solid modeling;Big Data;Big Data Lake;Scalable Data Modeling;Hadoop;Spark;Business Intelligence;Big Data Analytics</keywords>
    <abstract>The enormous growth of the internet, enterprise applications, social media, and IoT devices in the current time caused a huge spike in enterprise data growth. Big data platform provided scalable storage to manage enterprise data growth and served easier data access to decision-makers, stakeholders and business users. It is a well-known challenge to classify, organize and store all this data and process it to provide business insights. Due to nature, variety, velocity, volume and value of data make it difficult to effectively process big data. Enterprises face challenges to apply complex business rules, to generate insights and to support data-driven decisions in a timely fashion. As big data lake integrates streams of data from a bunch of business units, stakeholders usually analyze enterprise-wide data from various data models. Data models are a vital component of Big data platform. Users may do complex processing, run queries and perform big table joins to generate required metrics depending on the available data models. It is usually a time consuming and resource-intensive process to find the value from data. It is a no-brainer that big data platform in the enterprise needs high-quality data modeling methods to reach an optimal mix of cost, performance, and quality. This paper addresses these challenges by proposing an effective and scalable way to organize and store data in Big Data Lake. It presents some of the basic principles and methodology to build scalable data models in a distributed environment. It also describes how it overcomes common challenges and presents findings.</abstract>
    <year>2019</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData47090.2019.9005614</doi>
  </row>
  <row>
    <author>Taleb, Ikbal and Serhani, Mohamed Adel and Dssouli, Rachida</author>
    <title>Big Data Quality Assessment Model for Unstructured Data</title>
    <keywords>Big Data;Data integrity;Data mining;Feature extraction;Data models;Measurement;Quality assessment;Big Data;Data Quality;Unstructured Data;Quality of Unstructured Big Data</keywords>
    <abstract>Big Data has gained an enormous momentum the past few years because of the tremendous volume of generated and processed Data from diverse application domains. Nowadays, it is estimated that 80% of all the generated data is unstructured. Evaluating the quality of Big data has been identified to be essential to guarantee data quality dimensions including for example completeness, and accuracy. Current initiatives for unstructured data quality evaluation are still under investigations. In this paper, we propose a quality evaluation model to handle quality of Unstructured Big Data (UBD). The later captures and discover first key properties of unstructured big data and its characteristics, provides some comprehensive mechanisms to sample, profile the UBD dataset and extract features and characteristics from heterogeneous data types in different formats. A Data Quality repository manage relationships between Data quality dimensions, quality Metrics, features extraction methods, mining methodologies, data types and data domains. An analysis of the samples provides a data profile of UBD. This profile is extended to a quality profile that contains the quality mapping with selected features for quality assessment. We developed an UBD quality assessment model that handles all the processes from the UBD profiling exploration to the Quality report. The model provides an initial blueprint for quality estimation of unstructured Big data. It also, states a set of quality characteristics and indicators that can be used to outline an initial data quality schema of UBD.</abstract>
    <year>2018</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/INNOVATIONS.2018.8605945</doi>
  </row>
  <row>
    <author>Arruda, Darlan and Madhavji, Nazim H.</author>
    <title>QualiBD: A Tool for Modelling Quality Requirements for Big Data Applications</title>
    <keywords>Tools;Data models;Containers;Software;Real-time systems;Big Data applications;Big Data Applications;Quality Requirements;Big Data Goal-oriented Requirements Language;Requirements Modelling Tool</keywords>
    <abstract>The development of Big Data applications is not well-explored, to our knowledge. Embracing Big Data in system building, questions arise as to how to elicit, specify, analyse, model, and document Big Data quality requirements. In our ongoing research, we explore a requirements modelling language for Big Data software applications. In this paper, we introduce QualiBD, a modelling tool that implements the proposed goal-oriented requirements language that facilitates the modelling of Big Data quality requirements.</abstract>
    <year>2019</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData47090.2019.9006294</doi>
  </row>
  <row>
    <author>Norman, Ryan and Bolin, Jason and Powell, Edward T. and Amin, Sanket and Nacker, John</author>
    <title>Using Big Data Analytics to Create a Predictive Model for Joint Strike Fighter</title>
    <keywords>Big Data;Knowledge management;Tools;US Department of Defense;Cloud computing;Computer architecture;Data analysis;Big Data;Data Analytics;Knowledge Management;Data Management;Virtualization;Cloud Computing;Predictive Maintainance;Department of Defense;Test and Evaluation</keywords>
    <abstract>The amount of information needed to acquire knowledge on today's acquisition systems is growing exponentially due to more complex, higher resolution, software-intensive acquisition systems that need to operate in System-of-Systems (SoS), Family-of-Systems (FoS), Joint, and Coalition environments. Unfortunately, the tools and methods necessary to rapidly collect, aggregate, and analyze this information have not evolved as a whole in conjunction with this increased system complexity and, therefore, has made analysis and evaluation increasingly deficient and ineffective. The Test Resource Management Center's (TRMC's) vision is to build a DoD test and evaluation (T&amp;E) knowledge management (KM) and analysis capability that leverages commercial big data analysis and cloud computing technologies to improve evaluation quality and reduce decision-making time. An evaluation revolution, starting with the Joint Strike Fighter (JSF) program, is underway to ensure the T&amp;E community can support the demands of next-generation weapon systems.The true product of T&amp;E is knowledge ascertained through the collection of information about a system or item under test. However, the T&amp;E community's ability to provide this knowledge is hampered by more complex systems, more complex environments, and the need to be more agile in support of strategic initiatives, such as agile acquisition and the 3rd Offset Strategy. This increased complexity and need for speed cause delayed analysis and problems that go undetected during T&amp;E. The primary reason for these shortfalls is antiquated tools and processes that make data hard to locate, aggregate, and convert into knowledge. In short, DoD has not evolved its evaluation infrastructure as its weapon systems have evolved.Conversely, commercial entities, such as medical observation and diagnosis, electric power distribution, retail, and industrial manufacturing, have embraced agility in their methodologies while modernizing analytics capabilities to keep up with the massive influx of data. Raw physical sensors could provide data, higher-quality image or video cameras, radio frequency identification (RFID) devices, faster data collectors, more detailed point-of-sale information or digitized records, and ultimately is providing more data to analysts in size and complexity than ever before. As more data has become available, an interrelated phenomenon is the desire of analysts to ask more detailed questions about their consumers and their business infrastructure. To drive the process of implementing big data analytics, businesses have begun establishing analytics centers which either take pre-defined business cases and apply methods to address them or implement existing knowledge within the data architecture to create a higher level of awareness to business groups or the company at-large. To meet these demands, data storage and computation architectures have become more sophisticated, dozens of technologies were developed for large-scale processing (such as Apache Hadoop or GreenPlum), and streaming architectures which allow data to be processed and actioned on in real-time as it is collected have become commonplace. The net result of these commercial best practices is a solid foundation for the DoD to transform how it uses data to achieve faster, better, and smarter decisions throughout the acquisition lifecycle.</abstract>
    <year>2018</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData.2018.8622388</doi>
  </row>
  <row>
    <author>Pan, Xing and Zhang, Manli and Chen, Xi</author>
    <title>A Method of Quality Improvement Based on Big Quality Warranty Data Analysis</title>
    <keywords>Warranties;Data mining;Product design;Quality assessment;Databases;Big Data;Reliability engineering;quality warranty data;big data analysis;association rules;quality improvement;PDCA</keywords>
    <abstract>Quality warranty data includes big data of product use and customer services, which is foundation of product quality and reliability improvement. This paper presents a method of quality warranty data analysis, which is based on the big data analysis technology. By means of the method of association rules mining, it distinguishes the association rules of failure modes while feeding back the information to the process of product design, production, and usage. To achieve product fault location and fault disposal, the key factors such as fault type and fault cause are analyzed. Meanwhile, this paper adopted the principles of PDCA circulation to propose a procedure of product quality improvement. The quality improvement procedure based on quality warranty data analysis provides a comprehensive and systematic quality improvement for different stages and different types of products. Finally, a case study of household appliances in China is given to illustrate the method.</abstract>
    <year>2018</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/QRS-C.2018.00115</doi>
  </row>
  <row>
    <author>Peng, Fang and Wang, Honggang and Zhuang, Li and Wang, Minnan and Yang, Chengyue</author>
    <title>Methods of enterprise electronic file content information mining under big data environment</title>
    <keywords>Scalability;Big Data;Information age;Search problems;Data mining;Electronic countermeasures;Software engineering;Big data environment;Enterprise electronic documents;Data mining</keywords>
    <abstract>As the product of the digital age, big data technology and computer information technology can greatly improve the efficiency and quality of file management and promote the development of enterprises. Based on this, this paper first analyzes the current status of enterprise archives management; Secondly, this paper discusses the countermeasures of information mining of electronic documents of innovative enterprises in the digital age. Text information mining is beneficial to improve the efficiency of text information search and utilization, aiming at the existing problems of traditional methods, the text information mining method is proposed.</abstract>
    <year>2020</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICBASE51474.2020.00008</doi>
  </row>
  <row>
    <author>Li, Mingda and Wang, Hongzhi and Li, Jianzhong</author>
    <title>Mining conditional functional dependency rules on big data</title>
    <keywords>Big Data;Training;Data mining;Cleaning;Sampling methods;Heuristic algorithms;Fault tolerance;data mining;conditional functional dependency;big data;data quality</keywords>
    <abstract>Current Conditional Functional Dependency (CFD) discovery algorithms always need a well-prepared training dataset. This condition makes them difficult to apply on large and low-quality datasets. To handle the volume issue of big data, we develop the sampling algorithms to obtain a small representative training set. We design the fault-tolerant rule discovery and conflict-resolution algorithms to address the low-quality issue of big data. We also propose parameter selection strategy to ensure the effectiveness of CFD discovery algorithms. Experimental results demonstrate that our method can discover effective CFD rules on billion-tuple data within a reasonable period.</abstract>
    <year>2020</year>
    <ENTRYTYPE>article</ENTRYTYPE>
    <doi>10.26599/BDMA.2019.9020019</doi>
  </row>
  <row>
    <author>Hongxun, Tian and Honggang, Wang and Kun, Zhou and Mingtai, Shi and Haosong, Li and Zhongping, Xu and Taifeng, Kang and Jin, Li and Yaqi, Cai</author>
    <title>Data quality assessment for on-line monitoring and measuring system of power quality based on big data and data provenance theory</title>
    <keywords>Data integrity;Power quality;Monitoring;Power measurement;Redundancy;Big Data;Business;power qualiy;data quality;big data;data provenance;data assessment</keywords>
    <abstract>Currently, on-line monitoring and measuring system of power quality has accumulated a huge amount of data. In the age of big data, those data integrated from various systems will face big data application problems. This paper proposes a data quality assessment system method for on-line monitoring and measuring system of power quality based on big data and data provenance to assess integrity, redundancy, accuracy, timeliness, intelligence and consistency of data set and single data. Specific assessment rule which conforms to the situation of on-line monitoring and measuring system of power quality will be devised to found data quality problems. Thus it will provide strong data support for big data application of power quality.</abstract>
    <year>2018</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICCCBDA.2018.8386521</doi>
  </row>
  <row>
    <author>O’Shea, Enda and Khan, Rafflesia and Breathnach, Ciara and Margaria, Tiziana</author>
    <title>Towards Automatic Data Cleansing and Classification of Valid Historical Data An Incremental Approach Based on MDD</title>
    <keywords>Java;Quality assurance;Semantics;Big Data;Tools;Syntactics;Cleaning;Data Collection;Data Analytics;Model-Driven Development;Historical Data;Data Parsing;Data Cleaning;Ethics;Data Assurance;Data Quality</keywords>
    <abstract>The project Death and Burial Data: Ireland 1864-1922 (DBDIrl) examines the relationship between historical death registration data and burial data to explore the history of power in Ireland from 1864 to 1922. Its core Big Data arises from historical records from a variety of heterogeneous sources, some aspects are pre-digitized and machine readable. A huge data set (over 4 million records in each source) and its slow manual enrichment (ca 7,000 records processed so far) pose issues of quality, scalability, and creates the need for a quality assurance technology that is accessible to non-programmers. An important goal for the researcher community is to produce a reusable, high-level quality assurance tool for the ingested data that is domain specific (historic data), highly portable across data sources, thus independent of storage technology.This paper outlines the step-wise design of the finer granular digital format, aimed for storage and digital archiving, and the design and test of two generations of the techniques, used in the first two data ingestion and cleaning phases.The first small scale phase was exploratory, based on metadata enrichment transcription to Excel, and conducted in parallel with the design of the final digital format and the discovery of all the domain-specific rules and constraints for the syntax and semantic validity of individual entries. Excel embedded quality checks or database-specific techniques are not adequate due to the technology independence requirement. This first phase produced a Java parser with an embedded data cleaning and evaluation classifier, continuously improved and refined as insights grew. The next, larger scale phase uses a bespoke Historian Web Application that embeds the Java validator from the parser, as well as a new Boolean classifier for valid and complete data assurance built using a Model-Driven Development technique that we also describe. This solution enforces property constraints directly at data capture time, removing the need for additional parsing and cleaning stages. The new classifier is built in an easy to use graphical technology, and the ADD-Lib tool it uses is a modern low-code development environment that auto-generates code in a large number of programming languages. It thus meets the technology independence requirement and historians are now able to produce new classifiers themselves without being able to program. We aim to infuse the project with computational and archival thinking in order to produce a robust data set that is FAIR compliant (Free Accessible Inter-operable and Re-useable).</abstract>
    <year>2020</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData50022.2020.9378148</doi>
  </row>
  <row>
    <author>Ning, Xiuli and Xu, Yingcheng and Gao, Xiaohong and Li, Ying</author>
    <title>Missing data of quality inspection imputation algorithm base on stacked denoising auto-encoder</title>
    <keywords>Filling;Algorithm design and analysis;Noise reduction;Training;Clustering algorithms;Inspection;Big Data;Big data of quality inspection;Stacked denoising auto-encoder;Filling algorithm</keywords>
    <abstract>Analyzing and processing big data of quality inspection is the key factor in ensuring product quality and People's property security. Big data of quality inspection collected by social network and E-commerce is missing in most cases. And the incompleteness of data brings huge challenge for analyzing and processing. Therefore, the algorithm of data filling based on stacked denoising auto-encoder is proposed in this text. As the experiment shows that the algorithm proposed in this text is effective in dealing with big data of quality inspection.</abstract>
    <year>2017</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICBDA.2017.8078781</doi>
  </row>
  <row>
    <author>Beecks, Christian and Uysal, Merih Seran and Seidl, Thomas</author>
    <title>Gradient-based signatures for big multimedia data</title>
    <keywords>Multimedia communication;Multimedia databases;Adaptation models;Data models;Indexing;Big data;Query processing;Big Multimedia Data;Content-based Information Access;Gradient-based Signatures;Feature Signatures</keywords>
    <abstract>With the continuous increase of heterogeneous multimedia data, the question of how to access big multimedia data efficiently has become of crucial importance. In order to provide fast access to complex multimedia data, we propose to approximate content-based features of multimedia objects by means of generative models. The proposed gradient-based signatures epitomize a high quality content-based approximation of multimedia objects and facilitate efficient indexing and query processing at large scale.</abstract>
    <year>2015</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData.2015.7364093</doi>
  </row>
  <row>
    <author>Juneja, Ashish and Das, Nripendra Narayan</author>
    <title>Big Data Quality Framework: Pre-Processing Data in Weather Monitoring Application</title>
    <keywords>Big Data;Data integrity;Meteorology;Monitoring;Data mining;Organizations;Big Data;Big Data Quality;Data Quality;preprocessing;pre-processing</keywords>
    <abstract>Big Data has become an imminent part of all industries and business sectors today. All organizations in any sector like energy, banking, retail, hardware, networking, etc all generate huge quantum of heterogenous data which if mined, processed and analyzed accurately can reveal immensely useful patterns for business heads to apply to generate and grow their businesses. Big Data helps in acquiring, processing and analyzing large amounts of heterogeneous data to derive valuable results. Quality of information is affected by size, speed and format in which data is generated. Hence, Quality of Big Data is of great relevance and importance. We propose addressing various aspects of the raw data to improve its quality in the pre-processing stage, as the raw data may not usable as-is. We are exploring process like Cleansing to fix as much data as feasible, Noise filters to remove bad data, as well sub-processes for Integration and Filtering along with Data Transformation/Normalization. We evaluate and profile the Big Data during acquisition stage, which is adapted to expectations to avoid cost overheads later while also improving and leading to accurate data analysis. Hence, it is imperative to improve Data quality even it is absorbed and utilized in an industry's Big Data system. In this paper, we propose a Pre-Processing Framework to address quality of data in a weather monitoring and forecasting application that also takes into account global warming parameters and raises alerts/notifications to warn users and scientists in advance.</abstract>
    <year>2019</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/COMITCon.2019.8862267</doi>
  </row>
  <row>
    <author>Jin, Li and Haosong, Li and Zhongping, Xu and Ting, Wang and Shuai, Wang and Yutong, Wei and Dongliang, Hu and Chunting, Kang and Jia, Wu and Dan, Su</author>
    <title>Research on Wide-area Distributed Power Quality Data Fusion Technology of Power Grid</title>
    <keywords>Power quality;Data integration;Distributed databases;Monitoring;Power grids;Computer architecture;Data models;Power Quality;Wide Area Distribution;Data Integration</keywords>
    <abstract>With the advancement of the "big operation" system construction, the online monitoring system for power quality has been integrated, and various power quality data have been incorporated into relevant organizations for unified management. Power quality management has a larger range of data, more types, and higher frequency. It needs to realize the unified storage management and efficient access of massive heterogeneous power quality data for the characteristics of data applications and the collection and aggregation of these effective data. This paper proposes a new type of grid wide-area distributed power quality data integration architecture, which is designed for multi-source, heterogeneous, distributed data integration technology and wide-area distributed data storage technology to solve the big data source problem and realize the sharing of power quality data information of the whole network.</abstract>
    <year>2019</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICCCBDA.2019.8725668</doi>
  </row>
  <row>
    <author>Becker, David and King, Trish Dunn and McMullen, Bill</author>
    <title>Big data, big data quality problem</title>
    <keywords>Big data;Biomedical imaging;Personnel;Complexity theory;Bioinformatics;Process control;Instruments;Big Data;Data Quality;Returns to Scale</keywords>
    <abstract>A USAF sponsored MITRE research team undertook four separate, domain-specific case studies about Big Data applications. Those case studies were initial investigations into the question of whether or not data quality issues encountered in Big Data collections are substantially different in cause, manifestation, or detection than those data quality issues encountered in more traditionally sized data collections. The study addresses several factors affecting Big Data Quality at multiple levels, including collection, processing, and storage. Though not unexpected, the key findings of this study reinforce that the primary factors affecting Big Data reside in the limitations and complexities involved with handling Big Data while maintaining its integrity. These concerns are of a higher magnitude than the provenance of the data, the processing, and the tools used to prepare, manipulate, and store the data. Data quality is extremely important for all data analytics problems. From the study's findings, the "truth about Big Data" is there are no fundamentally new DQ issues in Big Data analytics projects. Some DQ issues exhibit return-s-to-scale effects, and become more or less pronounced in Big Data analytics, though. Big Data Quality varies from one type of Big Data to another and from one Big Data technology to another.</abstract>
    <year>2015</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData.2015.7364064</doi>
  </row>
  <row>
    <author>Wong, Ka Yee. and Wong, Raymond K.</author>
    <title>Big Data Quality Prediction on Banking Applications: Extended Abstract</title>
    <keywords>Big Data;Data models;Banking;Neural networks;Industries;Data integrity;Computational modeling;Big Data;Data Noise;Data Quality;Prediction and Machine Learning</keywords>
    <abstract>Big data has been transformed into knowledge by information systems to add value in businesses. Enterprises relying on it benefit from risk management to a certain extent. The value, however, depends on the quality of data. The quality needs to be verified before any use of the data. Specifically, measuring the quality by simulating the real life situation and even forecast it accurately turns into a hot topic. In recent years, there have been numerous researches on the measurement and assessment of data quality. These are yet to utilize a scientific computational method for the measurement and prediction. Current methods either fail to make an accurate prediction or do not consider the correlation and time sequence factors of the data. To address this, we design a model to extend machine learning technique to business applications predicting this. Firstly, we implement the model to detect data noises from a risk dataset according to an international data quality standard from banking industry and then estimate their impacts with Gaussian and Bayesian methods. Secondly, we direct sequential learning in multiple deep neural networks for the prediction with an attention mechanism. The model is experimented with various network methodologies to show the predictive power of machine learning technique and is evaluated by validation data to confirm the model effectiveness. The model is scalable to apply to any industries utilizing big data other than the banking industry.</abstract>
    <year>2020</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/DSAA49011.2020.00119</doi>
  </row>
  <row>
    <author>Rao, Dhana and Gudivada, Venkat N and Raghavan, Vijay V.</author>
    <title>Data quality issues in big data</title>
    <keywords>Databases;Big data;Measurement;Context;Cleaning;Biology;Computers;Data quality;big data;biological data;information quality</keywords>
    <abstract>Though the issues of data quality trace back their origin to the early days of computing, the recent emergence of Big Data has added more dimensions. Furthermore, given the range of Big Data applications, potential consequences of bad data quality can be for more disastrous and widespread. This paper provides a perspective on data quality issues in the Big Data context. it also discusses data integration issues that arise in biological databases and attendant data quality issues.</abstract>
    <year>2015</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData.2015.7364065</doi>
  </row>
  <row>
    <author>Taleb, Ikbal and Kassabi, Hadeel T. El and Serhani, Mohamed Adel and Dssouli, Rachida and Bouhaddioui, Chafik</author>
    <title>Big Data Quality: A Quality Dimensions Evaluation</title>
    <keywords>Big data;Measurement;Feature extraction;Quality assessment;Social network services;Companies;Context;Big Data;data quality dimensions;data quality evaluation;Big data sampling</keywords>
    <abstract>Data is the most valuable asset companies are proud of. When its quality degrades, the consequences are unpredictable, can lead to complete wrong insights. In Big Data context, evaluating the data quality is challenging, must be done prior to any Big data analytics by providing some data quality confidence. Given the huge data size, its fast generation, it requires mechanisms, strategies to evaluate, assess data quality in a fast, efficient way. However, checking the Quality of Big Data is a very costly process if it is applied on the entire data. In this paper, we propose an efficient data quality evaluation scheme by applying sampling strategies on Big data sets. The Sampling will reduce the data size to a representative population samples for fast quality evaluation. The evaluation targeted some data quality dimensions like completeness, consistency. The experimentations have been conducted on Sleep disorder's data set by applying Big data bootstrap sampling techniques. The results showed that the mean quality score of samples is representative for the original data, illustrate the importance of sampling to reduce computing costs when Big data quality evaluation is concerned. We applied the Quality results generated as quality proposals on the original data to increase its quality.</abstract>
    <year>2016</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0122</doi>
  </row>
  <row>
    <author>Ogudo, Kingsley A. and Nestor, Dahj Muwawa Jean</author>
    <title>Modeling of an Efficient Low Cost, Tree Based Data Service Quality Management for Mobile Operators Using in-Memory Big Data Processing and Business Intelligence use Cases</title>
    <keywords>Quality of service;Big Data;Business intelligence;Structured Query Language;Tools;Sparks;Service Quality Management;In-Memory Big Data;Business Intelligence;Service Quality Index;Over The Top Application (OTT);Data Traffic and ROI</keywords>
    <abstract>Network Operators are shifting their business interest towards Data services in a geometric progression manner, as Data services is becoming the major source of Telco revenue. The wide use of Data platforms; such as WhatsApp, Skype, Hangout and other Over the Top (OTT) voice applications over the traditional voice services is a clear indication that Network Operators need to adjust their business model and needs. And couple with the adoption of Smartphones usage which grows continuously year by year, this means more subscribers to manage, large amount of transactions generated, more network resources to be added and evidently more human technical expertise required to ensure good service quality. That has led to high investment on Robust Service Quality Management (SQM) and Customer Experience Management (CEM) to stay competitive in the market. The high investment is justified by the integration of Big Data Solutions, Machine Learning capabilities and good visualization of insight data. However, the Return on Investment (ROI) of the expensive systems are not as conspicuous as the provided functionalities and business rules. Therefore, in this paper an efficient model for low cost SQM system is presented, exploring the advantages of In-Memory Big Data processing and low cost business Intelligence tools to showcase how a good Service Quality Management can be implemented with no big investment.</abstract>
    <year>2018</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICABCD.2018.8465410</doi>
  </row>
  <row>
    <author>Gonzalez-Alonso, P. and Vilar, R. and Lupiañez-Villanueva, F.</author>
    <title>Meeting Technology and Methodology into Health Big Data Analytics Scenarios</title>
    <keywords>Big Data;Organizations;Medical services;Tools;Medical diagnostic imaging;Design methodology;Healthcare;health analytics;big data analytics;big data analytical frameworks;translational medicine</keywords>
    <abstract>Health organizations are collecting more data from a wider array of sources at greater speed every day. The analysis of this vast amount of data creates new opportunities to deliver modern personalized health and social care services. Big Data Analytics and underlying technologies have the potential to process and analyze these data to extract meaningful insights for improving quality of care, efficiency and sustainability of health and social care systems. Health organizations face therefore a new scenario where analytical tools must accommodate both traditional business intelligence and Big Data approaches, resulting in important technological and methodological challenges to be tackled. In this paper, we present a methodological approach to address the introduction of Big Data Analytics technologies into an integrated care provider.</abstract>
    <year>2017</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/CBMS.2017.71</doi>
  </row>
  <row>
    <author>Zhang, Pengcheng and Xiong, Fang and Gao, Jerry and Wang, Jimin</author>
    <title>Data quality in big data processing: Issues, solutions and open problems</title>
    <keywords>Big Data;Data integrity;Data analysis;Social network services;Data preprocessing;Internet;Big Data;Big data processing;Data Quality;Recommendation system;Prediction system</keywords>
    <abstract>With the rapid development of social networks, Internet of things, Cloud computing as well as other technologies, big data age is arriving. The increasing number of data has brought great value to the public and enterprises. Meanwhile how to manage and use big data better has become the focus of all walks of life. The 4V characteristics of big data have brought a lot of issues to the big data processing. The key to big data processing is to solve data quality issue, and to ensure data quality is a prerequisite for the successful application of big data technique. In this paper, we use recommendation systems and prediction systems as typical big data applications, and try to find out the data quality issues during data collection, data preprocessing, data storage and data analysis stages of big data processing. According to the elaboration and analysis of the proposed issues, the corresponding solutions are also put forward. Finally, some open problems to be solved in the future are also raised.</abstract>
    <year>2017</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/UIC-ATC.2017.8397554</doi>
  </row>
  <row>
    <author>Wu, Donghui</author>
    <title>A big data analytics framework for forecasting rare customer complaints: A use case of predicting MA members' complaints to CMS</title>
    <keywords>Decision trees;Training;Training data;Big Data;Prediction algorithms;Ethics;Contracts;CMS Star Ratings;customer complaints;big data analytics;ensemble methods;ethics;call center data;medical claims</keywords>
    <abstract>Centers for Medicare &amp; Medicaid Services (CMS) publishes Medicare Part C Star Ratings each year to measure the quality of care of Medicare Advantage (MA) contracts. One of the key measures is Complaints about the Health Plan, which is captured in Complaints Tracking Module (CTM). Complaints resulted in CTM are rare events: for MA contracts with 2-5 star ratings, number of complaints for every 1,000 members range from .10 to 1.84 over last 5 years. Reducing number of complaints is extremely important to MA plans as they impact CMS reimbursements to MA plans. Forecasting and reducing complaints is an extremely technically challenging task, and involves ethics considerations in patients' rights and privacy. In this research, we constructed a big data analytics framework for forecasting rare customer complaints. First, we built a big data ingestion pipelines on a Hadoop platform: a) Ingest MA plan's customer complaints data from CTM from past 3 years. b) Ingest health plan's call center data for MA members from past 3 years, including both structured data and unstructured text script for the calls. c) Ingest MA members' medical claims, including members' demographics and enrollment history. d) Ingest MA members' pharmacy claims. e) Integrate and unified data from above sources, and enrich the data with additional engineered features into a big wide table, one row per member for analysis and modeling. Second, we designed a unique decision tree based Large Ensemble with Over-Sampling (LEOS) algorithm, which mimics random forest but with extreme oversampling of target class to increase bias, and leverages the parallel computing of Hadoop clusters by generating thousands of fixed size training data sets, and for each such dataset training a decision trees with similar fixed tree structure, and ensemble them. Third, we validated our framework and LEOS learning algorithm with real data, and also discussed ethics issues we encountered in handling data and applying findings from research.</abstract>
    <year>2017</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData.2017.8258406</doi>
  </row>
  <row>
    <author>Muniswamaiah, Manoj and Agerwala, Tilak and Tappert, Charles C.</author>
    <title>Federated Query processing for Big Data in Data Science</title>
    <keywords>Databases;Machine learning algorithms;Engines;Machine learning;Task analysis;Protocols;Big Data;Machine learning;big data;federated query;database;query optimizer;in-memory data;data science</keywords>
    <abstract>As the number of databases continues to grow data scientists need to use data from different sources to run machine learning algorithms for analysis. Data science results depend upon the quality of data been extracted. The objective of this research paper is to implement a federated query processing framework which extracts data from different data sources and stores the result datasets in a common in-memory data format. This helps data scientists to perform their analysis and execute machine learning algorithms using different data engines without having to convert the data into their native data format and improve the performance.</abstract>
    <year>2019</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData47090.2019.9005530</doi>
  </row>
  <row>
    <author>Chasupa, Tung-lersloy and Paireekreng, Worapat</author>
    <title>The Framework of Extracting Unstructured Usage for Big Data Platform</title>
    <keywords>Industries;Data analysis;Organizations;Big Data;Tools;Data mining;Interviews;Unstructured Data;Unstructured Big Data;Big Data;Extracting Unstructured Big Data Platform;Unstructured Data Canvas</keywords>
    <abstract>Big Data becomes crucial tools for new era of data analytics. The amount of unstructured data is also increasing. As a result, the number of unstructured data projects are increased. However, several organizations are still lack of knowledge how to determine the unstructured data in the organization and exploit it. Therefore, the tool of extracting unstructured data is needed. This research aims to propose the framework to identify the unstructured usage in the organization. The framework has been derived from the interview of the experts in areas. After that, the framework has been used to verify the results. The success case and failed are also shown. This can be seen that the proposed framework can be used in the organization to help the user extract the unstructured data usage in the organization. It can help to make the decision related to unstructured data project.</abstract>
    <year>2021</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/IBDAP52511.2021.9552131</doi>
  </row>
  <row>
    <author>Kumar, Jitendra and Crow, Michael C. and Devarakonda, Ranjeet and Giansiracusa, Michael and Guntupally, Kavya and Olatt, Joseph V. and Price, Zach and Shanafield, Harold A. and Singh, Alka</author>
    <title>Provenance–aware workflow for data quality management and improvement for large continuous scientific data streams</title>
    <keywords>Data integrity;Instruments;Atmospheric measurements;Dictionaries;Big Data;Portals;scientific data workflows;data quality;provenance;atmospheric science</keywords>
    <abstract>Data quality assessment, management and improvement is an integral part of any big data intensive scientific research to ensure accurate, reliable, and reproducible scientific discoveries. The task of maintaining the quality of data, however, is non-trivial and poses a challenge for a program like the Department of Energy's Atmospheric Radiation Measurement (ARM) that collects data from hundreds of instruments across the world, and distributes thousands of streaming data products that are continuously produced in near-real-time for an archive 1.7 Petabyte in size and growing. In this paper, we present a computational data processing workflow to address the data quality issues via an easy and intuitive web-based portal that allows reporting of any quality issues for any site, facility or instruments at a granularity down to individual variables in the data files. This portal allows instrument specialists and scientists to provide corrective actions in the form of symbolic equations. A parallel processing framework applies the data improvement to a large volume of data in an efficient, parallel environment, while optimizing data transfer and file I/O operations; corrected files are then systematically versioned and archived. A provenance tracking module tracks and records any change made to the data during its entire life cycle which are communicated transparently to the scientific users. Developed in Python using open source technologies, this software architecture enables fast and efficient management and improvement of data in an operational data center environment.</abstract>
    <year>2019</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData47090.2019.9006358</doi>
  </row>
  <row>
    <author>Serhani, Mohamed Adel and El Kassabi, Hadeel T. and Taleb, Ikbal and Nujum, Alramzana</author>
    <title>An Hybrid Approach to Quality Evaluation across Big Data Value Chain</title>
    <keywords>Big data;Metadata;Measurement;Quality assessment;Quality of service;Unified modeling language;Big Data;Quality assessment;Metadata;Quality metrics;quality Metadata;Quality of process;Hybrid quality assessment</keywords>
    <abstract>While the potential benefits of Big Data adoption are significant, and some initial successes have already been realized, there remain many research and technical challenges that must be addressed to fully realize this potential. The Big Data processing, storage and analytics, of course, are major challenges that are most easily recognized. However, there are additional challenges related for instance to Big Data collection, integration, and quality enforcement. This paper proposes a hybrid approach to Big Data quality evaluation across the Big Data value chain. It consists of assessing first the quality of Big Data itself, which involve processes such as cleansing, filtering and approximation. Then, assessing the quality of process handling this Big Data, which involve for example processing and analytics process. We conduct a set of experiments to evaluate Quality of Data prior and after its pre-processing, and the Quality of the pre-processing and processing on a large dataset. Quality metrics have been measured to access three Big Data quality dimensions: accuracy, completeness, and consistency. The results proved that combination of data-driven and process-driven quality evaluation lead to improved quality enforcement across the Big Data value chain. Hence, we recorded high prediction accuracy and low processing time after we evaluate 6 well-known classification algorithms as part of processing and analytics phase of Big Data value chain.</abstract>
    <year>2016</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigDataCongress.2016.65</doi>
  </row>
  <row>
    <author>Nobles, Alicia L. and Vilankar, Ketki and Wu, Hao and Barnes, Laura E.</author>
    <title>Evaluation of data quality of multisite electronic health record data for secondary analysis</title>
    <keywords>Documentation;Data warehouses;Medical services;Big data;Databases;Cleaning;Medical diagnostic imaging;electronic health records;data quality;big data;multiple data vendors;metrics</keywords>
    <abstract>Currently, a large amount of data is amassed in electronic health records (EHRs). However, EHR systems are largely information silos, that is, uses of these systems are often confined to management of patient information and analytics specific to a clinician's practice. A growing trend in healthcare is combining multiple databases to support epidemiological research. The College Health Surveillance Network is the first national data warehouse containing EHR data from 31 different student health centers. Each member university contributes to the data warehouse by uploading select EHR data including patient demographics, diagnoses, and procedures to a common server on a monthly basis. In this paper, we focus on the data quality dimensions from a subsample of the data comprised of over 5.7 million patient visits for approximately 980,000 patients with 4,465 unique diagnoses from 23 of those universities. We examine the data for measures of completeness, consistency, and availability for secondary use for epidemiological research. Additionally, clinical documentation practices and EHR vendor were evaluated as potential contributors to data quality. We found that overall about 70% of the data in the data warehouse is available for secondary use, and identified clinical documentation practices that are correlated to a reduction in data quality. This suggests that automated quality control and proactive clinical documentation support could reduce ad-hoc data cleaning needs resulting in greater data availability for secondary use.</abstract>
    <year>2015</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData.2015.7364060</doi>
  </row>
  <row>
    <author>Mao, Yifan and Huang, Shasha and Cui, Shuo and Wang, HaiFeng and Zhang, Junyan and Ding, Wenhao</author>
    <title>Multi dimensional data distribution monitoring based on OLAP</title>
    <keywords>Measurement;Analytical models;Standards organizations;Data integration;Systems architecture;Financial management;Information age;data link;OLAP;multidimensional data model</keywords>
    <abstract>With the rapid development of the Internet, society is gradually entering the information age, and various data in enterprises have become the most important strategic core resources of all enterprises. The operation and decision-making of enterprises all require a large amount of data analysis. Nowadays, many companies do not pay enough attention to the monitoring of data asset distribution. In addition, various internal systems such as financial management and ERP systems are relatively independent. Each system has its own data organization standard, which makes it difficult to conduct a unified management of data. This also directly leads to the one-sided and subjective problem of enterprise managers' distribution of data assets. With the construction of the data center of each enterprise, the data of each system is aggregated to the center through data integration technology. Therefore, all enterprises need to build a multi-dimensional data distribution monitoring model around data links to comprehensively monitor the status of various data distributions across the company's entire network, and improve data service capabilities and sharing capabilities as well as the company's operational capabilities. This article uses OLAP technology to construct a multi-dimensional data distribution monitoring model for the data link in the process of power enterprise data integration. This article first selects the dimensions and metrics that need to be monitored in the multidimensional data, and then constructs the conceptual model, logical model and physical model of the multidimensional data using on line analytical processing technology. Finally, an example analysis of OLAP system architecture based on B/S structure is realized. The overall data distribution of the enterprise can be grasped by analyzing the various dimensions of the data link, such as System type, location distribution, and time.</abstract>
    <year>2020</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ITCA52113.2020.00070</doi>
  </row>
  <row>
    <author>Li, Lianzhi</author>
    <title>Evaluation Model of Education Service Quality Satisfaction in Colleges and Universities Dependent on Classification Attribute Big Data Feature Selection Algorithm</title>
    <keywords>Education;Correlation;Data models;Encyclopedias;Big Data;Mutual information;Compounds;Classification Attribute Big Data Feature Selection Algorithm;Education Service Quality in Colleges and Universities;Education Service in Colleges and Universities;Satisfaction Evaluation</keywords>
    <abstract>In view of the insufficiency in the education service quality in colleges and universities, a kind of evaluation model of the education service quality satisfaction in the colleges and universities that is dependent on the classification attribute big data feature selection algorithm is put forward in this paper based on the existing work. On the basis of detailed description of the model components, further study on the evaluation method of the proposed model for the education service quality satisfaction in the colleges and universities is carried out. Under the guidance of the evaluation model of the education service quality satisfaction in the colleges and universities, the method for the construction of the evaluation model of the education service quality satisfaction in the colleges and universities is studied with the orientation to the education service resources in the colleges and universities under the open big data environment. In addition, experimental verification is carried out on the basis of the evaluation data in the 360 Encyclopedia on the education service quality satisfaction in the colleges and universities. The experimental results show that the model and method put forward in this paper can effectively evaluate the quality of the education service in the colleges and universities.</abstract>
    <year>2019</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICITBS.2019.00160</doi>
  </row>
  <row>
    <author>Catarci, Tiziana and Scannapieco, Monica and Console, Marco and Demetrescu, Camil</author>
    <title>My (fair) big data</title>
    <keywords>Big Data;Pipelines;Ontologies;Metadata;Google;Quality-driven policies;Big Data pipeline;ontology-based quality checking;Big Data confidentiality</keywords>
    <abstract>Policy making has the strict requirement to rely on quantitative and high quality information. This paper will address the data quality issue for policy making by showing how to deal with Big Data quality in the different steps of a processing pipeline, with a focus on the integration of Big Data sources with traditional sources. In this respect, a relevant role is played by metadata and in particular by ontologies. Integration systems relying on ontologies enable indeed a formal quality evaluation of inaccuracy, inconsistency and incompleteness of integrated data. The paper will finally describe data confidentiality as a Big Data quality dimension, showing the main issues to be faced for its assurance.</abstract>
    <year>2017</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData.2017.8258267</doi>
  </row>
  <row>
    <author>Taleb, Ikbal and Serhani, Mohamed Adel and Dssouli, Rachida</author>
    <title>Big Data Quality: A Survey</title>
    <keywords>Big Data;Data integrity;Quality management;Data visualization;Databases;Sensors;Social network services;Big Data, Data Quality, Quality Management framework, Quality of Big Data</keywords>
    <abstract>With the advances in communication technologies and the high amount of data generated, collected, and stored, it becomes crucial to manage the quality of this data deluge in an efficient and cost-effective way. The storage, processing, privacy and analytics are the main keys challenging aspects of Big Data that require quality evaluation and monitoring. Quality has been recognized by the Big Data community as an essential facet of its maturity. Yet, it is a crucial practice that should be implemented at the earlier stages of its lifecycle and progressively applied across the other key processes. The earlier we incorporate quality the full benefit we can get from insights. In this paper, we first identify the key challenges that necessitates quality evaluation. We then survey, classify and discuss the most recent work on Big Data management. Consequently, we propose an across-the-board quality management framework describing the key quality evaluation practices to be conducted through the different Big Data stages. The framework can be used to leverage the quality management and to provide a roadmap for Data scientists to better understand quality practices and highlight the importance of managing the quality. We finally, conclude the paper and point to some future research directions on quality of Big Data.</abstract>
    <year>2018</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigDataCongress.2018.00029</doi>
  </row>
  <row>
    <author>Wang, Fengling and Wang, Han and Xue, Liang</author>
    <title>Research on Data Security in Big Data Cloud Computing Environment</title>
    <keywords>Cloud computing;Data security;Data integrity;Big Data;Maintenance engineering;Virtualization;Information technology;big data;cloud computing;data security;big data cloud computing;security policy</keywords>
    <abstract>In the big data cloud computing environment, data security issues have become a focus of attention. This paper delivers an overview of conceptions, characteristics and advanced technologies for big data cloud computing. Security issues of data quality and privacy control are elaborated pertaining to data access, data isolation, data integrity, data destruction, data transmission and data sharing. Eventually, a virtualization architecture and related strategies are proposed to against threats and enhance the data security in big data cloud environment.</abstract>
    <year>2021</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/IAEAC50856.2021.9391025</doi>
  </row>
  <row>
    <author>Guan, Zhibin and Ji, Tongkai and Qian, Xu and Ma, Yan and Hong, Xuehai</author>
    <title>A Survey on Big Data Pre-processing</title>
    <keywords>Big Data;Data integration;Data analysis;Dimensionality reduction;Feature extraction;Data integrity;big data processing;data pre-processing;data cleansing;dimension reduction;data quality</keywords>
    <abstract>In this paper, we briefly introduce some basic concepts and characteristics of big data. We are surrounded by massive amount of data but starving for knowledge. In the era of Big Data, how to quickly obtain high-quality and valuable information from massive amounts of data has become an important research direction. Hence, we focus our attention to the data pre-processing which is a sub-content of the data processing workflow. In this paper, the four phases of data pre-processing, including data cleansing, data integration, data reduction, and data transformation, have been discussed. And different approaches for a variety of purposes have been presented, which show current methods and techniques need to be further modified in order to improve the quality of data before data analysis.</abstract>
    <year>2017</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ACIT-CSII-BCD.2017.49</doi>
  </row>
  <row>
    <author>Li, Yuqian and Li, Peng and Zhu, Feng and Wang, Ruchuan</author>
    <title>Design of higher education quality monitoring and evaluation platform based on big data</title>
    <keywords>Education;Monitoring;Big Data;Data mining;Servers;Memory;Indexes;big data;monitoring and evaluation;system design</keywords>
    <abstract>Through the continuous collection and in-depth analysis of the quality monitoring data of colleges and universities, we combine the efficiency processing of big data and data evaluation, monitor the status of higher education normally, and construct a higher education quality monitoring and evaluation platform based on Spark. This platform is teaching centered with schools as its basis, including subsystems of data acquisition, data analysis, machine learning, data storage, data analysis and other areas. Through the application of the higher education quality monitoring platform, we can understand the current situation of the development of higher education scientifically, and provide the basis for the macro-decision of education administration department.</abstract>
    <year>2017</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICCSE.2017.8085513</doi>
  </row>
  <row>
    <author>Li, Xiaohan and Yu, Bowen and Feng, Guanyu and Wang, Haojie and Chen, Wenguang</author>
    <title>LotusSQL: SQL engine for high-performance big data systems</title>
    <keywords>Structured Query Language;Optimization;Engines;C++ languages;Sparks;Big Data;Query processing;big data;C++;Structured Query Language (SQL);query optimization</keywords>
    <abstract>In recent years, Apache Spark has become the de facto standard for big data processing. SparkSQL is a module offering support for relational analysis on Spark with Structured Query Language (SQL). SparkSQL provides convenient data processing interfaces. Despite its efficient optimizer, SparkSQL still suffers from the inefficiency of Spark resulting from Java virtual machine and the unnecessary data serialization and deserialization. Adopting native languages such as C++ could help to avoid such bottlenecks. Benefiting from a bare-metal runtime environment and template usage, systems with C++ interfaces usually achieve superior performance. However, the complexity of native languages also increases the required programming and debugging efforts. In this work, we present LotusSQL, an engine to provide SQL support for dataset abstraction on a native backend Lotus. We employ a convenient SQL processing framework to deal with frontend jobs. Advanced query optimization technologies are added to improve the quality of execution plans. Above the storage design and user interface of the compute engine, LotusSQL implements a set of structured dataset operations with high efficiency and integrates them with the frontend. Evaluation results show that LotusSQL achieves a speedup of up to 9× in certain queries and outperforms Spark SQL in a standard query benchmark by more than 2× on average.</abstract>
    <year>2021</year>
    <ENTRYTYPE>article</ENTRYTYPE>
    <doi>10.26599/BDMA.2021.9020009</doi>
  </row>
  <row>
    <author>Rueda, Diego F. and Vergara, Dahyr and Reniz, David</author>
    <title>Big Data Streaming Analytics for QoE Monitoring in Mobile Networks: A Practical Approach</title>
    <keywords>Quality of experience;Big Data;Monitoring;Streaming media;Real-time systems;Tools;Big Data Analytics;customer experience management;mobile networks;quality of experience;streaming data processing</keywords>
    <abstract>Traditionally, Mobile Network Operators (MNOs) use a set of Key Performance Indicators (KPIs) to measure the quality offered to their customers. However, these KPIs do not reflect the quality perceived by the customers because they are high-level and network-based metrics. Instead, Quality of Experience (QoE) monitoring of the most common mobile applications can help MNOs to determine when and where customer experience is degraded. In this paper, a customized tool based on Big Data Streaming is proposed to solve the needs of customer experience monitoring in a real-life MNO and to overcome the challenges of processing a large amount of data collected in 3G and 4G mobile networks. Moreover, real-life case studies of value creation through Big Data Analytics for telecommunication industry are also defined. Results show that the streaming data processing enables new opportunities for the MNO to take actions focused on customer experience improvement in near real-time.</abstract>
    <year>2018</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData.2018.8622590</doi>
  </row>
  <row>
    <author>Xiang, Zheng and Xiaofang, Liu and Weigang, Gao</author>
    <title>Analysis of the Application of Military Big Data in Equipment Quality Information Management</title>
    <keywords>Big Data;Military equipment;Information management;Distributed databases;Data analysis;Maintenance engineering;military big data;equipment quality information;management</keywords>
    <abstract>This At present, big data has risen to the national strategy. Big data is fully integrated into the military field, becoming the driving force of military scientific research, the core element of construction management, and an important resource for war success. This paper mainly expounds the basic connotation of big data technology and military big data, and analyzes the application of military big data in equipment quality information management, and proposes information collection, storage, analysis, processing, exchange and feedback on equipment quality information management. The countermeasures provide methods and basis for military big data in equipment information management.</abstract>
    <year>2019</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICCCBDA.2019.8725744</doi>
  </row>
  <row>
    <author>Wu, Xiangwei and Yang, Hongqi and Liu, Yuke and Nie, Guojia and Yang, Lihao and Yang, Yun</author>
    <title>An Intelligent Selection Method Based On Electronic Component Quality Data System</title>
    <keywords>Switches;Data systems;Data models;Information and communication technology;component;quality data;data system;intelligent selection</keywords>
    <abstract>Aiming at the difficulty of electronic component quality data management and application, and the lack of data system and application methods required for data management in selection scenarios, this paper proposes an intelligent selection method based on electronic component quality data system, uses Bi-LSTM-ATT model for entity identification, and identifies data association based on entity relationship. By calculating the Tanimoto coefficient, the intelligent matching and push of similar products and substitute products are realized, and the intellectualization of component selection is fully supported. Finally, taking the scenario of fast switching diode selection as an example, the feasibility of the method proposed in this paper is verified, which provides a model for the intelligent application of quality data resources.</abstract>
    <year>2021</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/CECIT53797.2021.00045</doi>
  </row>
  <row>
    <author>Abdallah, Mohammad</author>
    <title>Big Data Quality Challenges</title>
    <keywords>Big Data;Quality Measurement;Quality Model;Quality Assurance</keywords>
    <abstract>Big Data, is a growing technique these days. There are many uses of Big Data; Artificial Intelligence, Health Care, Business, and many more. For that reason, it becomes necessary to deal with this massive volume of data with caution and care in a term to make sure that the data used and produced is in high quality. Therefore, the Big Data quality is must, and its rules have to be satisfied. In this paper, the main Big Data Quality Factors, which need to be measured, is presented in the perspective of the data itself, the data management, data processing, and data users. This research highlights the quality factors that may be used later to create different Big Data quality models.</abstract>
    <year>2019</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICBDCI.2019.8686099</doi>
  </row>
  <row>
    <author>Arruda, Darlan and Laigner, Rodrigo</author>
    <title>Requirements Engineering Practices and Challenges in the Context of Big Data Software Development Projects: Early Insights from a Case Study</title>
    <keywords>Conferences;Big Data;Tools;Software;Data models;Requirements engineering;System analysis and design;Big Data Systems;Requirements Engineering;Case Study;Big Data Challenges;Big Data Requirements</keywords>
    <abstract>This paper reports on the results of an exploratory case study on a large-scale Big Data systems development project in the Oil&amp;Gas domain within a non-profit organisation. The aim of this study was to investigate the RE practices and challenges in such projects, currently bereft in the scientific literature. This investigation was focused on: (a) RE practices; (b) sources and distribution of requirements; (c) the role of Big Data characteristics and technologies in RE and systems design; and (d) RE challenges in engineering Big Data Systems. The main results show that (a) there is a lack of specific project tailored RE practices, tools, and frameworks for elicitation, specification and modelling, analysis, and prioritisation of requirements; (b) 40% of the system's requirements are considered Big Data-related from which 75% are identified from internal sources; (c) Big Data characteristics and technologies play an important role in defining quality requirements and system's architecture; (d) five challenges in eliciting, documenting, and analysing Big Data related requirements were identified and discussed. The findings suggest academics and practitioners opportunities to engage in further research in this area.</abstract>
    <year>2020</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData50022.2020.9377734</doi>
  </row>
  <row>
    <author>Priebe, Torsten and Markus, Stefan</author>
    <title>Business information modeling: A methodology for data-intensive projects, data science and big data governance</title>
    <keywords>Business;Data models;Terminology;Big data;Data warehouses;Wheels;Standards organizations;Data Modeling;Project Methodology;Data Governance;Metadata;Information Catalog</keywords>
    <abstract>This paper discusses an integrated methodology to structure and formalize business requirements in large data-intensive projects, e.g. data warehouses implementations, turning them into precise and unambiguous data definitions suitable to facilitate harmonization and assignment of data governance responsibilities. We place a business information model in the center - used end-to-end from analysis, design, development, testing to data quality checks by data stewards. In addition, we show that the approach is suitable beyond traditional data warehouse environments, applying it also to big data landscapes and data science initiatives - where business requirements analysis is often neglected. As proper tool support has turned out to be inevitable in many real-world settings, we also discuss software requirements and their implementation in the Accurity Glossary tool. The approach is evaluated based on a large banking data warehouse project the authors are currently involved in.</abstract>
    <year>2015</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData.2015.7363987</doi>
  </row>
  <row>
    <author>Guizhi, Min and Zhongbo, Liu and Zhanmin, Zhang and Xuefeng, Jin and Weiyi, Xie and Shuai, Wang</author>
    <title>Research and Application of Big Data Analysis for Oil and Gas Production</title>
    <keywords>Oils;Production engineering;Big Data;Fuel processing industries;History;Data mining;Internet of Things;oil and gas production;big data analysis;data mining platform</keywords>
    <abstract>The history of oil and gas development and production is a history of data development. The generation of a large amount of information data has laid the cornerstone for the application of big data analysis. How to effectively mine data resources, use big data analysis to guide oilfield production practices, and provide a theoretical basis for decision-making to improve quality and efficiency is the technology core. In recent years, Huabei Oilfield has explored the application of big data analysis in oil and gas production. According to the types and characteristics of oilfield data, it has proposed and created a closed-loop big data analysis “seven-step method” system from acquisition, processing, tracking, and evaluation, preliminary designed and developed a data mining platform for oil production engineering based on Hadoop/Spark; The platform has been applied in 6 oil and gas production units and achieved remarkable social and economic benefits.</abstract>
    <year>2021</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICBAIE52039.2021.9389892</doi>
  </row>
  <row>
    <author>Han, Weiguo and Jochum, Matthew</author>
    <title>A Machine Learning Approach for Data Quality Control of Earth Observation Data Management System</title>
    <keywords>Data integrity;Machine learning;Earth;Satellites;Big Data;Process control;Monitoring;Big Data;Machine Learning;Earth Observation Data;Data management;Data Quality;Random Forest</keywords>
    <abstract>In the big data era, innovative technologies like cloud computing, artificial intelligence, and machine learning are increasingly utilized in the large-scale data management systems of many industry sectors to make them more scalable and intelligent. Applying them to automate and optimize earth observation data management is a hot topic. To improve data quality control mechanisms, a machine learning method in combination with built-in quality rules is presented in this paper to evolve processes around data quality and enhance management of earth observation data. The rules of quality check are set up to detect the common issues, including data completeness, data latency, bad data, and data duplication, and the machine learning model is trained, tested, and deployed to address these quality issues automatically and reduce manual efforts.</abstract>
    <year>2020</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/IGARSS39084.2020.9323615</doi>
  </row>
  <row>
    <author>Blanquer, Ignacio and Meira, Wagner</author>
    <title>EUBra-BIGSEA, A Cloud-Centric Big Data Scientific Research Platform</title>
    <keywords>Big Data;Data analysis;Data privacy;Quality of service;Data models;Security;Biological system modeling;Cloud-Computing,-Big-Data,-Quality-of-Service,-Data-Analytics</keywords>
    <abstract>This paper describes the achievements of project EUBra-BIGSEA, which has delivered programming models and data analytics tools for the development of distributed Big Data applications. As framework components, multiple data models are supported (e.g. data streams, multidimensional data, etc.) and efficient mechanisms to ensure privacy and security, on top of a QoS-aware layer for the smart and rapid provisioning of resources in a cloud-based environment.</abstract>
    <year>2018</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/DSN-W.2018.00023</doi>
  </row>
  <row>
    <author>Yu, Xuqiao</author>
    <title>Big Data Driven Model for New York Taxi Trips Analysis</title>
    <keywords>Industries;Analytical models;Snow;Urban areas;Weather forecasting;Big Data;Data models;big data;data mining;correlation analysis;nyc taxi trip</keywords>
    <abstract>Due to the accumulation of large amount of evolution in metropolitan areas, urban data is understood and has become the first-hand prospect of manageable data, the driven analysis which can be recycled to improve life quality in urban areas. In this study, the influential factors on total fare amount charged to passengers are explored and analysed by using the taxi trip data in the year 2015 to provide the insights for people in NYC to plan their trips in a most economically efficient way.</abstract>
    <year>2021</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICBDA51983.2021.9403054</doi>
  </row>
  <row>
    <author>Taleb, Ikbal and Dssouli, Rachida and Serhani, Mohamed Adel</author>
    <title>Big Data Pre-processing: A Quality Framework</title>
    <keywords>Big data;Data integration;Accuracy;Distributed databases;Data analysis;Business;Big Data;Data Quality;pre-processing</keywords>
    <abstract>With the abundance of raw data generated from various sources, Big Data has become a preeminent approach in acquiring, processing, and analyzing large amounts of heterogeneous data to derive valuable evidences. The size, speed, and formats in which data is generated and processed affect the overall quality of information. Therefore, Quality of Big Data (QBD) has become an important factor to ensure that the quality of data is maintained at all Big data processing phases. This paper addresses the QBD at the pre-processing phase, which includes sub-processes like cleansing, integration, filtering, and normalization. We propose a QBD model incorporating processes to support Data quality profile selection and adaptation. In addition, it tracks and registers on a data provenance repository the effect of every data transformation happened in the pre-processing phase. We evaluate the data quality selection module using large EEG dataset. The obtained results illustrate the importance of addressing QBD at an early phase of Big Data processing lifecycle since it significantly save on costs and perform accurate data analysis.</abstract>
    <year>2015</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigDataCongress.2015.35</doi>
  </row>
  <row>
    <author>Juddoo, Suraj and George, Carlisle</author>
    <title>Discovering Most Important Data Quality Dimensions Using Latent Semantic Analysis</title>
    <keywords>Data integrity;Big Data;Semantics;Industries;Bibliographies;Libraries;Production;Big Data;data quality;health data;data quality dimensions;latent semantic analysis</keywords>
    <abstract>Big Data quality is a field which is emerging. Many authors nowadays agree that data quality is still very relevant, even for Big Data uses. However, there is a lack of frameworks or guidelines about how to carry out those big data quality initiatives. The starting point of any data quality work is to determine the properties of data quality, termed as data quality dimensions (DQDs). Even those dimensions lack precise rigour in terms of definition from existing literature. This current research aims to contribute towards identifying the most important DQDs for big data in the health industry. It is a continuation of a previous work, which already identified five most important DQDs, using a human judgement based technique known as inner hermeneutic cycle. To remove potential bias coming from the human judgement aspect, this research uses the same set of literature but applies a statistical technique known to extract knowledge from a set of documents known as latent semantic analysis. The results confirm only 2 similar most important DQDs, namely accuracy and completeness.</abstract>
    <year>2018</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICABCD.2018.8465129</doi>
  </row>
  <row>
    <author>Khaleel, Majida Yaseen and Hamad, Murtadha M.</author>
    <title>Data Quality Management for Big Data Applications</title>
    <keywords>Distributed databases;Metadata;Big Data;Standards;File systems;Data integrity;Big Data;data quality;unstructured Data Distributed data file system;and statistical model.</keywords>
    <abstract>Currently, as a result of the continuous increase of data, one of the key issues is the development of systems and applications to deal with storage, management and processing of big numbers of data. These data are found in unstructured ways. Data management with traditional approaches is inappropriate because of the large and complex data sizes. Hadoop is a suitable solution for the continuous increase in data sizes. The important characteristics of the Hadoop are distributed processing, high storage space, and easy administration. Hadoop is better known for distributed file systems. In this paper, we have proposed techniques and algorithms that deal with big data including data collecting, data preprocessing, algorithms for data cleaning, A Technique for Converting Unstructured Data to Structured Data using metadata, distributed data file system (fragmentation algorithm) and Quality assurance algorithms by using the model is the statistical model to evaluate the highest educational institutions. We concluded that Metadata accelerates query response required and facilitates query execution, metadata will be content for reports, fields and descriptions. Total time access for three complex queries in distributed processing it is 00: 03: 00 per second while in nondistributed processing it is at 00: 15: 77 per second, average is approximately five minutes per second. Quality assurance note values (T-test) is 0.239 and values (T-dis) is 1.96, as a result of dealing with scientific sets and humanities sets. In the comparison law, it can be deduced that if the t-test is smaller than the t-dis; so there is no difference between the mean of the scientific and humanities samples, the values of C.V for both scientific is (8.585) and humanities sets is (7.427), using the law of homogeneity know whether any sets are more homogeneous whenever the value of a small C.V was more homogeneous however the humanity set is more homogeneity.</abstract>
    <year>2019</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/DeSE.2019.00072</doi>
  </row>
  <row>
    <author>Guntupally, Kavya and Devarakonda, Ranjeet and Kehoe, Kenneth</author>
    <title>Spring Boot based REST API to Improve Data Quality Report Generation for Big Scientific Data: ARM Data Center Example</title>
    <keywords>Springs;Databases;Service-oriented architecture;Data integrity;Tools;Servers;Big Data;auto configuration;CRUD;java framework;service-oriented architecture;REST;spring boot</keywords>
    <abstract>Web application technologies are growing rapidly with continuous innovation and improvements. This paper focuses on the popular Spring Boot [1] java-based framework for building web and enterprise applications and how it provides the flexibility for service-oriented architecture (SOA). One challenge with any Spring-based applications is its level of complexity with configurations. Spring Boot makes it easy to create and deploy stand-alone, production-grade Spring applications with very little Spring configuration. Example, if we consider Spring Model-View-Controller (MVC) framework [2], we need to configure dispatcher servlet, web jars, a view resolver, and component scan among other things. To solve this, Spring Boot provides several Auto Configuration options to setup the application with any needed dependencies. Another challenge is to identify the framework dependencies and associated library versions required to develop a web application. Spring Boot offers simpler dependency management by using a comprehensive, but flexible, framework and the associated libraries in one single dependency, which provides all the Spring related technology that you need for starter projects as compared to CRUD web applications. This framework provides a range of additional features that are common across many projects such as embedded server, security, metrics, health checks, and externalized configuration. Web applications are generally packaged as war and deployed to a web server, but Spring Boot application can be packaged either as war or jar file, which allows to run the application without the need to install and/or configure on the application server. In this paper, we discuss how Atmospheric Radiation Measurement (ARM) Data Center (ADC) at Oak Ridge National Laboratory, is using Spring Boot to create a SOA based REST [4] service API, that bridges the gap between frontend user interfaces and backend database. Using this REST service API, ARM scientists are now able to submit reports via a user form or a command line interface, which captures the same data quality or other important information about ARM data.</abstract>
    <year>2018</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData.2018.8621924</doi>
  </row>
  <row>
    <author>Assefi, Mehdi and Behravesh, Ehsun and Liu, Guangchi and Tafti, Ahmad P.</author>
    <title>Big data machine learning using apache spark MLlib</title>
    <keywords>Sparks;Big Data;Libraries;Data models;Computer architecture;Machine learning algorithms;Apache Spark MLlib;Big Data Machine Learning;Big Data Analytics;Machine Learning</keywords>
    <abstract>Artificial intelligence, and particularly machine learning, has been used in many ways by the research community to turn a variety of diverse and even heterogeneous data sources into high quality facts and knowledge, providing premier capabilities to accurate pattern discovery. However, applying machine learning strategies on big and complex datasets is computationally expensive, and it consumes a very large amount of logical and physical resources, such as data file space, CPU, and memory. A sophisticated platform for efficient big data analytics is becoming more important these days as the data amount generated in a daily basis exceeds over quintillion bytes. Apache Spark MLlib is one of the most prominent platforms for big data analysis which offers a set of excellent functionalities for different machine learning tasks ranging from regression, classification, and dimension reduction to clustering and rule extraction. In this contribution, we explore, from the computational perspective, the expanding body of the Apache Spark MLlib 2.0 as an open-source, distributed, scalable, and platform independent machine learning library. Specifically, we perform several real world machine learning experiments to examine the qualitative and quantitative attributes of the platform. Furthermore, we highlight current trends in big data machine learning research and provide insights for future work.</abstract>
    <year>2017</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData.2017.8258338</doi>
  </row>
  <row>
    <author>Shanmugam, Srinivasan and Seshadri, Gokul</author>
    <title>Aspects of Data Cataloguing for Enterprise Data Platforms</title>
    <keywords>Context;Business;Metadata;Big data;Electronic mail;Reliability;Indexes;Enterprise data;Data catalogue;Metadata of data;Data context;Data quality;Data governance;Data as a service</keywords>
    <abstract>As the adoption of enterprise Big Data platforms mature, the necessity to maintain systematic catalogue of data being processed and managed by these platforms becomes imperative. Enterprise data catalogues serve as centralized repositories of storing such metadata about data being handled by such platforms, enabling better data governance, security and control. Standardized approaches, methodologies and tools for data catalogues are being discussed and evolved. This paper introduces some key variables, attributes and indexes that need to be handled in such cataloguing solutions -- such as data contexts, data-system relationships, data quality, reliability, sensitivity and accessibility. The paper also discusses specific approaches on how each of these aspects can be adopted and applied to different enterprise contexts effectively.</abstract>
    <year>2016</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigDataSecurity-HPSC-IDS.2016.52</doi>
  </row>
  <row>
    <author>Snineh, Sidi Mohamed and Bouattane, Omar and Youssfi, Mohamed and Daaif, Abdelaziz</author>
    <title>Towards a multi-agents model for errors detection and correction in big data flows</title>
    <keywords>Big Data;Multi-agent systems;Companies;Task analysis;Error correction;Data integrity;Real-time systems;Multi-agents;Big Data;Data quality;detection and correction of data errors</keywords>
    <abstract>The quality of data for decision-making will always be a major factor for companies that want to remain competitors. In addition, the era of Big Data has brought new challenges for the processing, management, storage of data and in particular the challenge represented by the veracity of these data which is one of the 5Vs that characterizes Big Data. This characteristic that defines the quality or reliability of the data and its sources must be verified in the future systems of each company. In this paper, we present an approach that helps to improve the quality of Big Data by the distributed execution of algorithms for detecting and correcting data errors. The idea is to have a multi-agents model for errors detection and correction in big data flow. This model linked to a repository specific to each company. This repository contains the most frequent errors, metadata, error types, error detection algorithms and error correction algorithms. Each agent of this model represents an algorithm and will be deployed in multiple instances when needed. The use of these agents will go through two steps. In the first step, the detection agents and error correction agents manage each flow entering the system in real time. In the second step, all the processed data flows in first step will be a dataset to which the error detection and correction agents are applied in batch in order to process other types of errors. Among architectures who allow this processing type, we have chosen Lambda architecture.</abstract>
    <year>2019</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICDS47004.2019.8942297</doi>
  </row>
  <row>
    <author>Chang, Yue Shan and Lin, Kuan-Ming and Tsai, Yi-Ting and Zeng, Yu-Ren and Hung, Cheng-Xiang</author>
    <title>Big data platform for air quality analysis and prediction</title>
    <keywords>Semantics;Big Data;Air quality;Data mining;Urban areas;Monitoring;Government;Air Quality;Big Data;Prediction;Cloud Environment</keywords>
    <abstract>With the advance of industry, air quality (AQ) is increasingly becoming worse. There are increasingly AQ monitors device have been deployed around country for monitoring air-quality all year long. To estimate and predict AQ, such as PM (particulate matter) 2.5, become an important issue for government to improve people's quality of life. As we can know, there are many factors can affect the AQ, such as traffic, factory exhaust emissions, weather, incineration of garbage, and so on. In most well-developed countries, these pollution sources are monitored for future environmental policy making. In this paper, we will propose a semantic ETL (Extract-Transform-Load) framework on cloud platform for AQ prediction. In the platform, we exploit ontology to concretize the relationship of PM 2.5 from various data sources and to merge those data with the same concept but different naming into the unified database. We implement the ETL framework on the cloud platform, which includes computing nodes and storage nodes. The computing nodes are used to execute data mining algorithms for predicting, and storage modes are used to store retrieved, preprocessed, and analyzed data. We utilize restful web service as the front end API to retrieve analyzed data, and finally we exploit browser to show the visualized result to demonstrate the estimation and prediction. It shows that the big data access framework on the cloud platform can work well for air quality analysis.</abstract>
    <year>2018</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/WOCC.2018.8372743</doi>
  </row>
  <row>
    <author>Immonen, Anne and Pääkkönen, Pekka and Ovaska, Eila</author>
    <title>Evaluating the Quality of Social Media Data in Big Data Architecture</title>
    <keywords>Big data;Social network services;Computer architecture;Meta data;Online services;architecture;big data;metadata;quality attribute;quality of data;Architecture;big data;metadata;quality attribute;quality of data</keywords>
    <abstract>The use of freely available online data is rapidly increasing, as companies have detected the possibilities and the value of these data in their businesses. In particular, data from social media are seen as interesting as they can, when properly treated, assist in achieving customer insight into business decision making. However, the unstructured and uncertain nature of this kind of big data presents a new kind of challenge: how to evaluate the quality of data and manage the value of data within a big data architecture? This paper contributes to addressing this challenge by introducing a new architectural solution to evaluate and manage the quality of social media data in each processing phase of the big data pipeline. The proposed solution improves business decision making by providing real-time, validated data for the user. The solution is validated with an industrial case example, in which the customer insight is extracted from social media data in order to determine the customer satisfaction regarding the quality of a product.</abstract>
    <year>2015</year>
    <ENTRYTYPE>article</ENTRYTYPE>
    <doi>10.1109/ACCESS.2015.2490723</doi>
  </row>
  <row>
    <author>Juddoo, Suraj and George, Carlisle</author>
    <title>A Qualitative Assessment of Machine Learning Support for Detecting Data Completeness and Accuracy Issues to Improve Data Analytics in Big Data for the Healthcare Industry</title>
    <keywords>Industries;Machine learning algorithms;Data integrity;Clustering algorithms;Medical services;Big Data;Tools;Big Data;Data Quality;Data Inaccuracy;Data incompleteness;Machine Learning</keywords>
    <abstract>Tackling Data Quality issues as part of Big Data can be challenging. For data cleansing activities, manual methods are not efficient due to the potentially very large amount of data.. This paper aims to qualitatively assess the possibilities for using machine learning in the process of detecting data incompleteness and inaccuracy, since these two data quality dimensions were found to be the most significant by a previous research study conducted by the authors. A review of existing literature concludes that there is no unique machine learning algorithm most suitable to deal with both incompleteness and inaccuracy of data. Various algorithms are selected from existing studies and applied against a representative big (healthcare) dataset. Following experiments, it was also discovered that the implementation of machine learning algorithms in this context encounters several challenges for Big Data quality activities. These challenges are related to the amount of data particualar machine learning algorithms can scale to and also to certain data type restrictions imposed by some machine learning algorithms. The study concludes that 1) data imputation works better with linear regression models, 2) clustering models are more efficient to detect outliers but fully automated systems may not be realistic in this context. Therefore, a certain level of human judgement is still needed.</abstract>
    <year>2020</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ELECOM49001.2020.9297009</doi>
  </row>
  <row>
    <author>Haryadi, Adiska Fardani and Hulstijn, Joris and Wahyudi, Agung and van der Voort, Haiko and Janssen, Marijn</author>
    <title>Antecedents of big data quality: An empirical examination in financial service organizations</title>
    <keywords>Big data;Industries;Insurance;Companies;Finance;big data;data quality;big data quality;antecedents;finance</keywords>
    <abstract>Big data has been acknowledged for its enormous potential. In contrast to the potential, in a recent survey more than half of financial service organizations reported that big data has not delivered the expected value. One of the main reasons for this is related to data quality. The objective of this research is to identify the antecedents of big data quality in financial institutions. This will help to understand how data quality from big data analysis can be improved. For this, a literature review was performed and data was collected using three case studies, followed by content analysis. The overall findings indicate that there are no fundamentally new data quality issues in big data projects. Nevertheless, the complexity of the issues is higher, which makes it harder to assess and attain data quality in big data projects compared to the traditional projects. Ten antecedents of big data quality were identified encompassing data, technology, people, process and procedure, organization, and external aspects.</abstract>
    <year>2016</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData.2016.7840595</doi>
  </row>
  <row>
    <author>Li, Tao and He, Yihai and Zhu, Chunling</author>
    <title>Big Data Oriented Macro-Quality Index Based on Customer Satisfaction Index and PLS-SEM for Manufacturing Industry</title>
    <keywords>Indexes;Mathematical model;Data models;Manufacturing industries;Computational modeling;Big data;Customer satisfaction;macro-quality evaluation;big data;macro-quality index;customer satisfaction index;PLS-SEM</keywords>
    <abstract>The aim of this paper was to develop a novel macro-quality index driven by big quality data regarding the macro-quality management demands of manufacturing enterprises in Industry 4.0. Firstly, the connotation of big data of macro-quality management in manufacturing industry is expounded, which is the collection of the quality data in product lifecycle including the product quality data, process quality data and organizational ability data. Secondly, referring to the customer satisfaction index theory, a new big data oriented macro-quality index computation model based on the partial least square-structural equation modeling(PLS-SEM) theory is proposed, and the partial least square(PLS) is adopted to estimate the path parameters. Finally, a case study of macro-quality situation evaluation for the manufacturing industry of a city in China is presented. The final result shows that the proposed macro-quality index model is applicable and predictable.</abstract>
    <year>2016</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICIICII.2016.0052</doi>
  </row>
  <row>
    <author>Huang, Xiaotao and Qin, Niannian and Zhang, Xiaofang and Wang, Fen</author>
    <title>Experimental teaching design and practice on big data course</title>
    <keywords>Big Data;Education;Data mining;Clustering algorithms;Data visualization;Classification algorithms;Industries;Experimental teaching;Big data course;Big data processing lifecycle;Big data project;Big data</keywords>
    <abstract>With the rapid development of big data technology and the rapid growth of big data industry market, big data talent demand is also a substantial increase in China. In order to cultivate more talented people satisfying the needs of the community, we have designed the big data course for undergraduates. The big data course stresses not only on many theories but also lots of practice. The project of “big data talent development trend analysis” is designed in the experimental teaching on big data. By doing this project, students can master all the technologies of big data processing lifecycle, including data collection, data preprocessing, data mining and data visualization. We evaluate students who master big data core technology with a multi-evaluation method and design the experiment evaluation system on big data. Through our two years' practice, the results show that all these designs have achieved the good effect and improved the teaching quality.</abstract>
    <year>2017</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICCSE.2017.8085555</doi>
  </row>
  <row>
    <author>Zheng, Yiyi</author>
    <title>Computer-Aided Realization of Innovative Clothing Design under the Background of Big Data: from the Perspective of Image Quality Evaluation</title>
    <keywords>Image quality;Industries;Technological innovation;Design automation;Clothing;Production;Big Data;Image Quality Evaluation;Big Data;Computer-Aided Realization;Innovative Clothing Design</keywords>
    <abstract>This paper studies the computer-aided realization of innovative clothing design under the background of big data from the perspective of image quality evaluation. Now it explains the application of computer technology in the production of renderings, analyzes the current situation of computer-aided design in clothing design, and proposes some application strategies. To promote the innovative development of the apparel design industry. Combining the background of big data, this article proposes an innovative clothing design strategy based on image quality evaluation. Starting from the various elements of clothing design, from the perspective of data mining, discover fashion elements to achieve the purpose of clothing design innovation, so as to meet the ever-changing social needs.</abstract>
    <year>2022</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICSCDS53736.2022.9760944</doi>
  </row>
  <row>
    <author>Haug, Frank S.</author>
    <title>Bad big data science</title>
    <keywords>Big data;Metadata;Distributed databases;Stakeholders;Data science;Big Data;Data Quality;Data Warehousing;Distributed Database;Metadata</keywords>
    <abstract>As hardware and software technologies have improved, our definition of a “manageable amount of data” has increased in its scope dramatically. The term “big data” can be applied to any of several different projects and technologies sharing the ultimate goal of supporting analysis on these large, heterogeneous, and evolving data sets. The term “data science” refers to the statistical, technical, and domain-specific knowledge required to ensure that the analysis is done properly. Techniques for managing some common causes for bad data and invalid analysis have been used in other areas, such as data warehousing and distributed database. However, big data projects face special challenges when trying to combine big data and data science without producing inaccurate, misleading, or invalid results. This paper discusses potential causes for “bad big data science”, focusing primarily on the data quality of the input data, and suggests methods for minimizing them based on techniques originally developed for data warehousing and distributed database projects.</abstract>
    <year>2016</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData.2016.7840935</doi>
  </row>
  <row>
    <author>Wang, Jinghan and Zhang, Jinnan and Yuan, XueGuang and Tang, Yu and Hao, Hongyu and Zuo, Yong and Tan, Zebin and Qiao, Min and Cao, Yang Hua and Ai, Lingmei and Wan, Yihang and Chen, Hao</author>
    <title>Air quality data analysis and forecasting platform based on big data</title>
    <keywords>Air quality;Big Data;Data mining;Photonics;Optical fiber communication;Telecommunications;Clustering algorithms;Air quality;Big data;Data mining;Data visualization</keywords>
    <abstract>Nowadays, with the continuous development of big data technology, various industries use big data technology to process and mine massive data, and realize the value of data efficiently. In terms of air quality data processing, big data technology can also play a certain advantage. The platform is based on big data technology to design an air quality data analysis and prediction platform including data layer, business layer, interaction layer and visualization platform. Data is cleaned, calibrated, and stored in the data layer to ensure data consistency, integrity, and security. The air quality data is analyzed and predicted at the business layer. The interaction layer includes the functions of algorithm management, data query, and the data visualization platform provides intuitive information display. This design is a significant application for fully exploiting environmental data information. It has powerful data processing functions and scalability, which is a reliable data analysis and prediction platform.</abstract>
    <year>2019</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/CAC48633.2019.8996332</doi>
  </row>
  <row>
    <author>Challa, Jagat Sesh and Goyal, Poonam and Nikhil, S. and Mangla, Aditya and Balasubramaniam, Sundar S. and Goyal, Navneet</author>
    <title>DD-Rtree: A dynamic distributed data structure for efficient data distribution among cluster nodes for spatial data mining algorithms</title>
    <keywords>Data structures;Clustering algorithms;Data mining;Indexing;Distributed databases;Algorithm design and analysis;Data mining;data distribution;spatial locality;neighborhood queries;k-NN queries;density based clustering</keywords>
    <abstract>Parallelizing data mining algorithms has become a necessity as we try to mine ever increasing volumes of data. Spatial data mining algorithms like Dbscan, Optics, Slink, etc. have been parallelized to exploit a cluster infrastructure. The efficiency achieved by existing algorithms can be attributed to spatial locality preservation using spatial indexing structures like k-d-tree, quad-tree, grid files, etc. for distributing data among cluster nodes. However, these indexing structures are static in nature, i.e., they need to scan the entire dataset to determine the partitioning coordinates. This results in high data distribution cost when the data size is large. In this paper, we propose a dynamic distributed data structure, DD-Rtree, which preserves spatial locality while distributing data across compute nodes in a shared nothing environment. Moreover, DD-Rtree is dynamic, i.e., it can be constructed incrementally making it useful for handling big data. We compare the quality of data distribution achieved by DD-Rtree with one of the recent distributed indexing structure, SD-Rtree. We also compare the efficiency of queries supported by these indexing structures along with the overall efficiency of DBSCAN algorithm. Our experimental results show that DD-Rtree achieves better data distribution and thereby resulting in improved overall efficiency.</abstract>
    <year>2016</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData.2016.7840586</doi>
  </row>
  <row>
    <author>Shao, Jun and Xu, Daqi and Feng, Chun and Chi, Mingmin</author>
    <title>Big data challenges in China centre for resources satellite data and application</title>
    <keywords>Satellites;Remote sensing;Big Data;Monitoring;Land observing satellites;data center;big data;data challenges</keywords>
    <abstract>China Centre for Resources Satellite Data and Application (abbreviate as CRESDA) is a core platform to store, process, analyze, and distribute land observing satellite data in China. It can provide high quality and effective services for the State Council and the relevant departments of government and local authorities. In the era of big data, the data center benefits from big data opportunities as well as suffering from big data challenges. In the paper, the big data challenges of the CRESDA are summarized. In particular, four major challenges are comprised of the 3V dimensions of big data (i.e. Volume, Variety, and Velocity) and one specific challenge (i.e., extensibility) in the data center.</abstract>
    <year>2015</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/WHISPERS.2015.8075482</doi>
  </row>
  <row>
    <author>Bai, Zhongxian and Zhuo, Rongqing</author>
    <title>Quality Management of Crowd Sensing Data Based on Machine Learning</title>
    <keywords>Time series analysis;Temperature sensors;Mobile handsets;Temperature distribution;Databases;Data models;Machine Learning;Crowd Sensing;Big Data Analysis;Abnormal Data Detection Management;Clustering Method</keywords>
    <abstract>Recently, research on crowd sensing data quality management is a new subject area developed based on wireless sensor network related concepts. Crowd sensing data has also experienced many links in the process of network propagation, so it is inevitable that there are abnormal data in the database. Therefore, how to filter these unreliable data to get more real data is particularly important. This paper takes somatosensory temperature as an example, solves the problem of calculating the similarity of unequal long-term sequences by using DTW technology, and then clusters and compares the data in the database to find out the abnormal data. Thereby, the accuracy of the somatosensory temperature database is improved, and relevant users can obtain more accurate information. The experimental results show that when the minimum DTW value exceeds the threshold t = 0.7, the more the number of simulation sequences, the more stable the accuracy rate, and the faster the growth rate of the running time.</abstract>
    <year>2020</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/CIBDA50819.2020.00049</doi>
  </row>
  <row>
    <author>Debattista, Jeremy and Lange, Christoph and Scerri, Simon and Auer, Sören</author>
    <title>Linked 'Big' Data: Towards a Manifold Increase in Big Data Value and Veracity</title>
    <keywords>Big data;Semantics;Resource description framework;Data models;Encyclopedias;Vocabulary;linked data;Web of Data;Veracity;Value;Big Data dimension</keywords>
    <abstract>The Web of Data is an increasingly rich source of information, which makes it useful for Big Data analysis. However, there is no guarantee that this Web of Data will provide the consumer with truthful and valuable information. Most research has focused on Big Data's Volume, Velocity, and Variety dimensions. Unfortunately, Veracity and Value, often regarded as the fourth and fifth dimensions, have been largely overlooked. In this paper we discuss the potential of Linked Data methods to tackle all five V's, and particularly propose methods for addressing the last two dimensions. We draw parallels between Linked and Big Data methods, and propose the application of existing methods to improve and maintain quality and address Big Data's veracity challenge.</abstract>
    <year>2015</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BDC.2015.34</doi>
  </row>
  <row>
    <author>Zuo, Yiming and Vernica, Rares and Lei, Yang and Barcelo, Steven and Rogacs, Anita</author>
    <title>A Big Data Platform for Surface Enhanced Raman Spectroscopy Data with an Application on Image-Based Sensor Quality Control</title>
    <keywords>Quality control;Servers;Cloud computing;Big Data;Visualization;Inspection;Databases;Big data platform, Surface Enhanced Raman Spectroscopy, sensor quality control</keywords>
    <abstract>Surface-enhanced Raman spectroscopy (SERS) significantly enhances the Raman scattering by molecules, enabling detection and identification of small quantities of relevant bio-/chemical markers in a wide range of applications. In this paper, we present a big data platform with both a local client and cloud server built for acquiring, processing, visualizing and storing SERS sensor data. The local client controls the hardware (i.e., spectrometer and stage) to collect SERS spectra from HP designed sensors, and offers the options to analyze, visualize and save the spectra with meta-data records, including relevant experimental conditions. The cloud server contains remote databases and web interface for centralized data management to users from different locations. Here we describe how this platform was built and demonstrate its use for automated sensor quality control based on sensor images. Sensor quality control is a common practice, employed in sensor production to select high performing sensors. Image-based approach is a natural way to perform sensor quality control without destructing the sensors. Automating this process using the proposed platform can also reduce the time spent and achieve consistent result by avoiding human visual inspection.</abstract>
    <year>2019</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/MIPR.2019.00093</doi>
  </row>
  <row>
    <author>Sinaeepourfard, Amir and Garcia, Jordi and Masip-Bruin, Xavier and Marín-Tordera, Eva</author>
    <title>Towards a Comprehensive Data LifeCycle Model for Big Data Environments</title>
    <keywords>Data models;Adaptation models;Biological system modeling;Big Data;Organizations;Data acquisition;Computational modeling;Big Data;Data LifeCycle;Data Management;Data Organization;Data Complexity;Vs Challenges</keywords>
    <abstract>A huge amount of data is constantly being produced in the world. Data coming from the IoT, from scientific simulations, or from any other field of the eScience, are accumulated over historical data sets and set up the seed for future Big Data processing, with the final goal to generate added value and discover knowledge. In such computing processes, data are the main resource, however, organizing and managing data during their entire life cycle becomes a complex research topic. As part of this, Data LifeCycle (DLC) models have been proposed to efficiently organize large and complex data sets, from creation to consumption, in any field, and any scale, for an effective data usage and big data exploitation. 2. Several DLC frameworks can be found in the literature, each one defined for specific environments and scenarios. However, we realized that there is no global and comprehensive DLC model to be easily adapted to different scientific areas. For this reason, in this paper we describe the Comprehensive Scenario Agnostic Data LifeCycle (COSA-DLC) model, a DLC model which: i) is proved to be comprehensive as it addresses the 6Vs challenges (namely Value, Volume, Variety, Velocity, Variability and Veracity, and ii), it can be easily adapted to any particular scenario and, therefore, fit the requirements of a specific scientific field. In this paper we also include two use cases to illustrate the ease of the adaptation in different scenarios. We conclude that the comprehensive scenario agnostic DLC model provides several advantages, such as facilitating global data management, organization and integration, easing the adaptation to any kind of scenario, guaranteeing good data quality levels and, therefore, saving design time and efforts for the scientific and industrial communities.</abstract>
    <year>2016</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi/>
  </row>
  <row>
    <author>Xiang, Haiyun</author>
    <title>The Role and Application of Big Data Technology in Decision-Making Management of Colleges And Universities</title>
    <keywords>Industries;Education;Decision making;Standardization;Big Data;Data warehouses;Data science;big data;Colleges and universities;Decision-making management;Internal governance;data analysis</keywords>
    <abstract>with the advent of the era of big data, people’s way of thinking and habits have changed greatly. In Colleges and universities, big data has the characteristics of massive, high growth and diversity. Colleges and universities should seize the opportunity to tap the value of campus big data. Data driven self-development, from the ideological and political education, teaching quality, advantageous disciplines, social services, scientific research achievements and international influence and other aspects of data collection, extraction, cleaning, association analysis and mining, construction of university data "one table" project; and on this basis, build big data auxiliary system to assist managers in administrative decision-making, in order to promote the comprehensive construction of universities It brings new ideas for the management of colleges and universities in the new era.</abstract>
    <year>2021</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICDSBA53075.2021.00038</doi>
  </row>
  <row>
    <author>Doku, Ronald and Rawat, Danda B. and Liu, Chunmei</author>
    <title>Towards Federated Learning Approach to Determine Data Relevance in Big Data</title>
    <keywords>Data models;Blockchain;Machine learning;Mobile handsets;Data privacy;Cryptography;Big Data;Federated Learning Approach, Data Relevance, Big Data Analytics, Machine Learning</keywords>
    <abstract>In the past few years, data has proliferated to astronomical proportions; as a result, big data has become the driving force behind the growth of many machine learning innovations. However, the incessant generation of data in the information age poses a needle in the haystack problem, where it has become challenging to determine useful data from a heap of irrelevant ones. This has resulted in a quality over quantity issue in data science where a lot of data is being generated, but the majority of it is irrelevant. Furthermore, most of the data and the resources needed to effectively train machine learning models are owned by major tech companies, resulting in a centralization problem. As such, federated learning seeks to transform how machine learning models are trained by adopting a distributed machine learning approach. Another promising technology is the blockchain, whose immutable nature ensures data integrity. By combining the blockchain's trust mechanism and federated learning's ability to disrupt data centralization, we propose an approach that determines relevant data and stores the data in a decentralized manner.</abstract>
    <year>2019</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/IRI.2019.00039</doi>
  </row>
  <row>
    <author>Faroukhi, Abou Zakaria and El Alaoui, Imane and Gahi, Youssef and Amine, Aouatif</author>
    <title>Big Data Value Chain: A Unified Approach for Integrated Data Quality and Security</title>
    <keywords>Big Data;Security;Data integrity;Data models;Organizations;Decision making;Reliability;Big Data Value Chain;Data Management;Data Quality;Data Security;Process Integration;Orchestration</keywords>
    <abstract>Big Data has grown significantly in recent years. This growth has led organizations to adopt Big Data Value Chains (BDVC) as the appropriate framework for unlocking the value to make suitable decisions. Despite its promising opportunities, Big Data raises new concerns such as data quality and security that could radically impact the effectiveness of the BDVC. These two essential aspects have become an urgent need for any Big Data project to provide meaningful datasets and reliable insights. In this contribution, we highlight the importance of considering data quality and security requirements. Then, we propose a coherent, unified framework that extends BDVC with security and quality aspects. Through quality and security reports, the model can self-evaluate and arrange tasks according to orchestration and monitoring process, allowing the BDVC to evolve at the organization pace and to align strategically with its objectives as well as to federate a sustainable ecosystem.</abstract>
    <year>2020</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICECOCS50124.2020.9314391</doi>
  </row>
  <row>
    <author>Scavuzzo, Marco and Di Nitto, Elisabetta and Ardagna, Danilo</author>
    <title>[Journal First] Experiences and Challenges in Building a Data Intensive System for Data Migration</title>
    <keywords>Big Data;Databases;Quality of service;Tools;Fault tolerance;Fault tolerant systems;Software;Data intensive applications;Experiment driven action research;Big data;Data migration</keywords>
    <abstract>Recent analyses[2, 4, 5] report that many sectors of our economy and society are more and more guided by data-driven decision processes (e.g., health care, public administrations, etc.). As such, Data Intensive (DI) applications are becoming more and more important and critical. They must be fault-tolerant, they should scale with the amount of data, and be able to elastically leverage additional resources as and when these last ones are provided [3]. Moreover, they should be able to avoid data drops introduced in case of sudden overloads and should offer some Quality of Service (QoS) guarantees. Ensuring all these properties is, per se, a challenge, but it becomes even more difficult for DI applications, given the large amount of data to be managed and the significant level of parallelism required for its components. Even if today some technological frameworks are available for the development of such applications (for instance, think of Spark, Storm, Flink), we still lack solid software engineering approaches to support their development and, in particular, to ensure that they offer the required properties in terms of availability, throughput, data loss, etc. In fact, at the time of writing, identifying the right solution can require several rounds of experiments and the adoption of many different technologies. This implies the need for highly skilled persons and the execution of experiments with large data sets and a large number of resources, and, consequently, a significant amount of time and budget. To experiment with currently available approaches, we performed an action research experiment focusing on developing- testing-reengineering a specific DI application, Hegira4Cloud, that migrates data between widely used NoSQL databases, including so-called Database as a Service (DaaS), as well as on-premise databases. This is a representative DI system because it has to handle large volumes of data with different structures and has to guarantee that some important characteristics, in terms of data types and transactional properties, are preserved. Also, it poses stringent requirements in terms of correctness, high performance, fault tolerance, and fast and effective recovery. In our action research, we discovered that the literature offered some high level design guidelines for DI applications, as well as some tools to support modelling and QoS analysis/simulation of complex architectures, however the available tools were not yet. suitable to support DI systems. Moreover, we realized that the available big data frameworks we could have used were not flexible enough to cope with all possible application-specific aspects of our system. Hence, to achieve the desired level of performance, fault tolerance and recovery, we had to adopt a time-consuming, experiment-based approach [1, 6], which, in our case, consisted of three iterations: (1) the design and implementation of a Mediation Data Model capable of managing data extracted from different databases, together with a first monholitical prototype of Hegira4Cloud; (2) the improvement of performance of our prototype when managing and transferring huge amounts of data; (3) the introduction of fault-tolerant data extraction and management mechanisms, which are independent from the targeted databases. Among the others, an important issue that has forced us to reiterate in the development of Hegira4Cloud concerned the DaaS we interfaced with. In particular these DaaS, which are well-known services with a large number of users: (1) were missing detailed information regarding the behaviour of their APIs; (2) did not offer a predictable service; (3) were suffering of random downtimes not correlated with the datasets we were experimenting with. In this journal first presentation, we describe our experience and the issues we encountered that led to some important decisions during the software design and engineering process. Also, we analyse the state of the art of software design and verification tools and approaches in the light of our experience, and identify weaknesses, alternative design approaches and open challenges that could generate new research in these areas. More details can be found in the journal publication.</abstract>
    <year>2018</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1145/3180155.3182534</doi>
  </row>
  <row>
    <author>Chenran, Xiong and Youde, Wu</author>
    <title>The Geographic Environment Analysis of Regional Economic Development of Yunnan Province of China Based on the Big Data Technology</title>
    <keywords>Transportation;Big data;Smart cities;analysis of big data;Yunnan province of China;regional economic development;geographic environment</keywords>
    <abstract>This paper uses the analysis methods of data mining, data quality and management, semantic engine and prediction in big data to analyze the geographic environment of the Yunnan's economic development from its special location and geographic elements, beholding that the geographic elements lay basic material conditions for its economic development of Yunnan province. Geographic factors on one hand encourage the economic development and on the other hand affect together the economic development of Yunnan province.</abstract>
    <year>2015</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICITBS.2015.220</doi>
  </row>
  <row>
    <author>Xu, Xijiao and He, Huanming and Song, Wei and Gong, Jiayu</author>
    <title>Analysis on the Quality Model of Big Data Software</title>
    <keywords>Analytical models;Information science;Computational modeling;Software quality;Big Data;Data models;Computational complexity;Big Data;the Quality Requirements;Software Model</keywords>
    <abstract>With the rapid development of the big data system, The big data system has the characteristics of large data scale, diverse data and high computational complexity. Its testing method has to be constantly improved. By analyzing the general software quality model, and combining the characteristics of the big data software, a set of quality model for the big data software is formed.</abstract>
    <year>2021</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICIS51600.2021.9516862</doi>
  </row>
  <row>
    <author>Benbernou, Salima and Ouziri, Mourad</author>
    <title>Enhancing data quality by cleaning inconsistent big RDF data</title>
    <keywords>Resource description framework;Ontologies;Sparks;Cleaning;Inference algorithms;Automobiles;Jacobian matrices;Big data;Data quality;Inconsistency;Logical inference;RDF data cleaning;Apache Spark ecosystems</keywords>
    <abstract>We address the problem of dealing with inconsistencies in fusion of big data sources using Resource Description Framework (RDF) and ontologies. We propose a scalable approach ensuring data quality for query answering over big RDF data in a distributed way on a Spark ecosystem. In so doing, the cleaning inconsistent big RDF data approach is built on the following steps (1) modeling consistency rules to detect the inconsistency triples even if it is implicitly hidden including inference and inconsistent rules (2) detecting inconsistency through rule evaluation based on Apache Spark framework to discover the minimally sub-set of inconsistent triples (3) cleaning the inconsistency through finding the best repair for consistent query answering.</abstract>
    <year>2017</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData.2017.8257913</doi>
  </row>
  <row>
    <author>Shrivastava, Shrey and Patel, Dhaval and Bhamidipaty, Anuradha and Gifford, Wesley M. and Siegel, Stuart A. and Ganapavarapu, Venkata Sitaramagiridharganesh and Kalagnanam, Jayant R.</author>
    <title>DQA: Scalable, Automated and Interactive Data Quality Advisor</title>
    <keywords>Data integrity;Machine learning;Pipelines;Cleaning;Libraries;Buildings;Data quality;machine learning;data cleaning;scalability;automation;data science</keywords>
    <abstract>Fueled with growth in the fields of Internet of Things (IoT) and Big Data, data has become one of the most valuable assets in today's world. While we are leveraging this data for analyzing complex systems using machine learning and deep learning, a considerable amount of time and effort is spent on addressing data quality issues. If undetected, data quality issues can cause large deviations in the analysis, misleading data scientists. To ease the effort of identifying and addressing data quality challenges, we introduce DQA, a scalable, automated and interactive data quality advisor. In this paper, we describe the DQA framework, provide detailed description of its components and the benefits of integrating it in a data science process. We propose a programmatic approach for implementing the data quality framework which automatically generates dynamic executable graphs for performing data validations fine-tuned for a given dataset. We discuss the use of DQA to build a library of validation checks common to many applications. We provide insight into how DQA addresses many persistence and usability issues which currently make data cleaning a laborious task for data scientists. Finally, we provide a case study of how DQA is implemented in a realworld system and describe the benefits realized.</abstract>
    <year>2019</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData47090.2019.9006187</doi>
  </row>
  <row>
    <author>Han, Wei and Wu, Shenggang and Liu, Qiang and Cai, Jianzhen</author>
    <title>Research and Design of Construction Engineering Quality Management System Based on Big Data and BIM Technology</title>
    <keywords>Cloud computing;5G mobile communication;Project management;Transforms;Big Data;Digital twins;Internet of Things;Big data;BIM;quality management</keywords>
    <abstract>With the rapid development of information technologies such as big data, cloud computing, the Internet of Things, and 5G in recent years, the construction industry, as the traditional industry, urgently requests the transformation of its project management from extensive and low-efficiency mode to high-quality development mode. Under the background of the gradual development of newly-emerged technologies and concepts such as BIM technology, smart construction, and digital twin and based on the full investigation of the quality management requirements of construction enterprises, closed-loop management is focused on the whole process of construction engineering. From the perspective of construction engineering quality management, current theoretical tools such as the Internet of Things, big data, and BIM technology are combined. Taking Python as a basis, the mature and open-source WEB framework, database, and front-end and back-end technologies are used to design and construct a set of construction engineering quality management systems. We explore the digital potential of construction engineering quality management systems to provide a new way of thinking for the informatization, systematization, and system process-oriented development of construction engineering quality management.</abstract>
    <year>2022</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICCBE56101.2022.9888215</doi>
  </row>
  <row>
    <author>Cao, Rui and Gao, Jing</author>
    <title>Research on reliability evaluation of big data system</title>
    <keywords>Big Data;Fault trees;Software;Software reliability;Data models;Hardware;big data system;reliability;fault tree;evaluation</keywords>
    <abstract>The application of big data system is now more pervasive. The reliability of the large data system is crucial to both the academic and the industry. However, to date there are few studies on the reliability of the big data system, and lack of evaluation model. This paper uses the fault tree to model the reliability of the big data system on the cloud. The type of faults is summarized and the cause of fault is analyzed by experiments. The fault tree analysis (FTA) is used to evaluate the reliability of the big data system, which can provide reference for the fault processing and quality assurance of big data system.</abstract>
    <year>2018</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICCCBDA.2018.8386523</doi>
  </row>
  <row>
    <author>Rettig, Laura and Khayati, Mourad and Cudré-Mauroux, Philippe and Piórkowski, Michal</author>
    <title>Online anomaly detection over Big Data streams</title>
    <keywords>Entropy;Measurement;Correlation;Big data;Data structures;Yttrium;Sparks</keywords>
    <abstract>Data quality is a challenging problem in many real world application domains. While a lot of attention has been given to detect anomalies for data at rest, detecting anomalies for streaming applications still largely remains an open problem. For applications involving several data streams, the challenge of detecting anomalies has become harder over time, as data can dynamically evolve in subtle ways following changes in the underlying infrastructure. In this paper, we describe and empirically evaluate an online anomaly detection pipeline that satisfies two key conditions: generality and scalability. Our technique works on numerical data as well as on categorical data and makes no assumption on the underlying data distributions. We implement two metrics, relative entropy and Pearson correlation, to dynamically detect anomalies. The two metrics we use provide an efficient and effective detection of anomalies over high velocity streams of events. In the following, we describe the design and implementation of our approach in a Big Data scenario using state-of-the-art streaming components. Specifically, we build on Kafka queues and Spark Streaming for realizing our approach while satisfying the generality and scalability requirements given above. We show how a combination of the two metrics we put forward can be applied to detect several types of anomalies - like infrastructure failures, hardware misconfiguration or user-driven anomalies - in large-scale telecommunication networks. We also discuss the merits and limitations of the resulting architecture and empirically evaluate its scalability on a real deployment over live streams capturing events from millions of mobile devices.</abstract>
    <year>2015</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData.2015.7363865</doi>
  </row>
  <row>
    <author>Stojanovic, Nenad and Dinic, Marko and Stojanovic, Ljiljana</author>
    <title>Big data process analytics for continuous process improvement in manufacturing</title>
    <keywords>Clustering algorithms;Big data;Process control;Partitioning algorithms;Manufacturing;Six sigma;Analytical models;Big data;manufactoring;quality control</keywords>
    <abstract>One of the most important challenges in manufacturing is the continuous process improvement that requires new insights about the behavior/quality control of processes in order to understand the optimization/improvement potential. The paper elaborates on usage of big data-driven clustering for an efficient discovering of real-time unusualities in the process and their route-cause analysis. Our approach extends traditional clustering algorithms (like k-Means) with methods for better understanding the nature of clusters and provides a very efficient big data realization. We argue that this approach paves the way for a new generation of quality management tools based on big data analytics that will extend traditional statistical process control and empower Lean Six Sigma through big data processing. The proposed approach has been applied for improving process control in Whirlpool (washing machine tests, factory in Italy) and we present the most important finding from the evaluation study.</abstract>
    <year>2015</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData.2015.7363900</doi>
  </row>
  <row>
    <author>Ezzine, Imane and Benhlima, Laila</author>
    <title>A Study of Handling Missing Data Methods for Big Data</title>
    <keywords>Data integrity;Big Data;Data models;Dictionaries;Machine learning algorithms;Machine learning;Measurement;data quality;missing data;big data;functional dependancy;master data;machine learning</keywords>
    <abstract>Improving data quality is not a recent field but in the context of big data this is a challenging area as there is a crucial need for data quality to, for example, increase the accuracy of big data analytics or avoid storing redundant data. Missing data is one of the major problem that faces the quality of data. There are several methods and approaches that have been used in relational databases to handle missing data most of which have been adapted to big data. This paper aims to provide an overview of some methods and approaches for handling missing data in big data contexts.</abstract>
    <year>2018</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/CIST.2018.8596389</doi>
  </row>
  <row>
    <author>Wrembel, Robert</author>
    <title>Still Open Problems in Data Warehouse and Data Lake Research: extended abstract</title>
    <keywords>Social networking (online);Soft sensors;Transforms;Data warehouses;Big Data applications;Data models;Security;data integration;data warehouse;data lake;big data;extract transform load;data processing workflow;data processing pipeline;data quality;ETL optimization;data source evolution;metadata</keywords>
    <abstract>During recent years, we observe a widespread of new data sources, especially all types of social media and IoT devices, which produce huge data volumes, whose content ranges from fully structured to totally unstructured. All these types of data are commonly referred to as big data. They are typically described by the three most important characteristics, called 3V [1], namely: an extremely large volume, a variety of data models and structures (data representations), as well as a high velocity at which data are generated. We argue that out of these three Vs, the most challenging is variety [2]. Such data need to be integrated and transformed into a common representation, which is suitable for analysis, in a similar manner as traditional (mainly table-like) data.</abstract>
    <year>2021</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/SNAMS53716.2021.9732098</doi>
  </row>
  <row>
    <author>Jiang, Wei and Ning, Xiuli and Xu, Yingcheng</author>
    <title>Review on Big Data Fusion Methods of Quality Inspection for Consumer Goods</title>
    <keywords>Big Data;Inspection;Data integration;Semantics;Hidden Markov models;Thesauri;Support vector machines;Big data of quality inspection;data fusion;named entity recognition;entity resolution;data conflict resolution</keywords>
    <abstract>Quality big data comes from a wide range, the problem is how to eliminate the structure barriers between various types of data from different sources, to achieve the effective integration for isolated or fragmented data, and then to mine the information, knowledge and wisdom according to the actual needs. It is urgent for the quality inspection departments to solve the problem of making a decision the first time and take preventive measures. This paper introduced the research status of named entity recognition, solid unified detection, data conflict resolution, data fusion method from the characteristics of quality inspection data. The existence problems are analyzed and the research direction is looking forward of the research on data fusion in this paper.</abstract>
    <year>2018</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/CSCloud/EdgeCom.2018.00025</doi>
  </row>
  <row>
    <author>Micic, Natasha and Neagu, Daniel and Campean, Felician and Zadeh, Esmaeil Habib</author>
    <title>Towards a Data Quality Framework for Heterogeneous Data</title>
    <keywords>Data integrity;Measurement;Warranties;Metadata;Cleaning;Big Data;Heterogeneous Data Sets;Data Quality;Metadata;Data Cleaning;Data Quality Assessment</keywords>
    <abstract>Every industry has significant data output as a product of their working process, and with the recent advent of big data mining and integrated data warehousing it is the case for a robust methodology for assessing the quality for sustainable and consistent processing. In this paper a review is conducted on Data Quality (DQ) in multiple domains in order to propose connections between their methodologies. This critical review suggests that within the process of DQ assessment of heterogeneous data sets, not often are they treated as separate types of data in need of an alternate data quality assessment framework. We discuss the need for such a directed DQ framework and the opportunities that are foreseen in this research area and propose to address it through degrees of heterogeneity.</abstract>
    <year>2017</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/iThings-GreenCom-CPSCom-SmartData.2017.28</doi>
  </row>
  <row>
    <author>Karafili, Erisa and Lupu, Emil C. and Cullen, Alan and Williams, Bill and Arunkumar, Saritha and Calo, Seraphin</author>
    <title>Improving data sharing in data rich environments</title>
    <keywords>Big Data;Drones;Data privacy;Electronic mail;Access control;Big data;data sharing;data access;usage control;DSAs;drone systems;military scenario</keywords>
    <abstract>The increasing use of big data comes along with the problem of ensuring correct and secure data access. There is a need to maximise the data dissemination whilst controlling their access. Depending on the type of users different qualities and parts of data are shared. We introduce an alteration mechanism, more precisely a restriction one, based on a policy analysis language. The alteration reflects the level of trust and relations the users have, and are represented as policies inside the data sharing agreements. These agreements are attached to the data and are enforced every time the data are accessed, used or shared. We show the use of our alteration mechanism with a military use case, where different parties are involved during the missions, and they have different relations of trust and partnership.</abstract>
    <year>2017</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData.2017.8258270</doi>
  </row>
  <row>
    <author>Zhang, Pengcheng and Zhou, Xuewu and Li, Wenrui and Gao, Jerry</author>
    <title>A Survey on Quality Assurance Techniques for Big Data Applications</title>
    <keywords>Big Data applications;Quality assurance;Testing;Data models;Monitoring;Computer architecture;Quality Assurance;Big data;Big data application;MDA;Testing;Verification;Fault tolerance;Monitoring;Prediction</keywords>
    <abstract>With the rapid advance of big data and cloud computing, building high quality big data systems in different application fields has gradually became a popular research topic in academia and industry as well as government agencies. However, more quality problems lead to application errors. Although the current research work has discussed how to ensure the quality of big data applications from several aspects, there is no systematic discussion on how to ensure the quality of large data applications. Therefore, a systematic study on big data application quality assurance is very necessary and critical. This paper focuses on the survey of quality assurance techniques of big data applications, and it introduces big data properties and quality attributes. It mainly discusses the key approaches to ensure the quality of big data applications and they are testing, model-driven architecture (MDA), monitoring, fault tolerance, verification and also prediction techniques. In addition, this paper also discusses the impact of big data characteristics on big data applications.</abstract>
    <year>2017</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigDataService.2017.42</doi>
  </row>
  <row>
    <author>Kumar, Sunil and Singh, Maninder</author>
    <title>A novel clustering technique for efficient clustering of big data in Hadoop Ecosystem</title>
    <keywords>Clustering algorithms;Big Data;Ocean temperature;Data mining;Meteorology;Partitioning algorithms;Temperature control;clustering;Hadoop;big data;k-means;hierarchical</keywords>
    <abstract>Big data analytics and data mining are techniques used to analyze data and to extract hidden information. Traditional approaches to analysis and extraction do not work well for big data because this data is complex and of very high volume. A major data mining technique known as data clustering groups the data into clusters and makes it easy to extract information from these clusters. However, existing clustering algorithms, such as k-means and hierarchical, are not efficient as the quality of the clusters they produce is compromised. Therefore, there is a need to design an efficient and highly scalable clustering algorithm. In this paper, we put forward a new clustering algorithm called hybrid clustering in order to overcome the disadvantages of existing clustering algorithms. We compare the new hybrid algorithm with existing algorithms on the bases of precision, recall, F-measure, execution time, and accuracy of results. From the experimental results, it is clear that the proposed hybrid clustering algorithm is more accurate, and has better precision, recall, and F-measure values.</abstract>
    <year>2019</year>
    <ENTRYTYPE>article</ENTRYTYPE>
    <doi>10.26599/BDMA.2018.9020037</doi>
  </row>
  <row>
    <author>Wang, Xin and Zhao, Xinbin and Yu, Liling</author>
    <title>Data Mining on the Flight Quality of an Airline based on QAR Big Data</title>
    <keywords>Quality assurance;Atmospheric modeling;Big Data;Data models;Time measurement;Safety;Mathematical model;flight quality;pitch;QAR data;normal distribution;t test</keywords>
    <abstract>At present, the airlines have made some achievements in event analysis and investigation by using their quick access record (QAR) data. But where each airline's flight quality is in the industry, and whether there is a problem in itself, the airline can't find. In order to help airlines discover the existing flight quality problems, this article uses the QAR big data of the flight operational quality assurance (FOQA) Station of CAAC, and compares the industry-wide QAR data with the QAR data of individual airlines, and founds that the take-off pitch angle of a certain aircraft of A321 models is too small, by using mathematical statistics t test to verify, found the airline's the take-off pitch angle and the industry's the take-off pitch angle exist significant difference. The correlative speed at rotation and the speed at liftoff are also analyzed, and the significant difference is found. The FOQA Station of CAAC feeds back the problem to the airline and the authority. After the investigation of the airline and the authority, there are problems with the airline. And the airline immediately starts to rectify it.</abstract>
    <year>2020</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICCASIT50869.2020.9368701</doi>
  </row>
  <row>
    <author>Poon, Lex and Farshidi, Siamak and Li, Na and Zhao, Zhiming</author>
    <title>Unsupervised Anomaly Detection in Data Quality Control</title>
    <keywords>Data integrity;Decision making;Process control;Quality control;Organizations;Big Data;Data models;data quality control;data quality assessment;unsupervised learning;anomaly detection;automated data quality control</keywords>
    <abstract>Data is one of the most valuable assets of an organization and has a tremendous impact on its long-term success and decision-making processes. Typically, organizational data error and outlier detection processes perform manually and reactively, making them time-consuming and prone to human errors. Additionally, rich data types, unlabeled data, and increased volume have made such data more complex. Accordingly, an automated anomaly detection approach is required to improve data management and quality control processes. This study introduces an unsupervised anomaly detection approach based on models comparison, consensus learning, and a combination of rules of thumb with iterative hyper-parameter tuning to increase data quality. Furthermore, a domain expert is considered a human in the loop to evaluate and check the data quality and to judge the output of the unsupervised model. An experiment has been conducted to assess the proposed approach in the context of a case study. The experiment results confirm that the proposed approach can improve the quality of organizational data and facilitate anomaly detection processes.</abstract>
    <year>2021</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData52589.2021.9671672</doi>
  </row>
  <row>
    <author>Won, Heesun and Nguyen, Minh Chau and Gil, Myeong-Seon and Moon, Yang-Sae</author>
    <title>An Advanced Open Data Platform for Integrated Support of Data Management, Distribution, and Analysis</title>
    <keywords>Analytical models;Conferences;Big Data;Metadata;Artificial intelligence;Open data;Portals;open data;data hub;data distribution;data sharing;open data platform</keywords>
    <abstract>With the growing applications of big data and artificial intelligence, the quality of the service is an outcome of the quality of the data. Nevertheless, there is still a significant lack of data that has practical application value. To solve these problems, we propose SODAS (Smart Open Data As a Service) as a novel open data platform for efficient data sharing and utilization. We first analyze the major problems in the legacy CKAN and then draw up their solutions through core strategies. We next define four components and nine function blocks of SODAS for each core strategy. As a result, SODAS drives Open Data Portal, Open Data Reference Model, DataMap Publisher, and ADE Provisioning (Analytics and Development Environment Provisioning) by connecting the defined function blocks. We confirm that each function works correctly through the SODAS Web portal and apply SODAS to actual data distribution sites to prove its efficiency and practical use. SODAS is the first open data platform that provides secure interoperability between heterogeneous platforms based on international standards and enables domain-free data management with flexible metadata.</abstract>
    <year>2021</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData52589.2021.9671504</doi>
  </row>
  <row>
    <author>Gan, Wenting</author>
    <title>Design of Network Precision Marketing Based on Big Data Analysis Technology</title>
    <keywords>Visualization;Data analysis;Databases;Data integrity;Semantics;Big Data;Internet;Big Data;E-commerce;Internet marketing;Marketing design</keywords>
    <abstract>In the process of big data processing, big data analysis is the core work content. After obtaining a large amount of data, we use related analysis technology to perform data processing and analysis to obtain knowledge. Its related content includes visual analysis, data mining, predictive analysis, semantic analysis and data quality management. How to obtain big data, classify and store according to data types, mine valuable information from big data, and effectively apply big data in the field of precision marketing are hot topics of research. On the basis of researching the key technologies of big data analysis, this paper uses Hadoop big data to analyze and store the massive online logs generated by users' mobile terminals, and calculates and builds user characteristic databases. We use relevant analysis technology to analyze the user's location information, browsing and usage habits, hobbies, and focus content. At the same time, a precise marketing model is established according to user behavior characteristics and attributes, thereby improving the marketing effect of the enterprise.</abstract>
    <year>2020</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ECIT50008.2020.00026</doi>
  </row>
  <row>
    <author>Xu, Birong and Wang, Weijiang and Wu, Yuyan and Shi, Yueting and Lu, Chang</author>
    <title>Internet of things and big data analytics for smart oil field malfunction diagnosis</title>
    <keywords>Oils;Production;Standards;Fluids;Moisture;Big Data;Algorithm design and analysis;internet of thing;warning thresholds scheme;oil field fault diagnosis;big data;6 sigma</keywords>
    <abstract>With the rapid development of information technology and digital communication, the data types are more abundant by integration of various technologies. In this paper, based on the analysis of a large number of historical data of oil and water wells, the changes of some important parameters of the wells can be monitored and then used in the trend prediction and the early warning system. Subsequently, we use 6 Sigma algorithm to process the historical data, and by the big data trend analysis combining with various parameters, we can diagnose six operating conditions, such as sand production, abnormal of moisture content etc. Through experiments, the algorithm is stable and reliable in practical application, and it has great significance to ensure the normal production of oil field and improve the management ability for oil field.</abstract>
    <year>2017</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/ICBDA.2017.8078802</doi>
  </row>
  <row>
    <author>Zhang, Guobao</author>
    <title>A data traceability method to improve data quality in a big data environment</title>
    <keywords>Data Governance;Data Credibility;Data Traceability</keywords>
    <abstract>In the actual project of data sharing and data governance, the problems of data heterogeneity and low data quality have not been solved. Data quality detection based on syntax rules can effectively find data quality problems, but it had not improved the data quality, especially in a variety of heterogeneous data source environments. This paper designs a data governance model based on data traceability, and we can get data feedback and revision through this model. The proposed method considers the different ownership of data and may form a closed-loop data service chain including effective data validation, data tracking, and data revision, data release. The analysis of the case shows that the method is effective to improve the data quality and meet the requirements of data security also.</abstract>
    <year>2020</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/DSC50466.2020.00051</doi>
  </row>
  <row>
    <author>Hee, Kim</author>
    <title>Is data quality enough for a clinical decision?: Apply machine learning and avoid bias</title>
    <keywords>Data mining;Medical diagnostic imaging;Currencies;Tools;Medical services;Measurement;data quality;decision quality;healthcare;clinical data;EHRs;data quality assurance;stratified sampling;bias</keywords>
    <abstract>This paper provides a practical guideline for the assurance and (re-)usage of clinical data. It proposes a process which aims to provide a systematic data quality assurance even without involving a medical domain expert. Especially when (re-)using clinical data, data quality is an important topic because clinical data are not purposely collected. Therefore, data driven conclusions might be false, because a given dataset is not representative. These false data driven conclusions could even harm the life of patients. Thus, all researchers should adhere to some basic principles that can prevent false conclusions. Twelve empirical experiments were conducted in order to prove that my process is able to assure data quality with respect to the descriptive and predictive analysis. Descriptive results obtained by applying stratified sampling are conflicting in four out of nine population inputs. Sampling is carried based on the top ranked feature drawn by the Contextual Data Quality Assurance (CDQA). Between datasets these features are confirmed by the Mutual Data Quality Assurance (MDQA). Stratified sampled inputs improve predictive results compared to raw data. Both Area Under the Curve (AUC) scores and accuracy scores increase by three percent.</abstract>
    <year>2017</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/BigData.2017.8258221</doi>
  </row>
  <row>
    <author>Liang, JingXuan</author>
    <title>Research on the Relationship Between Big Data Management, Informatization and Enterprise Performances of Textile and Garment Enterprises under the Background of Digital Economy</title>
    <keywords>Analytical models;Clothing;Production;Machine learning;Big Data;Data models;Business intelligence;Big data management;enterprise informatization;enterprise performance</keywords>
    <abstract>With the rapid development of digital economy, textile and garment enterprises begin to focus on digital and intelligent transformation to seek high-quality development. In this process, big data management and information construction are indispensable basic work. Through the analysis of the background and current situation, this study builds a model of the relationship between big data management, enterprise informatization and enterprise performance, aiming to explore how data management under big data technology affects textile and garment enterprises and its influence. By collecting data of textile and garment enterprises and conducting empirical analysis, it is concluded that big data technology will affect the level of enterprise informatization through big data management in production, research and development and marketing. The improvement of enterprise informatization level can significantly boost the increase of enterprise performance and play a certain intermediary role. The research results of this paper provide some suggestions for textile and garment enterprise managers in the aspects of technology introduction, enterprise internal management and enterprise strategy formulation.</abstract>
    <year>2021</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/MLBDBI54094.2021.00095</doi>
  </row>
  <row>
    <author>Guler, Emine Rumeysa and Ozdemir, Suat</author>
    <title>Applications of Stream Data Mining on the Internet of Things: A Survey</title>
    <keywords>Big Data;Deep learning;Python;Cyber terrorism;Mathematical model;Data mining;Nanoelectromechanical systems;IoT;Big Data Analytics;Deep Learning;Stream Data Mining;Data Processing Platforms</keywords>
    <abstract>In the era of the Internet of Things (IoT), enormous amount of sensing devices collect and/or generate various sensory data over time for a wide range of fields and applications. Based on the nature of the application, these devices result in big or fast/real time data streams. The analytics technique on the subject matter used to discover new information, anticipate future predictions and make decisions on important issues makes IoT technology valuable for both the business world and the quality of everyday life. In this study, first of all, the concept of IoT and its architecture and relation with big and streaming data are emphasized. Information discovery process applied to the IoT streaming data is investigated and deep learning frameworks covered by this process are described comparatively. Finally, the most commonly used tools for analyzing IoT stream data are introduced and their characteristics are revealed.</abstract>
    <year>2018</year>
    <ENTRYTYPE>inproceedings</ENTRYTYPE>
    <doi>10.1109/IBIGDELFT.2018.8625289</doi>
  </row>
</data>
