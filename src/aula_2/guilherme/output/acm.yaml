- ENTRYTYPE: inproceedings
  abstract: In a world increasingly connected, and in which information flows quickly
    and affects a very large number of people, sentiment analysis has seen a spectacular
    development over the past ten years. This is due to the fact that the explosion
    of social networks has allowed anyone with internet access to publicly express
    his opinion. Moreover, the emergence of big data has brought enormous opportunities
    and powerful storage and analytics tools to the field of sentiment analysis. However,
    big data introduces new variables and constraints that could radically affect
    the traditional models of sentiment analysis. Therefore, new concerns, such as
    big data quality, have to be addressed to get the most out of big data. To the
    best of our knowledge, no contributions have been published so far which address
    big data quality in SA throughout its different processes. In this paper, we first
    highlight the most important big data quality metrics to consider in any big data
    project. Then, we show how these metrics could be specifically considered in SA
    approaches and this for each phase in the big data value chain.
  author: El Alaoui, Imane and Gahi, Youssef and Messoussi, Rochdi
  doi: 10.1145/3341620.3341629
  keywords: Big data quality metrics, Big data, Sentiment analysis
  title: Big Data Quality Metrics for Sentiment Analysis Approaches
  year: '2019'
- ENTRYTYPE: inproceedings
  abstract: In recent years, as more and more data sources have become available and
    the volumes of data potentially accessible have increased, the assessment of data
    quality has taken a central role whether at the academic, professional or any
    other sector. Given that users are often concerned with the need to filter a large
    amount of data to better satisfy their requirements and needs, and that data analysis
    can be based on inaccurate, incomplete, ambiguous, duplicated and of poor quality,
    it makes everyone wonder what the results of these analyses will really be like.
    However, there is a very complex process involved in the identification of new,
    valid, potentially useful and meaningful data from a large data collection and
    various information systems, and is critically dependent on a number of measures
    to be developed to ensure data quality. To this end, the main objective of this
    paper is to introduce a general study on data quality related with big data, by
    providing what other researchers came up with on that subject. The paper will
    be finalized by a comparative study between the different existing data quality
    models.
  author: Reda, Oumaima and Sassi, Imad and Zellou, Ahmed and Anter, Samir
  doi: 10.1145/3419604.3419803
  keywords: Quality Models, Data Quality, Data Quality evaluation, Big Data
  title: Towards a Data Quality Assessment in Big Data
  year: '2020'
- ENTRYTYPE: inproceedings
  abstract: "Big Data (BD) solutions are designed to better support decision-making\
    \ processes in order to optimize organizational performance. These BD solutions\
    \ use company\u2019s core business data, using typically large datasets. However,\
    \ data that doesn\u2019t meet adequate quality levels will lead to BD solutions\
    \ that will not produce useful results, and consequently may not be used to make\
    \ adequate business decisions. For a long time, companies have collected and stored\
    \ large amounts of data without being able to exploit the advantage of exploring\
    \ it. Nowadays, and thanks to the Big Data explosion, organizations have begun\
    \ to recognize the need for estimating the value of their data and, vice-versa,\
    \ managing data accordingly to their value. This need of managing the Value of\
    \ data has led to the concept of Smart Data. It not only involves the datasets,\
    \ but also the set of technologies, tools, processes and methodologies that enable\
    \ all the Values from the data to the End-users (Business, data scientist, BI\u2026\
    ). Consequently, Smart data is data actionable. We discovered that data quality\
    \ is one of the most important issues when it comes to \u201Csmartizing\u201D\
    \ data. In this paper, we introduce a methodology to make data smarter, taking\
    \ as a reference point, the quality level of the data itself."
  author: Baldassarre, Maria Teresa and Caballero, Ismael and Caivano, Danilo and
    Rivas Garcia, Bibiano and Piattini, Mario
  doi: 10.1145/3281022.3281026
  keywords: Big Data, Smart Data, Data Quality
  title: 'From Big Data to Smart Data: A Data Quality Perspective'
  year: '2018'
- ENTRYTYPE: inproceedings
  abstract: As Big Data becomes better understood, there is a need for a comprehensive
    definition of Big Data to support work in fields such as data quality for Big
    Data. Existing definitions of Big Data define Big Data by comparison with existing,
    usually relational, definitions, or define Big Data in terms of data characteristics
    or use an approach which combines data characteristics with the Big Data environment.
    In this paper we examine existing definitions of Big Data and discuss the strengths
    and limitations of the different approaches, with particular reference to issues
    related to data quality in Big Data. We identify the issues presented by incomplete
    or inconsistent definitions. We propose an alternative definition and relate this
    definition to our work on quality in Big Data.
  author: Emmanuel, Isitor and Stanier, Clare
  doi: 10.1145/3010089.3010090
  keywords: Data Quality Dimensions, Big Data, Data Quality, Big Data characteristics
  title: Defining Big Data
  year: '2016'
- ENTRYTYPE: article
  abstract: .nan
  author: Cron, Andrew and Nguyen, Huy L. and Parameswaran, Aditya
  doi: 10.1145/2331042.2331045
  keywords: .nan
  title: Big Data
  year: '2012'
- ENTRYTYPE: inproceedings
  abstract: We provide a summary of the outcomes from the Workshop on Big Data Benchmarking
    (WBDB2012) held on May 8-9, 2012 in San Jose, CA. The workshop discussed a number
    of issues related to big data benchmarking definitions and benchmark processes,
    and was attended by 60 invitees representing 45 different organizations from industry
    and academia. Attendees were selected based on their experience and expertise
    in one or more areas of big data, database systems, performance benchmarking,
    and big data applications. The participants concluded that there exists both a
    need and an opportunity for defining benchmarks to capture the end-to-end aspects
    of big data applications. The metrics for such benchmarks would need to include
    metrics for performance as well as price/performance, and consider several costs
    including total system cost, setup cost, and energy costs. The next Workshop on
    Big Data Benchmarking is scheduled to be held on December 17-18, 2012 in Pune,
    India.
  author: Baru, Chaitan and Bhandarkar, Milind and Nambiar, Raghunath and Poess, Meikel
    and Rabl, Tilmann
  doi: 10.1145/2378356.2378368
  keywords: mbds
  title: Big Data Benchmarking
  year: '2012'
- ENTRYTYPE: article
  abstract: New user interfaces can transform how we work with big data, and raise
    exciting research problems that span human-computer interaction, machine learning,
    and distributed systems.
  author: Heer, Jeffrey and Kandel, Sean
  doi: 10.1145/2331042.2331058
  keywords: .nan
  title: Interactive Analysis of Big Data
  year: '2012'
- ENTRYTYPE: inproceedings
  abstract: This paper clarifies the concept of Big Data with a discussion of its
    managerial implications and presents its defining characteristics differentiating
    Big Data with traditional analytics. This paper also introduces the concept of
    Big Data in the context of three industries, namely, finance, supply chain and
    marketing and discusses how this concept can be applied in the business world.
    With regard to this concept, fundamental yet critical discussions were made for
    any further understanding of Big Data. Finally, this paper contributes to our
    current knowledge of Big Data by relating and contrasting Big Data to traditional
    analysis while presenting context specific discussions for its applications. Although
    technical aspects of Big Data were not covered in this paper, this paper focused
    on serving as a business discussion for the concept of Big Data. For future work,
    business contents must be related to technical capabilities and solutions in order
    to provide a better understanding of Big Data.
  author: Zhou, Ming and Cao, Menglin and Park, Taeho and Pyeon, Jae-Ho
  doi: 10.1145/2837060.2837068
  keywords: .nan
  title: 'Clarifying Big Data: The Concept and Its Applications'
  year: '2015'
- ENTRYTYPE: inproceedings
  abstract: The Facebook Data Infrastructure supports a wide range of applications,
    including both external facing products and services and internal applications.
    This paper focuses on the Data Warehousing and Analytics platform of Facebook
    that provides support for batch-oriented analytics applications. Facebook's data
    infrastructure is built largely on top of open-source technologies such as Apache
    Hadoop, HDFS, MapReduce and Hive, and provides a rich set of tools for different
    users to perform analytics queries on Facebook data. As the Facebook user base
    continues to grow, we continue to enhance our data platform in order to deal with
    the challenges of scaling with increasing amounts of data.
  author: Menon, Aravind
  doi: 10.1145/2378356.2378364
  keywords: mbds
  title: Big Data @ Facebook
  year: '2012'
- ENTRYTYPE: inproceedings
  abstract: 'In healthcare sector huge quantities of data about patients and their
    medical conditions have been gathered through clinical databases and various other
    healthcare processes. Currently, it records nearly all aspects of care, including
    patient personal information, clinical trials, hospital records, diagnosis, medication,
    test results, imaging data, costs, administrative reports, etc. Like in other
    application domains, the big data revolution holds also great promise in the area
    of healthcare, as the available data about individual patients is very rich, and
    hides crucial knowledge that can be exploited to improve patients'' care while
    reducing its cost. For instance, in 2012 worldwide collected healthcare data was
    estimated to be in the region of 500 petabytes and it is expected to grow 50 times
    more in 2020 (25 Exabytes). Turning this massive amount of data into knowledge
    that can be used to identify needs, predict and prevent critical patients'' conditions,
    and help practitioners to make rapid and accurate decisions is not only a desire
    but is of urgent and crucial necessity. Therefore, healthcare organisations must
    have the ability to manage and analyse their data in a rapid and efficient manner
    to answer several critical questions related to diseases, treatments, patients''
    behaviours, and care management. However, building such system faces huge challenges:
    1) data complexity, 2) Privacy, security, ethical, legal, and social issues, and
    3) Interoperability, portability, and compatibility. We will discuss all these
    challenges and the requirements of healthcare ecosystem. This will lead us to
    describe some innovative methodologies of how to build such ecosystem to face
    the healthcare challenges of the next decade or so.'
  author: Kechadi, M-Tahar
  doi: 10.1145/3010089.3010143
  keywords: Data Analytics, System Design, Big data, Healthcare Data, Sensor Data,
    Data Mining, Ecosystems
  title: 'Healthcare Big Data: Challenges and Opportunities'
  year: '2016'
- ENTRYTYPE: inproceedings
  abstract: 'Big Data brings the challenge for Smart Grid. By using the method of
    SWOT, the double-edged sword effect of Big Data for the Smart Grid has been analyzed.
    Big Data provides both opportunities and challenges. The benefits and opportunities
    are which, Big Data bringing data view, changing thinking methods and tools, expanding
    the application scene, providing better service to the society, enhancing the
    value of the opportunity. At the same time, Big Data will lead to the challenges
    in Smart Grid, for example, because of security challenges of Big Data itself,
    Big Data more concentrated, cause safety challenges in Smart Grid is more serious;
    the energy consumption challenges of Big Data; Big Data privacy threat Smart Grid.
    From the viewpoint of information theory of Shannon, the following conclusions
    can be reached: the electric power consumption is positively correlated closely
    with the volume of Big Data; the energy consumption of data grows exponentially.'
  author: Miao, Xin
  doi: 10.1145/2640087.2644175
  keywords: power consumption, Big Data, safety, architecture, data exploration, privacy
    protection, Smart Grid
  title: Big Data and Smart Grid
  year: '2014'
- ENTRYTYPE: inproceedings
  abstract: Recently, a great deal of interest for Big Data has risen, mainly driven
    from a widespread number of research problems strongly related to real-life applications
    and systems, such as representing, modeling, processing, querying and mining massive,
    distributed, large-scale repositories (mostly being of unstructured nature). Inspired
    by this main trend, in this paper we discuss three important aspects of Big Data
    research, namely OLAP over Big Data, Big Data Posting, and Privacy of Big Data.
    We also depict future research directions, hence implicitly defining a research
    agenda aiming at leading future challenges in this research field.
  author: Cuzzocrea, Alfredo and Sacc\`{a}, Domenico and Ullman, Jeffrey D.
  doi: 10.1145/2513591.2527071
  keywords: OLAP over big data, big data, big data posting, privacy of big data
  title: 'Big Data: A Research Agenda'
  year: '2013'
- ENTRYTYPE: article
  abstract: Three computer scientists from UC Irvine address the question "What's
    next for big data?" by summarizing the current state of the big data platform
    space and then describing ASTERIX, their next-generation big data management system.
  author: Borkar, Vinayak R. and Carey, Michael J. and Li, Chen
  doi: 10.1145/2331042.2331057
  keywords: .nan
  title: 'Big Data Platforms: What''s Next?'
  year: '2012'
- ENTRYTYPE: inproceedings
  abstract: Human-Centered Big Data Research (HCBDR) is an area of work that focuses
    on the methodologies and research areas focused on understanding how humans interact
    with "big data". In the context of this paper, we refer to "big data" in a holistic
    sense, including most (if not all) the dimensions defining the term, such as complexity,
    variety, velocity, veracity, etc. Simply put, big data requires us as researchers
    of to question and reconsider existing approaches, with the opportunity to illuminate
    new kinds of insights that were traditionally out of reach to humans.
  author: Endert, Alex and Szymczak, Samantha and Gunning, Dave and Gersh, John
  doi: 10.1145/2609876.2609890
  keywords: .nan
  title: Modeling in Big Data Environments
  year: '2014'
- ENTRYTYPE: inproceedings
  abstract: We report on the sensemaking breakout group at the Human Centered Big
    Data Research (HCBDR-2014) workshop. The authors are a multi-disciplinary team
    of invited researchers and stakeholders who participated in this breakout session.
    This report includes an overview of our discussions on the many research challenges
    associated with sensemaking within a big data environment. Specifically, we focused
    on key topics that fit squarely in the intersection of the sensemaking and big
    data research, as other communities already exist for decision making and big
    data technologies independently. As part of this effort, our group developed and
    proposed a framework around which this community can target and structure future
    research. This framework is intended to allow the community to systematically
    identify areas where innovative research might make large contributions to sensemaking
    in a big data environment.
  author: Argenta, Chris and Benson, Jordan and Bos, Nathan and Paletz, Susannah B.
    F. and Pike, William and Wilson, Aaron
  doi: 10.1145/2609876.2609889
  keywords: .nan
  title: Sensemaking in Big Data Environments
  year: '2014'
- ENTRYTYPE: inproceedings
  abstract: 'Big Data is a new trend regarded by both academics and business areas
    as an interesting concept. The paradigm includes the storage and processing of
    petabyte-size datasets, boosting knowledge discovery over data and providing organizations
    with competitive advantage over their contenders. This paper comes to provide
    an overview of the concept, answering questions that one has when faced with the
    term for the first time: What is Big Data and what are its advantages? How does
    it work? How is it accepted among enterprises? How to deploy a Big Data solution?'
  author: Neves, Pedro and Bernardino, Jorge
  doi: 10.1145/2790755.2790785
  keywords: SaaS, DBMS, IaaS, PaaS, Big Data
  title: Big Data Issues
  year: '2015'
- ENTRYTYPE: inproceedings
  abstract: Big Data Analytics in particular and Data Science in general have become
    key disciplines in the last decade. The convergence of Information Technology,
    Statistics and Mathematics, to explore and extract information from Big Data have
    challenged the way many industries used to operate, shifting the decision making
    process in many organizations. A new breed of Big Data platforms has appeared,
    to fulfill the needs to process data that is large, complex, variable and rapidly
    generated. The author describes the experience in this field from a company that
    provides Big Data analytics as its core business.
  author: Villanustre, Flavio
  doi: .nan
  keywords: declarative programming, distributed algorithms, abstraction models, big
    data, dataflow programming
  title: 'Industrial Big Data Analytics: Lessons from the Trenches'
  year: '2015'
- ENTRYTYPE: inproceedings
  abstract: The term "Big Data" became a buzzword and is widely used in both research
    and industrial worlds. Typically the concept of big data assumes a variety of
    different sources of information and velocity of complex analytical processing,
    rather than just a huge and growing volume of data. All variety, velocity, and
    volume create new research challenges, as nearly all techniques and tools commonly
    used in data processing have to be re-considered. Variety and uncertainty of big
    data require a mixture of exact and similarity search and grouping of complex
    objects based on different attributes. High-level declarative query languages
    are important in this context due to expressiveness and potential for optimization.In
    this talk we are mostly interested in an algebraic layer for complex query processing
    which resides between user interface (most likely, graphical) and execution engine
    in layered system architecture. We analyze the applicability of existing models
    and query languages. We describe a systematic approach to similarity handling
    of complex objects, simultaneous application of different similarity measures
    and querying paradigms, complex searching and querying, combined semi-structured
    and unstructured search. We introduce the adaptive abstract operations based on
    the concept of fuzzy set, which are needed to support uniform handling of different
    kinds of similarity processing. To ensure an efficient implementation, approximate
    algorithms with controlled quality are required to enable quality versus performance
    trade-off for timeliness of similarity processing. Uniform and adaptive operations
    enable high-level declarative definition of complex queries and provide options
    for optimization.
  author: Novikov, Boris and Vassilieva, Natalia and Yarygina, Anna
  doi: 10.1145/2383276.2383278
  keywords: computer systems and technologies, query processing, query languages,
    big data
  title: Querying Big Data
  year: '2012'
- ENTRYTYPE: inproceedings
  abstract: 'This paper reveals ten big characteristics (10 Bigs) of big data and
    explores their non-linear interrelationships through presenting a unified framework
    of big data. The framework has three levels: fundamental level, technological
    level, and socio-economic level. The fundamental level has four big fundamental
    characteristics of big data. The technological level consists of three big technological
    characteristics of big data. The socioeconomic level has three big socioeconomic
    characteristics of big data. The paper looks at each level of the proposed framework
    from a service-oriented perspective. The proposed approach in this paper might
    facilitate the research and development of big data, big data analytics, business
    intelligence, and business analytics.'
  author: Sun, Zhaohao and Strang, Kenneth and Li, Rongping
  doi: 10.1145/3291801.3291822
  keywords: artificial intelligence, business analytics, big data, big data analytics,
    business intelligence
  title: Big Data with Ten Big Characteristics
  year: '2018'
- ENTRYTYPE: article
  abstract: Students working in the big data space get uniquely valuable experiences
    and perspectives by taking industrial internships, which can help further their
    research agendas.
  author: Chen, Yanpei and Ferguson, Andrew and Martin, Brian and Wang, Andrew and
    Wendell, Patrick
  doi: 10.1145/2331042.2331054
  keywords: .nan
  title: Big Data and Internships at Cloudera
  year: '2012'
- ENTRYTYPE: article
  abstract: 'The Big Data era is upon us: data is being generated, collected and analyzed
    at an unprecedented scale, and data-driven decision making is sweeping through
    society. Since the value of data explodes when it can be linked and fused with
    other data, addressing the big data integration (BDI) challenge is critical to
    realizing the promise of Big Data.BDI differs from traditional data integration
    in many dimensions: (i) the number of data sources, even for a single domain,
    has grown to be in the tens of thousands, (ii) many of the data sources are very
    dynamic, as a huge amount of newly collected data are continuously made available,
    (iii) the data sources are extremely heterogeneous in their structure, with considerable
    variety even for substantially similar entities, and (iv) the data sources are
    of widely differing qualities, with significant differences in the coverage, accuracy
    and timeliness of data provided. This tutorial explores the progress that has
    been made by the data integration community on the topics of schema mapping, record
    linkage and data fusion in addressing these novel challenges faced by big data
    integration, and identifies a range of open problems for the community.'
  author: Dong, Xin Luna and Srivastava, Divesh
  doi: 10.14778/2536222.2536253
  keywords: .nan
  title: Big Data Integration
  year: '2013'
- ENTRYTYPE: inproceedings
  abstract: 'The traditional world of relational databases and enterprise data warehouses
    is being challenged by growth in data volumes, the rise of unstructured and semi-structured
    data, and the desire to extract more valuable business insights. In order to remain
    competitive: we are entering the world of ''BIG DATA''. Scale-out, commodity hardware-based
    solutions based on the map-reduce programming model for parallel processing on
    large hardware are emerging to address these BIG DATA requirements that have challenged
    traditional technologies. The focus of this talk is on the potential business
    value to be created in this area by describing the opportunities and risks arising
    from the recent emergence of BIG DATA Analytics technology for companies. The
    role businesses can play in BIG DATA is also under discussion, and finally Telefonica''s
    experience is explained in applying BIG DATA technology, both internally for enhancement
    of its own business processes and externally, where we are applying the technology
    to benefit our customers directly.'
  author: Benjamins, V. Richard
  doi: 10.1145/2611040.2611042
  keywords: Business, Analytics, Big Data
  title: 'Big Data: From Hype to Reality?'
  year: '2014'
- ENTRYTYPE: inproceedings
  abstract: The steps of accessing, storing, and transmitting "Big Data" raise many
    interesting problems. But big data sets also amplify any system or software inefficiencies
    when large data sets require processing. So the efficiency of the generate code
    (and the runtime system) is crucial if we want to see widespread use of applications
    based on big data.Adaptive software exploits platform and data properties to custom-tailor
    program executions to the current environment. However, modern platforms have
    many features that make it difficult to support adaptive software. Multi-core
    systems with a non-uniform memory architecture expose various asymmetries and
    complicate the runtime system's task of data management, yet even modest multi-processors
    exhibit NUMA properties. Processor features like prefetchers are difficult to
    model by a compiler and may influence the execution in unexpected ways. Finally,
    performance monitoring units are supposed to allow a (just-in-time) compiler to
    obtain the information needed to adapt the generated code. But current performance
    monitoring units are incomplete and, worse, subject to change over time. An adaptive
    software system needs performance data that is readily available, reliable, and
    stable.In this talk I will discuss our experiences in modeling modern systems
    and argue for portable performance monitoring units that allow higher levels of
    the software tool chain to rely on live performance data.
  author: Gross, Thomas
  doi: 10.1145/2485732.2485757
  keywords: .nan
  title: 'Big Data: Little Software?'
  year: '2013'
- ENTRYTYPE: inproceedings
  abstract: Business analytics, occupying the intersection of the worlds of management
    science, computer science and statistical science, is a potent force for innovation
    in both the private and public sectors. The successes of business analytics in
    strategy, process optimization and competitive advantage has led to data being
    increasingly recognized as a valuable asset in many organizations. In recent years,
    thanks to a dramatic increase in the volume, variety and velocity of data, the
    loosely defined concept of "Big Data" has emerged as a topic of discussion in
    its own right -- with different viewpoints in both the business and technical
    worlds. From our perspective, it is important for discussions of "Big Data" to
    start from a well-defined business goal, and remain moored to fundamental principles
    of both cost/benefit analysis as well as core statistical science. This note discusses
    some business case considerations for analytics projects involving "Big Data",
    and proposes key questions that businesses should ask. With practical lessons
    from Big Data deployments in business, we also pose a number of research challenges
    that may be addressed to enable the business analytics community bring best data
    analytic practices when confronted with massive data sets.
  author: Gopalkrishnan, Vivekanand and Steier, David and Lewis, Harvey and Guszcza,
    James
  doi: 10.1145/2351316.2351318
  keywords: .nan
  title: 'Big Data, Big Business: Bridging the Gap'
  year: '2012'
- ENTRYTYPE: article
  abstract: 'This ACM Ubiquity Symposium presented some of the current thinking about
    big data developments across four topical dimensions: social, technological, application,
    and educational. While 10 articles can hardly touch the expanse of the field,
    we have sought to cover the most important issues and provide useful insights
    for the curious reader. More than two dozen authors from academia and industry
    provided shared their points of view, their current focus of interest and their
    outlines of future research. Big digital data has changed and will change the
    world in many ways. It will bring some big benefits in the future, but combined
    with big AI and big IoT devices creates several big challenges. These must be
    carefully addressed and properly resolved for the future benefit of humanity.'
  author: Johnson, Jeffrey and Denning, Peter and Delic, Kemal A. and Sousa-Rodrigues,
    David
  doi: 10.1145/3158352
  keywords: .nan
  title: 'Big Data: Big Data or Big Brother? That is the Question Now.'
  year: '2018'
- ENTRYTYPE: inproceedings
  abstract: 'The Big Data era is upon us: data is being generated, collected and analyzed
    at an unprecedented scale, and data-driven decision making is sweeping through
    all aspects of society. Since the value of data explodes when it can be linked
    and fused with other data, addressing the big data integration (BDI) challenge
    is critical to realizing the promise of Big Data. BDI differs from traditional
    data integration in many dimensions: (i) the number of data sources, even for
    a single domain, has grown to be in the tens of thousands, (ii) many of the data
    sources are very dynamic, as a huge amount of newly collected data are continuously
    made available, (iii) the data sources are extremely heterogeneous in their structure,
    with considerable variety even for substantially similar entities, and (iv) the
    data sources are of widely differing qualities, with significant differences in
    the coverage, accuracy and timeliness of data provided. This talk explores the
    progress that has been made by the data integration community in addressing these
    novel challenges faced by big data integration, and identifies a range of open
    problems for the community.'
  author: Srivastava, Divesh
  doi: .nan
  keywords: .nan
  title: Big Data Integration
  year: '2013'
- ENTRYTYPE: article
  abstract: .nan
  author: Lukesh, Susan S.
  doi: 10.1145/2614512.2629588
  keywords: .nan
  title: Big Data
  year: '2014'
- ENTRYTYPE: article
  abstract: .nan
  author: CACM Staff
  doi: 10.1145/3079064
  keywords: .nan
  title: Big Data
  year: '2017'
- ENTRYTYPE: inproceedings
  abstract: Digitalization, interconnection, open data, and the use of internet and
    social media by governments and citizens have consequently leaded to an enormous
    growth of data in the public sector. As government data available increases, many
    big data initiatives have been launched by governments in order to derive insights
    and create new value in many areas. This paper details some concepts related to
    government data specifically open data and big data in order to draw the relationships
    between them and show its potential value. The article also brings into light
    the impact of digital progress, made so far in the Moroccan Administration, on
    the growth of data in the public sector, and presents first steps taken by the
    government toward big data era. In particular, we focus on social media, open
    government data and e-government initiatives. In addition, the paper showcases
    examples of applying advanced analytics in the public finances specifically in
    Tax Administration to uncover insights and make better decisions from large datasets.
    This article also overviews some challenges facing future big data initiatives
    in the Moroccan public sector and proposes recommendations to address them.
  author: Khtira, R. and Elasri, B. and Rhanoui, M.
  doi: 10.1145/3090354.3090401
  keywords: Public finances, Open Government data, Moroccan public sector, Machine
    Learning, E-Government, Big data, Advanced Analytics
  title: 'From Data to Big Data: Moroccan Public Sector'
  year: '2017'
- ENTRYTYPE: inproceedings
  abstract: .nan
  author: Mohania, Mukesh
  doi: 10.1145/2837060.2837116
  keywords: .nan
  title: Big Data Processing Flow
  year: '2015'
- ENTRYTYPE: inproceedings
  abstract: WSNs consist of large number of small sensors densely deployed to monitor
    a phenomenon. Most of the data generated from the WSNs represent events happening
    at time intervals. Sometimes and according to the nature of the applications,
    this data stream is continuous and can reach high speeds. Therefore, adopting
    new techniques, platforms and tools to deal with this large amount of sensory
    data became necessary. Therefore, the Big Data paradigm can represent a good solution
    for the extraction, analysis, viewing, sharing, storage and transfer of such volume
    of data. This paper presents a survey on integrating Big Data tools for gathering,
    storing and analyzing the data generated by WSNs.
  author: Boubiche, Sabrina and Boubiche, Djallel Eddine and Azzedine, Bilami
  doi: 10.1145/3010089.3017606
  keywords: MapReduce, Big data, wireless sensor networks, Hadoop
  title: Integrating Big Data Paradigm in WSNs
  year: '2016'
- ENTRYTYPE: article
  abstract: An invitation to the digital science of life.
  author: Chan, Cliburn
  doi: 10.1145/2331042.2331061
  keywords: .nan
  title: Big Data in Computational Biology
  year: '2012'
- ENTRYTYPE: inproceedings
  abstract: The term Big Data has been used and abused extensively in the past few
    years, and means different things to different people. A commonly used notion
    says Big Data is about "volume" (of data), "velocity" (rate at which data is inserted/updated)
    and "variety" (of data types). In this tutorial, we use the term Big Data to refer
    to any data processing need that requires a high degree of parallelism. In other
    words, we focus primarily on the "volume" and "velocity" aspects.As part of this
    tutorial, we will cover some aspects of Big Data management, in particular scalable
    storage, scalable query processing, and scalable transaction processing.This is
    an introductory tutorial for those who are not familiar with the areas that we
    will be covering. The focus will be conceptual; it is not meant as a tutorial
    on how to use any specific system.
  author: Ramachandra, Karthik and Sudarshan, S.
  doi: .nan
  keywords: .nan
  title: 'Big Data: From Querying to Transaction Processing'
  year: '2013'
- ENTRYTYPE: inproceedings
  abstract: Data mining and analytics aims to analyze valuable data and extract implicit,
    previously unknown, and potentially useful information from the data. Due to advances
    in technology, high volumes of valuable data are generated at a high velocity
    in high varieties of data sources in various real-life business, scientific and
    engineering applications. Due to their high volumes, the quality and accuracy
    of these data depend on their veracity (uncertainty of data). This leads us into
    the new era of Big Data. This paper presents some works on big data mining and
    computing, especially on an important task of frequent pattern mining, which computes
    and mines from big data for interesting knowledge in the forms of frequently occurring
    sets of merchandise items in shopping markets, interesting co-located events,
    and/or popular individuals in social networks. The paper also shows how big data
    mining contributes to real-life applications and services.
  author: Leung, Carson K.
  doi: 10.1145/2837060.2837076
  keywords: frequent patterns, Data mining
  title: Big Data Mining Applications and Services
  year: '2015'
- ENTRYTYPE: inproceedings
  abstract: Malicious software, commonly known as malware are constantly getting smarter
    with the capabilities of undergoing self-modifications. They are produced in big
    numbers and widely deployed very fast through the Internet-capable devices. This
    is therefore a big data problem and remains challenging in the research community.
    Existing detection methods should be enhanced in order to effectively deal with
    today's malware. In this paper, we propose a novel real-time monitoring, analysis
    and detection approach that is achieved by applying big data analytics and machine
    learning in the development of a general detection model. The learnings achieved
    through big data render machine learning more efficient. Using the deep learning
    approach, we designed and developed a scalable detection model that brings improvement
    to the existing solutions. Our experiments achieved an accuracy of 97% and ROC
    of 0.99.
  author: Masabo, Emmanuel and Kaawaase, Kyanda Swaib and Sansa-Otim, Julianne
  doi: 10.1145/3195528.3195533
  keywords: deep learning, big data analytics, malware detection, machine learning
  title: 'Big Data: Deep Learning for Detecting Malware'
  year: '2018'
- ENTRYTYPE: inproceedings
  abstract: Big data system development is dramatically different from small (traditional,
    structured) data system development. At the end of 2014, big data deployment is
    still scarce and failures abound. Outsourcing has become a main strategy for many
    enterprises. We therefore selected an outsourcing company who has successfully
    deployed big data projects for our study. Our research results from analyzing
    10 outsourced big data projects provide a glimpse into early adopters of big data,
    illuminates the challenges for system development that stem from the 5Vs of big
    data and crystallizes the importance of architecture design choices and technology
    selection. We followed a collaborative practice research (CPR) method to develop
    and validate a new method, called BDD. BDD is the first attempt to systematically
    combine architecture design with data modeling approaches to address big data
    system development challenges. The use of reference architectures and a technology
    catalog are advancements to architecture design methods and are proving to be
    well-suited for big data system architecture design and system development.
  author: Chen, Hong-Mei and Kazman, Rick and Haziyev, Serge and Hrytsay, Olha
  doi: .nan
  keywords: system engineering, collaborative practice research, data system design
    methods, embedded case study methodology, software architecture, big data
  title: 'Big Data System Development: An Embedded Case Study with a Global Outsourcing
    Firm'
  year: '2015'
- ENTRYTYPE: inproceedings
  abstract: Housing in Florida is mainly driven by population growth of between 300,000
    and 400,000 people per year, which makes Florida the third largest homebuilding
    state in the U.S. This high rate of population growth sheds light on the importance
    of building healthy residential houses that are energy-efficient throughout all
    of Florida, in all its climate zones. The purpose of this study is to investigate
    the direction Florida's single-family homes constructed between the years 2009
    and 2018 are taking with respect to energy efficiency by analyzing their energy
    characteristics through big data analysis. Therefore, this study (a) developed
    a comprehensive literature review of the existing energy-efficient design strategies
    adopted in Florida. In addition, (b) explained the process of collecting information
    about materials and design strategies used in single-family houses constructed
    between the years 2009 and 2018, from energy forms prepared in the framework of
    the permit application process at every local building permit department in different
    counties all over Florida. Finally, (c) analyzed the evolution of information
    collected throughout the years, including but not limited to wall types, ceiling
    types, ducts' location, windows' features, heating and cooling systems, in the
    different climate zones of Florida influencing the building performance. The results
    of this big data analysis indicated that, on the average, single-family homes
    in Florida tended to get more energy-efficient and sustainable throughout the
    last decade (2009-2018). This conclusion cannot be totally confirmed unless all
    economic, environmental, and social aspects of sustainability are also taken into
    consideration.
  author: Elias, Rita and Issa, Raja R. A.
  doi: 10.1145/3363459.3363533
  keywords: Energy, design strategies, single-family houses, Florida, sustainability
  title: 'Big Data: A Decade of Energy Characteristics of Single-Family Homes in Florida'
  year: '2019'
- ENTRYTYPE: inproceedings
  abstract: In an era where Big Data can greatly impact a broad population, many novel
    opportunities arise, chief among them the ability to integrate data from diverse
    sources and "wrangle" it to extract novel insights. Conceived as a tool that can
    help both expert and non-expert users better understand public data, MATTERS was
    collaboratively developed by the Massachusetts High Tech Council, WPI and other
    institutions as an analytic platform offering dynamic modeling capabilities. MATTERS
    is an integrative data source on high fidelity cost and talent competitiveness
    metrics. Its goal is to extract, integrate and model rich economic, financial,
    educational and technological information from renowned heterogeneous web data
    sources ranging from The US Census Bureau, The Bureau of Labor Statistics to the
    Institute of Education Sciences, all known to be critical factors influencing
    economic competitiveness of states. This demonstration of MATTERS illustrates
    how we tackle challenges of data acquisition, cleaning, integration and wrangling
    into appropriate representations, visualization and story-telling with data in
    the context of state competitiveness in the high-tech sector.
  author: Neamtu, Rodica and Ahsan, Ramoza and Stokes, Jeff and Hoxha, Armend and
    Bao, Jialiang and Gvozdenovic, Stefan and Meyer, Ted and Patel, Nilesh and Rangan,
    Raghu and Wang, Yumou and Zhang, Dongyun and Rundensteiner, Elke A.
  doi: 10.1145/2658840.2658845
  keywords: Big data, diverse data sources, data integration
  title: 'Taming Big Data: Integrating Diverse Public Data Sources for Economic Competitiveness
    Analytics'
  year: '2014'
- ENTRYTYPE: article
  abstract: Big Data Systems (BDSs) are an emerging class of scalable software technologies
    whereby massive amounts of heterogeneous data are gathered from multiple sources,
    managed, analyzed (in batch, stream or hybrid fashion), and served to end-users
    and external applications. Such systems pose specific challenges in all phases
    of software development lifecycle and might become very complex by evolving data,
    technologies, and target value over time. Consequently, many organizations and
    enterprises have found it difficult to adopt BDSs. In this article, we provide
    insight into three major activities of software engineering in the context of
    BDSs as well as the choices made to tackle them regarding state-of-the-art research
    and industry efforts. These activities include the engineering of requirements,
    designing and constructing software to meet the specified requirements, and software/data
    quality assurance. We also disclose some open challenges of developing effective
    BDSs, which need attention from both researchers and practitioners.
  author: Davoudian, Ali and Liu, Mengchi
  doi: 10.1145/3408314
  keywords: software reference architecture, quality assurance, requirements engineering,
    Big Data, software engineering, Big Data systems
  title: 'Big Data Systems: A Software Engineering Perspective'
  year: '2020'
- ENTRYTYPE: inproceedings
  abstract: In the last several years, big data analytics has found an increasing
    role in our everyday lives. Data visualization has long been accepted as an integral
    part of data analytics. However, data visualization systems are not equipped to
    handle the complexities typically found in big data. Our work examines effective
    ways of visualizing big data, while also realizing that most visualization processes
    are interactive. During an interactive visualization session, an analyst issues
    several visualization requests, each of which builds on prior visualizations.
    In our approach, we integrate a distributed data processing system that can effectively
    process big data with a visualization system that can provide effective interactive
    visualization but for smaller amounts of data. The analyst's current request is
    used to infer contextual information about the analyst such as their expertise
    and tolerance for delay. This information is used to carefully determine additional
    data that can be sent to the visualization system for decreasing the response
    time for future requests, thus providing a better experience for the analyst and
    increasing their productivity.
  author: Mani, Murali and Fei, Si
  doi: 10.1145/3105831.3105857
  keywords: Data Visualization, Big Data, Data Analytics
  title: Effective Big Data Visualization
  year: '2017'
- ENTRYTYPE: inproceedings
  abstract: Semantic Big Data is about the creation of new applications exploiting
    the richness and flexibility of declarative semantics combined with scalable and
    highly distributed data management systems. In this work, we present an application
    scenario in which a domain ontology, Open Refine and the Okkam Entity Name System
    enable a frictionless and scalable data integration process leading to a knowledge
    base for tax assessment. Further, we introduce the concept of Entiton as a flexible
    and efficient data model suitable for large scale data inference and analytic
    tasks. We successfully tested our data processing pipeline on a real world dataset,
    supporting ACI Informatica in the investigation for Vehicle Excise Duty (VED)
    evasion in Aosta Valley region (Italy). Besides useful business intelligence indicators,
    we implemented a distributed temporal inference engine to unveil VED evasion and
    circulation ban violations. The results of the integration are presented to the
    tax agents in a powerful Siren Solution KiBi dashboard, enabling seamless data
    exploration and business intelligence.
  author: Bortoli, Stefano and Bouquet, Paolo and Pompermaier, Flavio and Molinari,
    Andrea
  doi: 10.1145/2928294.2928297
  keywords: inference, entity name system, tax assessment, semantic big data
  title: Semantic Big Data for Tax Assessment
  year: '2016'
- ENTRYTYPE: inproceedings
  abstract: With an increase in data volume, variety, and velocity, Big Data advances
    tend to focus on technologies such as data gathering, processing, data storage,
    and analytics, all of which assume that technology is the limiting factor in leveraging
    Big Data to its fullest potential. The research framework proposed here takes
    a more holistic look at the Joint Cognitive System, identifying human attention
    as the limiting resource for employing Big Data for operational use. The framework
    leverages prior research in attention management, sensory perception, and joint
    cognitive systems to lay out a Human Centered Big Data Research agenda for designing
    attention direction support in Big Data environments.
  author: Szymczak, Samantha and Zelik, Daniel J. and Elm, Wiliam
  doi: 10.1145/2609876.2609887
  keywords: Attention direction support, human-centered Big Data, cognitive systems
    engineering, joint cognitive system, human attention, mental models
  title: 'Support for Big Data''s Limiting Resource: Human Attention'
  year: '2014'
- ENTRYTYPE: article
  abstract: Data quantity and data quality, like two sides of a coin, are equally
    important to data management. This paper provides an overview of recent advances
    in the study of data quality, from theory to practice. We also address challenges
    introduced by big data to data quality management.
  author: Fan, Wenfei
  doi: 10.1145/2854006.2854008
  keywords: .nan
  title: 'Data Quality: From Theory to Practice'
  year: '2015'
- ENTRYTYPE: inproceedings
  abstract: The importance of Big Data is being realised worldwide with the advancement
    of information technologies, leveraging the capabilities of virtualization and
    cloud computing. Big Data infrastructure and the use of its tools and applications
    will significantly transform the data centers of businesses in the next decade.
    Data analytics is evolving with the new real-time capability of Big Data solutions
    to provide business intelligence for timely and effective decision making. However,
    Big Data poses various challenges related to the infrastructure and resource constraints,
    and other issues including security and privacy. This paper takes an initial step
    in recognizing the value of creating Big Data infrastructure for delivering high
    performance and scalable business intelligence in an organization. It presents
    the state-of-the-art tools and technologies for Big Data infrastructure and NIST
    framework. The advantages of data visualisation are illustrated thorough industry
    case scenarios. The Big Data trends and challenges are also discussed. Overall,
    this paper contributes to providing valuable insights unto the Big Data journey
    of an organization to enable a scalable infrastructure for achieving mission critical
    decision-making through data visualisation.
  author: Venkatraman, Ramanathan and Venkatraman, Sitalakshmi
  doi: 10.1145/3361758.3361768
  keywords: cloud, NoSQL database, data visualisation, Big Data infrastructure, Big
    Data, Hadoop
  title: Big Data Infrastructure, Data Visualisation and Challenges
  year: '2019'
- ENTRYTYPE: inproceedings
  abstract: Big data are a data trend present around us mainly through Internet --
    social networks and smart devices and meters -- mostly without us being aware
    of them. Also they are a fact that both industry and scientific research needs
    to deal with. They are interesting from analytical point of view, for they contain
    knowledge that cannot be ignored and left unused. Traditional system that supports
    the advanced analytics and knowledge extraction -- data warehouse -- is not able
    to cope with large amounts of fast incoming various and unstructured data, and
    may be facing a paradigm shift in terms of utilized concepts, technologies and
    methodologies, which have become a very active research area in the last few years.
    This paper provides an overview of research trends important for the big data
    warehousing, concepts and technologies used for data storage and (ETL) processing,
    and research approaches done in attempts to empower traditional data warehouses
    for handling big data.
  author: Pti\v{c}ek, Marina and Vrdoljak, Boris
  doi: 10.1145/3141128.3141139
  keywords: data warehouse, databases, big data, NewSQL, NoSQL, MapReduce
  title: Big Data and New Data Warehousing Approaches
  year: '2017'
- ENTRYTYPE: inproceedings
  abstract: It is widely accepted today that Relational databases are not appropriate
    in highly distributed shared-nothing architectures of commodity hardware, that
    need to handle poorly structured heterogeneous data. This has brought the blooming
    of NoSQL systems with the purpose of mitigating such problem, specially in the
    presence of analytical workloads. Thus, the change in the data model and the new
    analytical needs beyond OLAP take us to rethink methods and models to design and
    manage these newborn repositories. In this paper, we will analyze state of the
    art and future research directions.
  author: Abell\'{o}, Alberto
  doi: 10.1145/2811222.2811235
  keywords: big data, database design, nosql
  title: Big Data Design
  year: '2015'
- ENTRYTYPE: article
  abstract: 'One significant concern I have for the future of technical communication,
    a concern I often share with my students, involves the impact of "big data." Though
    the term is frequently used with a sneer, or at least a slightly unsettled laugh,
    the methods for retrieving information from large data sets are improving as I
    write this. One significant question the field faces is: "what new relationships
    will develop and what new work will technical communicators be responsible for
    in emergent big data projects, in coming years?"'
  author: Pflugfelder, Ehren Helmut
  doi: 10.1145/2524248.2524253
  keywords: .nan
  title: Big Data, Big Questions
  year: '2013'
- ENTRYTYPE: article
  abstract: Big data sets and analytics are increasingly being used by government
    agencies, non-governmental organizations, and privatecompanies to forward environmental
    protection. Improving energy efficiency, promoting environmental justice, tracking
    climate change, and monitoring water quality are just a few of the objectives
    being furthered by the use of Big Data. The authors provide a more detailed analysis
    of the emerging evidence-based insights on Environmental Big Data (EBD), by applying
    the well-defined method of systematic mapping. The analysis of results throws
    light on the current open issues of Environmental Big Data. Moreover, different
    facets of the study can be combined nto answer more specific research questions.
    The report reveals the need for more empirical research able to provide new metrics
    measuring efficiency and effectiveness of the proposed analytics and new methods
    and tools supporting data processing workflow in EBD
  author: Castelluccia, Daniela and Caldarola, Enrico G. and Boffoli, Nicola
  doi: 10.1145/3011286.3011307
  keywords: Data Management, Data Integration, Systematic Mapping, Environment, Big
    Data
  title: 'Environmental Big Data: A Systematic Mapping Study'
  year: '2017'
- ENTRYTYPE: inproceedings
  abstract: This article articulates the requirements for an effective big data value
    engineering method. It then presents a value discovery method, called Eco-ARCH
    (Eco-ARCHitecture), tightly integrated with the BDD (Big Data Design) method for
    addressing these requirements, filling a methodological void. Eco-ARCH promotes
    a fundamental shift in design thinking for big data system design -- from "bounded
    rationality" for problem solving to "expandable rationality" for design for innovation.
    The Eco-ARCH approach is most suitable for big data value engineering when system
    boundaries are fluid, requirements are ill-defined, many stakeholders are unknown
    and design goals are not provided, where no architecture pre-exists, where system
    behavior is non-deterministic and continuously evolving, and where co-creation
    with consumers and prosumers is essential to achieving innovation goals. The method
    was augmented and empirically validated in collaboration with an IT service company
    in the energy industry to generate a new business model that we call "eBay in
    the Grid".
  author: Chen, Hong-Mei and Kazman, Rick and Garbajosa, Juan and Gonzalez, Eloy
  doi: 10.1145/2896825.2896837
  keywords: architecture landscape, energy industry, innovation, value engineering,
    big data, value discovery, ecosystem
  title: Toward Big Data Value Engineering for Innovation
  year: '2016'
- ENTRYTYPE: inproceedings
  abstract: 'The Gartner''s 2014 Hype Cycle released last August moves Big Data technology
    from the Peak of Inflated Expectations to the beginning of the Trough of Disillusionment
    when interest starts to wane as reality does not live up to previous promises.
    As the hype is starting to dissipate it is worth asking what Big Data (however
    defined) means from a scientific perspective: Did the emergence of gigantic corpora
    exposed the limits of classical information retrieval and data mining and led
    to new concepts and challenges, the way say, the study of electromagnetism showed
    the limits of Newtonian mechanics and led to Relativity Theory, or is it all just
    "sound and fury, signifying nothing", simply a matter of scaling up well understood
    technologies? To answer this question, we have assembled a distinguished panel
    of eminent scientists, from both Industry and Academia: Lada Adamic (Facebook),
    Michael Franklin (University of California at Berkeley), Maarten de Rijke (University
    of Amsterdam), Eric Xing (Carnegie Mellon University), and Kai Yu (Baidu) will
    share their point of view and take questions from the moderator and the audience.'
  author: Broder, Andrei and Adamic, Lada and Franklin, Michael and Rijke, Maarten
    de and Xing, Eric and Yu, Kai
  doi: 10.1145/2684822.2697027
  keywords: big data
  title: 'Big Data: New Paradigm or "Sound and Fury, Signifying Nothing"?'
  year: '2015'
