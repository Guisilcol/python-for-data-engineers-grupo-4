[{"author":"Jorge Merino and Ismael Caballero and Bibiano Rivas and Manuel Serrano and Mario Piattini","title":"A Data Quality in Use model for Big Data","keywords":"Data Quality, Big Data, Measurement, Quality-in-Use, Model","abstract":"Beyond the hype of Big Data, something within business intelligence projects is indeed changing. This is mainly because Big Data is not only about data, but also about a complete conceptual and technological stack including raw and processed data, storage, ways of managing data, processing and analytics. A challenge that becomes even trickier is the management of the quality of the data in Big Data environments. More than ever before the need for assessing the Quality-in-Use gains importance since the real contribution\u2013business value\u2013of data can be only estimated in its context of use. Although there exists different Data Quality models for assessing the quality of regular data, none of them has been adapted to Big Data. To fill this gap, we propose the \u201c3As Data Quality-in-Use model\u201d, which is composed of three Data Quality characteristics for assessing the levels of Data Quality-in-Use in Big Data projects: Contextual Adequacy, Operational Adequacy and Temporal Adequacy. The model can be integrated into any sort of Big Data project, as it is independent of any pre-conditions or technologies. The paper shows the way to use the model with a working example. The model accomplishes every challenge related to Data Quality program aimed for Big Data. The main conclusion is that the model can be used as an appropriate way to obtain the Quality-in-Use levels of the input data of the Big Data analysis, and those levels can be understood as indicators of trustworthiness and soundness of the results of the Big Data analysis.","year":2016,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.future.2015.11.024","jcs_value":null,"scimago_value":null},{"author":"Simon Vydra and Bram Klievink","title":"Techno-optimism and policy-pessimism in the public sector big data debate","keywords":"Big data, Analytics, Government, Public administration, Policy-making, Decision-making, Science-policy interface, Network governance","abstract":"Despite great potential, high hopes and big promises, the actual impact of big data on the public sector is not always as transformative as the literature would suggest. In this paper, we ascribe this predicament to an overly strong emphasis the current literature places on technical-rational factors at the expense of political decision-making factors. We express these two different emphases as two archetypical narratives and use those to illustrate that some political decision-making factors should be taken seriously by critiquing some of the core \u2018techno-optimist\u2019 tenets from a more \u2018policy-pessimist\u2019 angle. In the conclusion we have these two narratives meet \u2018eye-to-eye\u2019, facilitating a more systematized interrogation of big data promises and shortcomings in further research, paying appropriate attention to both technical-rational and political decision-making factors. We finish by offering a realist rejoinder of these two narratives, allowing for more context-specific scrutiny and balancing both technical-rational and political decision-making concerns, resulting in more realistic expectations about using big data for policymaking in practice.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.giq.2019.05.010","jcs_value":null,"scimago_value":null},{"author":"Yuri A.W. Shardt and Xu Yang and Kevin Brooks and Andrei Torgashov","title":"Data Quality Assessment for System Identification in the Age of Big Data and Industry 4.0","keywords":"data quality assessment, system identification, big data, Industry 4.0, soft sensors","abstract":"As the amount of data stored from industrial processes increases with the demands of Industry 4.0, there is an increasing interest in finding uses for the stored data. However, before the data can be used its quality must be determined and appropriate regions extracted. Initially, such testing was done manually using graphs or basic rules, such as the value of a variable. With large data sets, such an approach will not work, since the amount of data to tested and the number of potential rules is too large. Therefore, there is a need for automated segmentation of the data set into different components. Such an approach has recently been proposed and tested using various types of industrial data. Although the industrial results are promising, there still remain many unanswered questions including how to handle a priori knowledge, over- or undersegmentation of the data set, and setting the appropriate thresholds for a given application. Solving these problems will provide a robust and reliable method for determining the data quality of a given data set.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.ifacol.2020.12.103","jcs_value":null,"scimago_value":null},{"author":"Jia Liu and Tianrui Li and Peng Xie and Shengdong Du and Fei Teng and Xin Yang","title":"Urban big data fusion based on deep learning: An overview","keywords":"Urban computing, Big data, Data fusion, Deep learning","abstract":"Urban big data fusion creates huge values for urban computing in solving urban problems. In recent years, various models and algorithms based on deep learning have been proposed to unlock the power of knowledge from urban big data. To clarify the methodologies of urban big data fusion based on deep learning (DL), this paper classifies them into three categories: DL-output-based fusion, DL-input-based fusion and DL-double-stage-based fusion. These methods use deep learning to learn feature representation from multi-source big data. Then each category of fusion methods is introduced and some examples are shown. The difficulties and ideas of dealing with urban big data will also be discussed.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.inffus.2019.06.016","jcs_value":null,"scimago_value":null},{"author":"Gianluca Elia and Gloria Polimeno and Gianluca Solazzo and Giuseppina Passiante","title":"A multi-dimension framework for value creation through Big Data","keywords":"Big Data analytics, Cognitive computing, Framework, Model, Systematic literature review, Value creation","abstract":"Big Data represents a promising area for value creation and frontier research. The potential to extract actionable insights from Big Data has gained increasing attention of both academics and practitioners operating in several industries. Marketing domain has become from the start a field for experiments with Big Data approaches, even if the adoption of Big Data solutions does not always generate effective value for the adopters. Therefore, the gap existing between the potential of value creation embedded in the Big Data paradigm and the current limited exploitation of this value represents an area of investigation that this paper aims to explore. In particular, by following a systematic literature review, this study aims at presenting a framework that outlines the multiple value directions that the Big Data paradigm can generate for the adopting organizations. Eleven distinct value directions have been identified and then grouped in five dimensions (Informational, Transactional, Transformational, Strategic, Infrastructural Value), which constitute the pillars of the proposed framework. Finally, the framework has been also preliminarily applied in three case studies conducted within three Italian based companies operating in different industries (e-commerce, fast-moving consumer goods, and banking) in the final aim to see its applicability in real business scenarios.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.indmarman.2020.03.015","jcs_value":null,"scimago_value":null},{"author":"Ajay Major and Suzanne M. Cox and Samuel L. Volchenboum","title":"Using big data in pediatric oncology: Current applications and future directions","keywords":"Pediatric oncology, Pediatric cancer, Big data, Data sharing, Data science, Informatics","abstract":"Pediatric cancer is a rare disease with a low annual incidence, which presents a significant challenge in being able to collect enough data to fuel clinical discoveries. Big data registry trials hold promise to advance the study of pediatric cancers by allowing for the combination of traditional randomized controlled trials with the power of larger cohort sizes. The emergence of big data resources and data-sharing initiatives are becoming transformative for pediatric cancer diagnosis and treatment. This review discusses the uses of big data in pediatric cancer, existing pediatric cancer registry initiatives and research, the challenges in harmonizing these data to improve accessibility for study, and building pediatric data commons and other important future endeavors.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1053\/j.seminoncol.2020.02.006","jcs_value":null,"scimago_value":null},{"author":"Maryam Ghasemaghaei and Goran Calic","title":"Can big data improve firm decision quality? The role of data quality and data diagnosticity","keywords":"Big data utilization, Data quality, Decision quality, Data diagnosticity","abstract":"Anecdotal evidence suggests that, despite the large variety of data, the huge volume of generated data, and the fast velocity of obtaining data (i.e., big data), quality of big data is far from perfect. Therefore, many firms defer collecting and integrating big data as they have concerns regarding the impact of utilizing big data on data diagnosticity (i.e., retrieval of valuable information from data) and firm decision making quality. In this study, we use the Organizational Learning Theory and Wang and Strong's data quality framework to explore the impact of processing big data on firm decision quality and the mediating role of data quality (DQ) and data diagnosticity on this relationship. We validate the proposed research model using survey data from 130 firms, obtained from data analysts and IT managers. Results confirm the critical role of DQ in increasing data diagnosticity and improving firm decision quality when processing big data; suggesting important implications for practice and theory. Findings also reveal that while big data utilization positively impacts contextual DQ, accessibility DQ, and representational DQ, interestingly, it negatively impacts intrinsic DQ. Furthermore, findings show that while intrinsic DQ, contextual DQ, and representational DQ significantly increase data diagnosticity, accessibility DQ does not influence it. Most importantly, the findings show that big data utilization does not significantly impact the quality of firm decisions and it is fully mediated through DQ and data diagnosticity. The results of this study contribute to practice by providing important guidelines for managers to improve firm decision quality through the use of big data.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.dss.2019.03.008","jcs_value":null,"scimago_value":null},{"author":"Giorgia Carra and Jorge I.F. Salluh and Fernando Jos\u00e9 {da Silva Ramos} and Geert Meyfroidt","title":"Data-driven ICU management: Using Big Data and algorithms to improve outcomes","keywords":"Big data, Data mining, Machine learning, Predictive modeling, Intensive care unit","abstract":"The digitalization of the Intensive Care Unit (ICU) led to an increasing amount of clinical data being collected at the bedside. The term \u201cBig Data\u201d can be used to refer to the analysis of these datasets that collect enormous amount of data of different origin and format. Complexity and variety define the value of Big Data. In fact, the retrospective analysis of these datasets allows to generate new knowledge, with consequent potential improvements in the clinical practice. Despite the promising start of Big Data analysis in medical research, which has seen a rising number of peer-reviewed articles, very limited applications have been used in ICU clinical practice. A close future effort should be done to validate the knowledge extracted from clinical Big Data and implement it in the clinic. In this article, we provide an introduction to Big Data in the ICU, from data collection and data analysis, to the main successful examples of prognostic, predictive and classification models based on ICU data. In addition, we focus on the main challenges that these models face to reach the bedside and effectively improve ICU care.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.jcrc.2020.09.002","jcs_value":null,"scimago_value":null},{"author":"Hicham Moad Safhi and Bouchra Frikh and Brahim Ouhbi","title":"Assessing reliability of Big Data Knowledge Discovery process","keywords":"Knowledge discovery, Reliability, Trustworthiness, Quality, Big Data mining","abstract":"Extracting knowledge from Big Data is the process of transforming this data into actionable information. The exponential growth of data has initiated a myriad of new opportunities, and made data become the most valuable raw material of production for many organizations. Mining Big Data is coupled with some challenges, known as the 3V\u2019s of Big Data: Volume, Variety and Velocity. However, a major challenge that needs to be addressed, and often is ignored in the literature, concerns reliability. Actually, data is agglomerated from multiple disparate sources, and each of Knowledge Discovery (KDD) process steps may be carried out by different organizations. These considerations lead us to ask a critical question that is weather the information we have at each step is reliable enough to proceed to the next one? This paper therefore aims to provide a framework that automatically assesses reliability of the knowledge discovery process. We focus on Linked Open Data (LOD) as a source of data, as it constitutes a relevant data provider in many Big Data applications. However, our framework can also be adapted for unstructured data. This framework will assist scientists to automatically and efficiently measure the reliability of each KDD process stage as well as detect unreliable steps that should be revised. Following this methodology, KDD process will be optimized and therefore produce knowledge with higher quality.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.procs.2019.01.005","jcs_value":null,"scimago_value":null},{"author":"M. TALHA and A. ABOU {EL KALAM} and N. ELMARZOUQI","title":"Big Data: Trade-off between Data Quality and Data Security","keywords":"Big Data, Data Quality, Data Security, Trade-off between Quality, Security","abstract":"The essence of an information system lies in the data; if it is not of good quality or not sufficiently protected, the consequences will undoubtedly be harmful. Quality and Security are two essential aspects that add value to data and their implementation has become a real need and must be adopted before any data exploitation. Due to the high volume of data, their diversity and their rapid generation, effective implementation of such systems requires well thought out mechanisms and strategies. This paper provides an overview of Data Quality and Data Security in a Big Data context. We want through this paper to highlight the conflicts that may exist during the implementation of data security and data quality management systems. Such a conflict makes the complexity greater and requires new adapted solutions.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.procs.2019.04.127","jcs_value":null,"scimago_value":null},{"author":"\u00c1lvaro Valencia-Parra and \u00c1ngel Jes\u00fas Varela-Vaca and Luisa Parody and Mar\u00eda Teresa G\u00f3mez-L\u00f3pez","title":"Unleashing Constraint Optimisation Problem solving in Big Data environments","keywords":"Big Data, Optimisation problem, Constraint programming, Distributed data, Heterogeneous data format","abstract":"The application of the optimisation problems in the daily decisions of companies is able to be used for finding the best management according to the necessities of the organisations. However, optimisation problems imply a high computational complexity, increased by the current necessity to include a massive quantity of data (Big Data), for the creation of optimisation problems to customise products and services for their clients. The irruption of Big Data technologies can be a challenge but also an important mechanism to tackle the computational difficulties of optimisation problems, and the possibility to distribute the problem performance. In this paper, we propose a solution that lets the query of a data set supported by Big Data technologies that imply the resolution of Constraint Optimisation Problem (COP). This proposal enables to: (1) model COPs whose input data are obtained from distributed and heterogeneous data; (2) facilitate the integration of different data sources to create the COPs; and, (3) solve the optimisation problems in a distributed way, to improve the performance. It is done by means of a framework and supported by a tool capable of modelling, solving and querying the results of optimisation problems. The tool integrates the Big Data technologies and commercial solvers of constraint programming. The suitability of the proposal and the development have been evaluated with real data sets whose computational study and results are included and discussed.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.jocs.2020.101180","jcs_value":null,"scimago_value":null},{"author":"Wang Kun and Liu Tong and Xie Xiaodan","title":"Application of Big Data Technology in Scientific Research Data Management of Military Enterprises","keywords":"big data technology, scientific research data, data analysis, decision","abstract":"Scientific research data has an important strategic position for the development of enterprises and countries, and is an important basis for management to conduct strategic research and decision-making. Compared with the Internet industry, big data technology started late in the military enterprises, while military enterprises research data often has the characteristics of decentralization, low relevance, and diverse data types. It cannot fully utilize the advantages of data resources to enhance the core competitiveness of enterprises. To this end, this paper deeply explores the application methods of big data technology in military scientific research data management, and lays a foundation for the construction of scientific research big data platform.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.procs.2019.01.221","jcs_value":null,"scimago_value":null},{"author":"Liang Hong and Mengqi Luo and Ruixue Wang and Peixin Lu and Wei Lu and Long Lu","title":"Big Data in Health Care: Applications and Challenges","keywords":"Big Data, public health, cloud computing, medical applications","abstract":"The concept of Big Data is popular in a variety of domains. The purpose of this review was to summarize the features, applications, analysis approaches, and challenges of Big Data in health care. Big Data in health care has its own features, such as heterogeneity, incompleteness, timeliness and longevity, privacy, and ownership. These features bring a series of challenges for data storage, mining, and sharing to promote health-related research. To deal with these challenges, analysis approaches focusing on Big Data in health care need to be developed and laws and regulations for making use of Big Data in health care need to be enacted. From a patient perspective, application of Big Data analysis could bring about improved treatment and lower costs. In addition to patients, government, hospitals, and research institutions could also benefit from the Big Data in health care.","year":2018,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.2478\/dim-2018-0014","jcs_value":null,"scimago_value":null},{"author":"Jos\u00e9 Carpio-Pinedo and Javier Guti\u00e9rrez","title":"Consumption and symbolic capital in the metropolitan space: Integrating \u2018old\u2019 retail data sources with social big data","keywords":"Commercial space, Retail, Retail geography, Symbolic capital, Big data, Foursquare, Madrid","abstract":"While commerce is one of the key activities in cities, its spatial description still requires further attention, especially by considering the different dimensions of commercial space: physical, economic and socio-symbolic. The latter is becoming more and more important in an era where consumption is at the centre of social relations. Further, although data availability has been an enduring obstacle in commercial research, we are witnessing the advent of new data sources, and social-network big data is an opportunity to unveil the places to which consumers attribute prestige or symbolic capital, at the extent of entire metropolitan areas. This paper compares the physical, economic and socio-symbolic dimensions of commercial spaces through the analysis of three different commercial data sources: cadastral micro-data, business register and social-network big data. For the case of Madrid Metropolitan Area, the three databases are compared with correlation analysis and density maps, coming out as partly redundant and partly complementary. Getis-Ord's hotspot statistics integrated into a cluster analysis enable a comprehensive understanding of commercial environments, enriching previous spatial hierarchies. The spatial distribution of symbolic capital unveils a relation with socio-spatial segregation and paves the way to new reflections on the spatiality of consumption as a social practice.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.cities.2020.102859","jcs_value":null,"scimago_value":null},{"author":"Marijn Janssen and Haiko {van der Voort} and Agung Wahyudi","title":"Factors influencing big data decision-making quality","keywords":"Big data, Big data analytics, Big data chain, E-government, Governance, Decision-making, Decision-making quality","abstract":"Organizations are looking for ways to harness the power of big data (BD) to improve their decision making. Despite its significance the effects of BD on decision-making quality has been given scant attention in the literature. In this paper factors influencing decision-making based on BD are identified using a case study. BD is collected from different sources that have various data qualities and are processed by various organizational entities resulting in the creation of a big data chain. The veracity (manipulation, noise), variety (heterogeneity of data) and velocity (constantly changing data sources) amplified by the size of big data calls for relational and contractual governance mechanisms to ensure BD quality and being able to contextualize data. The case study reveals that taking advantage of big data is an evolutionary process in which the gradually understanding of the potential of big data and the routinization of processes plays a crucial role.","year":2017,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.jbusres.2016.08.007","jcs_value":null,"scimago_value":null},{"author":"Ines Sbai and Saoussen Krichen","title":"A real-time Decision Support System for Big Data Analytic: A case of Dynamic Vehicle Routing Problems","keywords":"Big data analytic, Decision Support System, DVRP, S-GA, Spark","abstract":"Recently, the explosion of large amounts of traffic data has guided data scientists to create models with big data for a better decision-making. Big Data applications process and analyze this huge amounts of data (collected from a variety of heterogeneous data sources) that cannot be processed with traditional technologies. In this paper, Big Data frameworks are used for solving an optimization problem known as Dynamic Vehicle Routing Problem (DVRP). Hence, due to the NP-Hardness of the problem and to deal with a large size of data, we develop a parallel Spark Genetic Algorithm named (S-GA). This parallelism aims to take the advantage of Spark\u2019s in-memory computing ability (as a master-slave distribution computing) and GA\u2019s iterations operations. Parallel operations were used for fitness evaluation and genetic operations. Based on the parallel S-GA a decision support system is developed for the DVRP in order to generate the best routes. The experiments show that our proposed architecture is improved due to its capacity when coping with Big Data optimization problems by interconnecting components and deploying on different nodes of a cluster.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.procs.2020.09.089","jcs_value":null,"scimago_value":null},{"author":"Alexander Binder and Eva-Maria Iwer and Werner Quint","title":"Big Data Management Using Ontologies for CPQ Solutions","keywords":"CPQ, Semantic Technologies, Ontologies, Ontology Matching, Data Quality","abstract":"In recent years, due to a progressive complexity of handling and processing business data, proper big data management has become a challenge, especially for SMEs that have limited resources for investing in the requested business transformation process. As a solution, we suggest an ontology-based CPQ software approach, where we show how the implementation of semantic technologies and ontologies affects data integration processes. We also propose a method called \"ontology-based data matching\", which allows the semiautomatic generation of alignments used to formalize the coherence between ontologies. The proposed method will ensure consistency during integration, significantly improving the productivity of enterprises.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.promfg.2020.11.051","jcs_value":null,"scimago_value":null},{"author":"Carla Wilkin and Ald\u00f3nio Ferreira and Kristian Rotaru and Luigi Red Gaerlan","title":"Big data prioritization in SCM decision-making: Its role and performance implications","keywords":"Big data, Big data availability, Big data prioritization, Supply chain management, Performance","abstract":"Given exponential growth in the size of big data, its multi-channel sources and variability in quality that create challenges concerning cost-effective use, firms have invested significantly in databases and analytical tools to inform decision-making. In this regard, one means to avoid the costs associated with producing less than insightful reports and negative effects on performance through wasted resources is prioritizing data in terms of relevance and quality. The aim of this study is to investigate this approach by developing and testing a scale to evaluate Big Data Availability and the role of Big Data Prioritization for more effective use of big data in decision-making and performance. Focusing on the context of supply chain management (SCM), we validate this scale through a survey involving 84 managers. Findings support a positive association between Big Data Availability and its use in SCM decision-making, and suggest that Big Data Prioritization, as conceptualized in the study, has a positive impact on the use of big data in SCM decision-making and SCM performance. Through developing a scale to evaluate association between Big Data Availability and use in SCM decision-making, we make an empirical contribution to value generation from big data.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.accinf.2020.100470","jcs_value":null,"scimago_value":null},{"author":"Mohamed Saifeddine {Hadj Sassi} and Faiza Ghozzi Jedidi and Lamia Chaari Fourati","title":"A New Architecture for Cognitive Internet of Things and Big Data","keywords":"Internet of Things, Big-Data, Architecture, Cognitive, Data-flow","abstract":"Big data and the Internet of Things (IoT) are considered as the main paradigms when defining new information architecture projects. Accordingly, technologies that make up these solutions could have an important role to play in business information architecture. Solutions that have approached big data and the IoT as unique technology initiatives, struggle in finding value in such efforts and in the technology itself. A connection to the requirements (volume, velocity, and variety) is mandatory to reach the potential business goals. In this context, we propose a new architecture for Cognitive Internet of Things (CIoT) and big data. The proposed architecture benefits computing mechanisms by combining the data WareHouse (DWH) and Data Lake (DL), and defining a tool for heterogeneous data collection.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.procs.2019.09.208","jcs_value":null,"scimago_value":null},{"author":"Shan Ren and Yingfeng Zhang and Yang Liu and Tomohiko Sakao and Donald Huisingh and Cecilia M.V.B. Almeida","title":"A comprehensive review of big data analytics throughout product lifecycle to support sustainable smart manufacturing: A framework, challenges and future research directions","keywords":"Big data analytics, Smart manufacturing, Servitization, Sustainable production, Conceptual framework, Product lifecycle","abstract":"Smart manufacturing has received increased attention from academia and industry in recent years, as it provides competitive advantage for manufacturing companies making industry more efficient and sustainable. As one of the most important technologies for smart manufacturing, big data analytics can uncover hidden knowledge and other useful information like relations between lifecycle decisions and process parameters helping industrial leaders to make more-informed business decisions in complex management environments. However, according to the literature, big data analytics and smart manufacturing were individually researched in academia and industry. To provide theoretical foundations for the research community to further develop scientific insights in applying big data analytics to smart manufacturing, it is necessary to summarize the existing research progress and weakness. In this paper, through combining the key technologies of smart manufacturing and the idea of ubiquitous servitization in the whole lifecycle, the term of sustainable smart manufacturing was coined. A comprehensive overview of big data in smart manufacturing was conducted, and a conceptual framework was proposed from the perspective of product lifecycle. The proposed framework allows analyzing potential applications and key advantages, and the discussion of current challenges and future research directions provides valuable insights for academia and industry.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.jclepro.2018.11.025","jcs_value":null,"scimago_value":null},{"author":"Hourieh Khalajzadeh and Andrew J. Simmons and Mohamed Abdelrazek and John Grundy and John Hosking and Qiang He","title":"An end-to-end model-based approach to support big data analytics development","keywords":"Big data analytics, Big data modeling, Big data toolkits, Domain-specific visual languages, Multidisciplinary teams, End-user tools","abstract":"We present BiDaML 2.0, an integrated suite of visual languages and supporting tool to help multidisciplinary teams with the design of big data analytics solutions. BiDaML tool support provides a platform for efficiently producing BiDaML diagrams and facilitating their design, creation, report and code generation. We evaluated BiDaML using two types of evaluations, a theoretical analysis using the \u201cphysics of notations\u201d, and an empirical study with 1) a group of 12 target end-users and 2) five individual end-users.\u00a0Participants mostly agreed that BiDaML was straightforward to understand\/learn, and prefer BiDaML for supporting complex data analytics solution modeling than other modeling languages.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.cola.2020.100964","jcs_value":null,"scimago_value":null},{"author":"Danilo Ardagna and Cinzia Cappiello and Walter Sam\u00e1 and Monica Vitali","title":"Context-aware data quality assessment for big data","keywords":"Data quality, Big data, Context-awareness, Data profiling, DQ assessment","abstract":"Big data changed the way in which we collect and analyze data. In particular, the amount of available information is constantly growing and organizations rely more and more on data analysis in order to achieve their competitive advantage. However, such amount of data can create a real value only if combined with quality: good decisions and actions are the results of correct, reliable and complete data. In such a scenario, methods and techniques for the Data Quality assessment can support the identification of suitable data to process. If for traditional database numerous assessment methods are proposed, in the Big Data scenario new algorithms have to be designed in order to deal with novel requirements related to variety, volume and velocity issues. In particular, in this paper we highlight that dealing with heterogeneous sources requires an adaptive approach able to trigger the suitable quality assessment methods on the basis of the data type and context in which data have to be used. Furthermore, we show that in some situations it is not possible to evaluate the quality of the entire dataset due to performance and time constraints. For this reason, we suggest to focus the Data Quality assessment only on a portion of the dataset and to take into account the consequent loss of accuracy by introducing a confidence factor as a measure of the reliability of the quality assessment procedure. We propose a methodology to build a Data Quality adapter module, which selects the best configuration for the Data Quality assessment based on the user main requirements: time minimization, confidence maximization, and budget minimization. Experiments are performed by considering real data gathered from a smart city case study.","year":2018,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.future.2018.07.014","jcs_value":null,"scimago_value":null},{"author":"Subhajit Pal and Sudip Mondal and Gourab Das and Sunirmal Khatua and Zhumur Ghosh","title":"Big data in biology: The hope and present-day challenges in it","keywords":"Big data, Cloud computing, Bioinformatics, High throughput data, MapReduce, Machine learning","abstract":"The wave of new technologies has opened up the opportunity for cost-effective generation of high-throughput profiles of biological systems. This is generating tons of biological data. It is thus leading us towards the \u201cbig data\u201d era which is creating a pressing need to bridge the gap between high-throughput technological development and our ability for managing, analyzing, and integrating the biological big data. To harness the maximum out of it, sufficient expertise needs to be developed for big data management and analysis. In this review, we discuss the challenges related to storage, transfer, access and analysis of unstructured and structured biological big data. Subsequently, it provides a comprehensive summary regarding the important strategies adopted for biological big data management which includes a discussion on all the recently used tools or software built for high throughput processing and analysis of biological big data. Finally it discusses the future perspectives of big data bioinformatics.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.genrep.2020.100869","jcs_value":null,"scimago_value":null},{"author":"Yiming Lin and Hongzhi Wang and Jianzhong Li and Hong Gao","title":"Data source selection for information integration in big data era","keywords":"Source selection, Data integration, Data cleaning","abstract":"In big data era, information integration often requires abundant data extracted from massive data sources. Due to a large number of data sources, data source selection plays a crucial role in information integration, since it is costly and even impossible to access all data sources. Data Source selection should consider both efficiency and effectiveness issues. For efficiency, the approach should scale to large data source amount. From effectiveness aspect, data quality and overlapping of sources are to be considered. In this paper, we study source selection problem in Big Data and propose methods which can scale to datasets with up to millions of data sources and guarantee the quality of results. Motivated by this, we propose a new metric taking the expected number of true values a source can provide as a criteria to evaluate the contribution of a data source. Based on our proposed index, we present a scalable algorithm and two pruning strategies to improve the efficiency without sacrificing precision. Experimental results on both real world and synthetic data sets show that our methods can select sources providing a large proportion of true values efficiently and can scale to massive data sources.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.ins.2018.11.029","jcs_value":null,"scimago_value":null},{"author":"Imane El Alaoui and Youssef Gahi","title":"The Impact of Big Data Quality on Sentiment Analysis Approaches","keywords":"Big Data Quality Metrics, Big Data Value Chain, Big Data, Big Social Data, Sentiment Analysis, Opinion Mining","abstract":"Human beings share their good or bad opinions about subjects, products, and services through internet and social networks. The ability to effectively analyze this kind of information is now seen as a key competitive advantage to better inform decisions. In order to do so, organizations employ Sentiment Analysis (SA) techniques on these data. However, the usage of social media around the world is ever-increasing, which considerably accelerates massive data generation and makes traditional SA systems unable to deliver useful insights. Such volume of data can be efficiently analyzed using the combination of SA techniques and Big Data technologies. In fact, big data is not a luxury but an essential necessary to make valuable predictions. However, there are some challenges associated with big data such as quality that could highly affect the SA systems\u2019 accuracy that use huge volume of data. Thus, the quality aspect should be addressed in order to build reliable and credible systems. For this, the goal of our research work is to consider Big Data Quality Metrics (BDQM) in SA that rely of big data. In this paper, we first highlight the most eloquent BDQM that should be considered throughout the Big Data Value Chain (BDVC) in any big data project. Then, we measure the impact of BDQM on a novel SA method accuracy in a real case study by giving simulation results.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.procs.2019.11.007","jcs_value":null,"scimago_value":null},{"author":"Madhu Mahesh Nashipudimath and Subhash K. Shinde and Jayshree Jain","title":"An efficient integration and indexing method based on feature patterns and semantic analysis for big data","keywords":"Big data, Integration, Feature patterns, Indexing, Semantic analysis","abstract":"Big Data has received much attention in the multi-domain industry. In the digital and computing world, information is generated and collected at a rate that quickly exceeds the boundaries. The traditional data integration system interconnects the limited number of resources and is built with relatively stable and generally complex and time-consuming design activities. However, the rapid growth of these large data sets creates difficulties in learning heterogeneous data structures for integration and indexing. It also creates difficulty in information retrieval for the various data analysis requirements. In this paper, a probabilistic feature Patterns (PFP) approach using feature transformation and selection method is proposed for efficient data integration and utilizing the features latent semantic analysis (F-LSA) method for indexing the unsupervised multiple heterogeneous integrated cluster data sources. The PFP approach takes the advantage of the features transformation and selection mechanism to map and cluster the data for the integration, and an analysis of the data features context relation using LSA to provide the appropriate index for fast and accurate data extraction. A huge volume of BibText dataset from different publication sources are processed to evaluated to understand the effectiveness of the proposal. The analytical study and the outcome results show the improvisation in integration and indexing of the work.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.array.2020.100033","jcs_value":null,"scimago_value":null},{"author":"Linlin Zhao and Heather L. Ciallella and Lauren M. Aleksunes and Hao Zhu","title":"Advancing computer-aided drug discovery (CADD) by big data and data-driven machine learning modeling","keywords":null,"abstract":"Advancing a new drug to market requires substantial investments in time as well as financial resources. Crucial bioactivities for drug candidates, including their efficacy, pharmacokinetics (PK), and adverse effects, need to be investigated during drug development. With advancements in chemical synthesis and biological screening technologies over the past decade, a large amount of biological data points for millions of small molecules have been generated and are stored in various databases. These accumulated data, combined with new machine learning (ML) approaches, such as deep learning, have shown great potential to provide insights into relevant chemical structures to predict in vitro, in vivo, and clinical outcomes, thereby advancing drug discovery and development in the big data era.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.drudis.2020.07.005","jcs_value":null,"scimago_value":null},{"author":"H.G. {van der Voort} and A.J. Klievink and M. Arnaboldi and A.J. Meijer","title":"Rationality and politics of algorithms. Will the promise of big data survive the dynamics of public decision making?","keywords":null,"abstract":"Big data promises to transform public decision-making for the better by making it more responsive to actual needs and policy effects. However, much recent work on big data in public decision-making assumes a rational view of decision-making, which has been much criticized in the public administration debate. In this paper, we apply this view, and a more political one, to the context of big data and offer a qualitative study. We question the impact of big data on decision-making, realizing that big data \u2013 including its new methods and functions \u2013 must inevitably encounter existing political and managerial institutions. By studying two illustrative cases of big data use processes, we explore how these two worlds meet. Specifically, we look at the interaction between data analysts and decision makers. In this we distinguish between a rational view and a political view, and between an information logic and a decision logic. We find that big data provides ample opportunities for both analysts and decision makers to do a better job, but this doesn't necessarily imply better decision-making, because big data also provides opportunities for actors to pursue their own interests. Big data enables both data analysts and decision makers to act as autonomous agents rather than as links in a functional chain. Therefore, big data's impact cannot be interpreted only in terms of its functional promise; it must also be acknowledged as a phenomenon set to impact our policymaking institutions, including their legitimacy.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.giq.2018.10.011","jcs_value":null,"scimago_value":null},{"author":"Patrick Mikalef and John Krogstie and Ilias O. Pappas and Paul Pavlou","title":"Exploring the relationship between big data analytics capability and competitive performance: The mediating roles of dynamic and operational capabilities","keywords":"Big data analytics, Dynamic capabilities, Operational capabilities, Business value, Resource-based view","abstract":"A central question for information systems (IS) researchers and practitioners is if, and how, big data can help attain a competitive advantage. To address this question, this study draws on the resource-based view, dynamic capabilities view, and on recent literature on big data analytics, and examines the indirect relationship between a firm\u2019s big data analytics capability (BDAC) and competitive performance. The study extends existing research by proposing that BDACs enable firms to generate insight that can help strengthen their dynamic capabilities, which, in turn, positively impact marketing and technological capabilities. To test our proposed research model, we used survey data from 202 chief information officers and IT managers working in Norwegian firms. By means of partial least squares structural equation modeling, results show that a strong BDAC can help firms build a competitive advantage. This effect is not direct but fully mediated by dynamic capabilities, which exerts a positive and significant effect on two types of operational capabilities: marketing and technological capabilities. The findings suggest that IS researchers should look beyond direct effects of big data investments and shift their attention on how a BDAC can be leveraged to enable and support organizational capabilities.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.im.2019.05.004","jcs_value":null,"scimago_value":null},{"author":"Hanen Balti and Ali {Ben Abbes} and Nedra Mellouli and Imed Riadh Farah and Yanfang Sang and Myriam Lamolle","title":"A review of drought monitoring with big data: Issues, methods, challenges and research directions","keywords":"Drought monitoring, Artificial intelligence, Big data, Machine learning, Statistical approach, Remote sensing","abstract":"Over recent years, the frequency and intensity of droughts have increased and there has been a large drying trend over many parts of the world. Consequently, drought monitoring using big data analytic has gained an explosive interest. Droughts stand among the most damaging natural disasters. It threatens agricultural production, ecological environment, and socio-economic development. For this reason, early warning, accurate evaluation, and efficient prediction are an emergency especially for the nations that are the most menaced by this danger. There are numerous emerging studies addressing big data and its applications in drought monitoring. In fact, big data handle data heterogeneity which is an additive value for the prediction of drought, it offers a view of the different dimensions such as the spatial distribution, the temporal distribution and the severity detection of this phenomenon. Big data analytic and drought are introduced and reviewed in this paper. Besides, this review includes different studies, researches and applications of big data to drought monitoring. Challenges related to data life cycle such as data challenges, data processing challenges and data infrastructure management challenges are also discussed. Finally, we conclude that big data analytic can be beneficial in drought monitoring but there is a need for statistical and artificial intelligence-based approaches.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.ecoinf.2020.101136","jcs_value":null,"scimago_value":null},{"author":"Denglong Lv and Shibing Zhu","title":"Achieving secure big data collection based on trust evaluation and true data discovery","keywords":"Big data collection, Trust evaluation, Trust model, True data discovery, Wireless sensor network","abstract":"Data collection is an important process in the life cycle of big data processing. It is the key part that must be completed first in all kinds of data applications, which determines the results of data analysis and application service quality. However, untrusted data sources and transmission links expose the data collection process to attacks and malicious threats such as counterfeiting, replay, and denial of service, and ultimately lead to untrustworthy data. In order to cope with the threat of data collection process and ensure data quality, this paper proposes trust evaluation scheme for data security collection based on wireless sensor network, one of the data collection applications, including direct trust, recommendation trust, link trust, and backhaul trust. Meanwhile, in order to realize the dynamic update of the trust of the data sources, a true data discovery and trust dynamic update mechanism based on \u03c9-FCM (Weight Fuzzy C-Mean) algorithm is proposed. The results of a large number of simulation experiments show that the proposed scheme, model and algorithm can effectively evaluate the trust of data sources and ensure the authenticity of the collected data.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.cose.2020.101937","jcs_value":null,"scimago_value":null},{"author":"Karim Moharm","title":"State of the art in big data applications in microgrid: A review","keywords":"Big data, Microgrid","abstract":"The prospering Big data era is emerging in the power grid. Multiple world-wide studies are emphasizing the big data applications in the microgrid due to the huge amount of produced data. Big data analytics can impact the design and applications towards safer, better, more profitable, and effective power grid. This paper presents the recognition and challenges of the big data and the microgrid. The construction of big data analytics is introduced. The data sources, big data opportunities, and enhancement areas in the microgrid like stability improvement, asset management, renewable energy prediction, and decision-making support are summarized. Diverse case studies are presented including different planning, operation control, decision making, load forecasting, data attacks detection, and maintenance aspects of the microgrid. Finally, the open challenges of big data in the microgrid are discussed.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.aei.2019.100945","jcs_value":null,"scimago_value":null},{"author":"Jeffrey Hughes and Kirstie Ball","title":"Sowing the seeds of value? Persuasive practices and the embedding of big data analytics","keywords":"Big data analytics, Persuasion, Practice, Capabilities, Value","abstract":"This paper draws on data from three organisational case studies and expert interviews to propose that persuasive practices are the precursors and enablers of analytical capability development. A bundle of seven practices was identified and observed to bridge multiple gaps between technical and non-technical colleagues on big data analytics (BDA) projects. The deployment of these practices varied according to the level of BDA maturity and featured a host of socio-material elements. This paper complements existing technical case studies with a fine-grained qualitative account of the managerial and human elements of BDA implementation. Effective deployment of persuasive practices potentially both embeds the benefits and mitigates the risks of BDA, sowing the seeds of many different forms of value.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2020.120300","jcs_value":null,"scimago_value":null},{"author":"Yuan Su and Yanni Yu and Ning Zhang","title":"Carbon emissions and environmental management based on Big Data and Streaming Data: A bibliometric analysis","keywords":"Big data, Streaming data, Carbon emission, Environmental management, Bibliometric analysis, Net-work analysis","abstract":"Climate change and environmental management are issues of global concern. The advent of the era of Big Data has created a new research platform for the assessment of environmental governance and policies. However, little is known about Big Data application to climate change and environmental management research. This paper adopts bibliometric analysis in conjunction with network analysis to systematically evaluate the publications on carbon emissions and environmental management based on Big Data and Streaming Data using R package and VOSviewer software. The analysis involves 274 articles after rigorous screening and includes citation analysis, co-citation analysis, and co-word analysis. Main findings include (1) Carbon emissions and environmental management based on big data and streaming data is an emerging multidisciplinary research topic, which has been applied in the fields of computer science, supply chain design, transportation, carbon price assessment, environmental policy evaluation, and CO2 emissions reduction. (2) This field has attracted the attention of nations which are major contributors to the world economy. In particular, European and American scholars have made the main contributions to this topic, and Chinese researchers have also had great impact. (3) The research content of this topic is primarily divided into four categories, including empirical studies of specific industries, air pollution governance, technological innovation, and low-carbon transportation. Our findings suggest that future research should bring greater depth of practical and modeling analysis to environmental policy assessment based on Big Data.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.scitotenv.2020.138984","jcs_value":null,"scimago_value":null},{"author":"Nick Hajli and Mina Tajvidi and Ayantunji Gbadamosi and Waqar Nadeem","title":"Understanding market agility for new product success with big data analytics","keywords":"Big data analytics, Customer agility, Effective use of data, New product success","abstract":"The complexity that characterises the dynamic nature of the various environmental factors makes it very compelling for firms to be capable of addressing the changing customers' needs. The current study examines the role of big data in new product success. We develop a qualitative research with case study approach to look at this. Specifically, we look at multiple cases to get in-depth understanding of customer agility for new product success with big data analytics. The findings of the study provide insight into the role of customer agility in new product success. This study unpacks the interconnectedness of the effective use of data aggregation tools, the effectiveness of data analysis tools and customer agility. It also explores the link between all of these factors and new product success. The study is reasonably telling in that it shows that the effective use of data aggregation and data analysis tools results in customer agility which in itself explains how an organisation senses and responds speedily to opportunities for innovation in the competitive marketing environment. The current study provides significant theoretical contributions by providing evidence for the role of big data analytics, big data aggregation tools, customer agility, organisational slack and environmental turbulence in new product success.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.indmarman.2019.09.010","jcs_value":null,"scimago_value":null},{"author":"Alex Neilson and  Indratmo and Ben Daniel and Stevanus Tjandra","title":"Systematic Review of the Literature on Big Data in the Transportation Domain: Concepts and Applications","keywords":"Big Data, Smart city, Intelligent transportation system, Connected vehicle, Road traffic safety, Vision Zero","abstract":"Research in Big Data and analytics offers tremendous opportunities to utilize evidence in making decisions in many application domains. To what extent can the paradigms of Big Data and analytics be used in the domain of transport? This article reports on an outcome of a systematic review of published articles in the last five years that discuss Big Data concepts and applications in the transportation domain. The goal is to explore and understand the current research, opportunities, and challenges relating to the utilization of Big Data and analytics in transportation. The review shows the potential of Big Data and analytics to garner insights and improve transportation systems through the analysis of various forms of data obtained from traffic monitoring systems, connected vehicles, crowdsourcing, and social media. We discuss some platforms and software architecture for the transport domain, along with a wide array of storage, processing, and analytical techniques, and describe challenges associated with the implementation of Big Data and analytics. This review contributes broadly to the various ways in which cities can utilize Big Data in transportation to guide the creation of sustainable and safer traffic systems. Since research in Big Data and transportation is, by and large, at infancy, this article does not prescribe recommendations to the various challenges identified, which also constitutes the limitation of the article.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.bdr.2019.03.001","jcs_value":null,"scimago_value":null},{"author":"Elaheh Yadegaridehkordi and Mehdi Hourmand and Mehrbakhsh Nilashi and Liyana Shuib and Ali Ahani and Othman Ibrahim","title":"Influence of big data adoption on manufacturing companies' performance: An integrated DEMATEL-ANFIS approach","keywords":"Big data, Firm performance, Manufacturing companies, DEMATEL, ANFIS","abstract":"Big Data is one of the recent technological advances with the strong applicability in almost every industry, including manufacturing. However, despite business opportunities offered by this technology, its adoption is still in early stage in many industries. Thus, this study aimed to identify and rank the significant factors influencing adoption of big data and in turn to predict the influence of big data adoption on manufacturing companies' performance using a hybrid approach of decision-making trial and evaluation laboratory (DEMATEL)- adaptive neuro-fuzzy inference systems (ANFIS). This study identified the critical adoption factors from literature review and categorized them into technological, organizational and environmental dimensions. Data was collected from 234 industrial managers who were involved in the decision-making process regarding IT procurement in Malaysian manufacturing companies. Research results showed that technological factors (perceived benefits, complexity, technology resources, big data quality and integration) have the highest influence on the big data adoption and firms' performance. This study is one of the pioneers in using DEMATEL-ANFIS approach in the big data adoption context. In addition to the academic contribution, findings of this study can hopefully assist manufacturing industries, big data service providers, and governments to precisely focus on vital factors found in this study in order to improve firm performance by adopting big data.","year":2018,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2018.07.043","jcs_value":null,"scimago_value":null},{"author":"Debashis Das and Chinmay Chakraborty and Sourav Banerjee","title":"Chapter 7 - A Framework Development on Big Data Analytics for Terahertz Healthcare","keywords":"3D imaging, Big data, Genomic expression, Healthcare technology, Medical sensors, Personal medical information, Radiology images","abstract":"This chapter is mainly focused on the development of big data analytics in terahertz healthcare technology. In today's world, \u201cbig data\u201d is a very familiar term, but the way it is interpreted is modified day by day. Healthcare is one aspect in which big data can be utilized to improve the overall system of healthcare, as, in the context of healthcare, the three primary \u201cV's\u201d of big data definition, volume, variety, and velocity, are very well suited. According to big data analysts, error-free analysis and outcome are ensured from big data, but this is really difficult for medical data due to the issues regarding the data quality. The final V related to big data is value, i.e., how much leverage the data can provide. Thus we can conclude that healthcare is a much preferred area for expert big data analysis. But there are several challenges to be faced in every aspect, starting from data collection to storage, analysis, prediction, etc. The main challenge is the unstructured nature of the data and the organizations from where the data are collected, following no standardized rules, which forms a big gap in the processing of information. Also, a huge investment is required for resources such as high-level expertise, knowledge, technologies used for data analytics, common data warehouses (for obtaining homogeneous data), etc. Despite having so many obstructions, big data analytics has already started growing rapidly in the healthcare sector.","year":2020,"ENTRYTYPE":"incollection","doi":"https:\/\/doi.org\/10.1016\/B978-0-12-818556-8.00007-0","jcs_value":null,"scimago_value":null},{"author":"Chiaojung Jillian Tsai and Nadeem Riaz and Scarlett Lin Gomez","title":"Big Data in Cancer Research: Real-World Resources for Precision Oncology to Improve Cancer Care Delivery","keywords":null,"abstract":"In oncology, the term \u201cbig data\u201d broadly describes the rapid acquisition and generation of massive amounts of information, typically from population cancer registries, electronic health records, or large-scale genetic sequencing studies. The challenge of using big data in cancer research lies in interdisciplinary collaboration and information processing to unify diverse data sources and provide valid analytics to harness meaningful information. This article provides an overview of how big data approaches can be applied in cancer research, and how they can be used to translate information into new ways to ultimately make informed decisions that improve cancer care and delivery.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.semradonc.2019.05.002","jcs_value":null,"scimago_value":null},{"author":"Safa Ben Atitallah and Maha Driss and Wadii Boulila and Henda Ben Gh\u00e9zala","title":"Leveraging Deep Learning and IoT big data analytics to support the smart cities development: Review and future directions","keywords":"Internet of Things, Deep Learning, Smart city, Big data analytics, Review","abstract":"The rapid growth of urban populations worldwide imposes new challenges on citizens\u2019 daily lives, including environmental pollution, public security, road congestion, etc. New technologies have been developed to manage this rapid growth by developing smarter cities. Integrating the Internet of Things (IoT) in citizens\u2019 lives enables the innovation of new intelligent services and applications that serve sectors around the city, including healthcare, surveillance, agriculture, etc. IoT devices and sensors generate large amounts of data that can be analyzed to gain valuable information and insights that help to enhance citizens\u2019 quality of life. Deep Learning (DL), a new area of Artificial Intelligence (AI), has recently demonstrated the potential for increasing the efficiency and performance of IoT big data analytics. In this survey, we provide a review of the literature regarding the use of IoT and DL to develop smart cities. We begin by defining the IoT and listing the characteristics of IoT-generated big data. Then, we present the different computing infrastructures used for IoT big data analytics, which include cloud, fog, and edge computing. After that, we survey popular DL models and review the recent research that employs both IoT and DL to develop smart applications and services for smart cities. Finally, we outline the current challenges and issues faced during the development of smart city services.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.cosrev.2020.100303","jcs_value":null,"scimago_value":null},{"author":"Saqib Shamim and Jing Zeng and Zaheer Khan and Najam Ul Zia","title":"Big data analytics capability and decision making performance in emerging market firms: The role of contractual and relational governance mechanisms","keywords":"Big data, Contractual governance, Relational governance, Big data analytics capability, Culture, Decision-making performance, Emerging markets","abstract":"This study examines the role of big data contractual and relational governance in big data decision-making performance of firms based in China. It investigates the mediation of big data analytics (BDA) capability in the association of contractual and relational governance with decision-making performance. Furthermore, moderating role of data-driven culture in the relationship of BDA capability and decision-making performance is examined. Data are collected from 108 Chinese firms engaged in big data-related activities. Structural equation modeling is employed to test the hypotheses. This study contributes towards the literature on big data management and governance mechanisms, by establishing the relationship of decision-making performance with big data contractual and relational governance directly and through the mediation of BDA capabilities. It also contributes towards knowledge based dynamic capabilities (KBDCs) view of firms, arguing that dynamic capabilities such as BDA capabilities can be influenced through knowledge sources and activities. We add to the discussions on whether contractual and relational governance are alternatives or they complement each other, by establishing the moderating role of big data relational governance in the relationship of contractual governance and decision-making performance. Finally, we argue that social capital can enhance KBDCs through contractual and relational governance in big data context.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2020.120315","jcs_value":null,"scimago_value":null},{"author":"Saqib Shamim and Jing Zeng and Syed Muhammad Shariq and Zaheer Khan","title":"Role of big data management in enhancing big data decision-making capability and quality among Chinese firms: A dynamic capabilities view","keywords":"Big data management, Dynamic capabilities, Big data decision-making capability, Decision-making quality, China","abstract":"This study examines the antecedents and influence of big data decision-making capabilities on decision-making quality among Chinese firms. We propose that such capabilities are influenced by big data management challenges such as leadership, talent management, technology, and organisational culture. By using primary data from 108 Chinese firms and utilising partial least squares, we tested the antecedents of big data decision-making capability and its impact on decision-making quality. Findings suggest that big data management challenges are the key antecedents of big data decision-making capability. Furthermore, the latter is vital for big data decision-making quality.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.im.2018.12.003","jcs_value":null,"scimago_value":null},{"author":"Tiago F. Silva and Cleber L. Rodrigues and Manfredo H. Tabacniks and Hugo D.C. Pereira and Thiago B. Saramela and Renato O. Guimar\u00e3es","title":"Ion beam analysis and big data: How data science can support next-generation instrumentation","keywords":"Ion beam analysis, Big data, Data quality assurance, Artificial intelligence","abstract":"With a growing demand for accurate ion beam analysis on a large number of samples, it becomes an issue of how to ensure the quality standards and consistency over hundreds or thousands of samples. In this sense, a virtual assistant that checks the data quality, emitting certificates of quality, is highly desired. Even the processing of a massive number of spectra is a problem regarding the consistency of the analysis. In this work, we report the design and first results of a virtual layer under implementation in our laboratory. It consists of a series of systems running in the cloud that perform the mentioned tasks and serves as a virtual assistant for member staff and users. We aim to bring the concept of the Internet of Things and artificial intelligence closer to the laboratory to support a new generation of instrumentation.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.nimb.2020.05.027","jcs_value":null,"scimago_value":null},{"author":"Ohbyung Kwon and Namyeon Lee and Bongsik Shin","title":"Data quality management, data usage experience and acquisition intention of big data analytics","keywords":"Big data analytics, Resource-based view, Data quality management, IT capability, Data usage","abstract":"Big data analytics associated with database searching, mining, and analysis can be seen as an innovative IT capability that can improve firm performance. Even though some leading companies are actively adopting big data analytics to strengthen market competition and to open up new business opportunities, many firms are still in the early stage of the adoption curve due to lack of understanding of and experience with big data. Hence, it is interesting and timely to understand issues relevant to big data adoption. In this study, a research model is proposed to explain the acquisition intention of big data analytics mainly from the theoretical perspectives of data quality management and data usage experience. Our empirical investigation reveals that a firm's intention for big data analytics can be positively affected by its competence in maintaining the quality of corporate data. Moreover, a firm's favorable experience (i.e., benefit perceptions) in utilizing external source data could encourage future acquisition of big data analytics. Surprisingly, a firm's favorable experience (i.e., benefit perceptions) in utilizing internal source data could hamper its adoption intention for big data analytics.","year":2014,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijinfomgt.2014.02.002","jcs_value":null,"scimago_value":null},{"author":"Jianzheng Liu and Jie Li and Weifeng Li and Jiansheng Wu","title":"Rethinking big data: A review on the data quality and usage issues","keywords":"Big data, Data quality and error, Data ethnics, Spatial information sciences","abstract":"The recent explosive publications of big data studies have well documented the rise of big data and its ongoing prevalence. Different types of \u201cbig data\u201d have emerged and have greatly enriched spatial information sciences and related fields in terms of breadth and granularity. Studies that were difficult to conduct in the past time due to data availability can now be carried out. However, big data brings lots of \u201cbig errors\u201d in data quality and data usage, which cannot be used as a substitute for sound research design and solid theories. We indicated and summarized the problems faced by current big data studies with regard to data collection, processing and analysis: inauthentic data collection, information incompleteness and noise of big data, unrepresentativeness, consistency and reliability, and ethical issues. Cases of empirical studies are provided as evidences for each problem. We propose that big data research should closely follow good scientific practice to provide reliable and scientific \u201cstories\u201d, as well as explore and develop techniques and methods to mitigate or rectify those \u2018big-errors\u2019 brought by big data.","year":2016,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.isprsjprs.2015.11.006","jcs_value":null,"scimago_value":null},{"author":"Canchu Lin and Anand Kunnathur","title":"Strategic orientations, developmental culture, and big data capability","keywords":"Big data capability, Customer orientation, Entrepreneurial orientation, Technology orientation, And developmental culture","abstract":"Prior research articulated the importance of developing a big data analytics capability but did not show how to cultivate this development. Drawing on the literature on this topic, this study develops the concept of Big Data capability, which enhances our understanding of Big Data practice beyond that captured in previous literature on the concept of big data analytics capability. This study further highlights the strategic implications of the concept by testing its relationship to three strategic orientations and one aspect of organizational culture. Findings show that customer, entrepreneurial, and technology orientations, and developmental culture are important contributors to the development of Big Data capability.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.jbusres.2019.07.016","jcs_value":null,"scimago_value":null},{"author":"Le Yao and Zhiqiang Ge","title":"Big data quality prediction in the process industry: A distributed parallel modeling framework","keywords":"Distributed modeling, MapReduce framework, Parallel computing, Quality prediction, Big data analytics","abstract":"With the ever increasing data collected from the process, the era of big data has arrived in the process industry. Therefore, the computational effort for data modeling and analytics in standalone modes has become increasingly demanding, particularly for large-scale processes. In this paper, a distributed parallel process modeling approach is presented based on a MapReduce framework for big data quality prediction. Firstly, the architecture for distributed parallel data modeling is formulated under the MapReduce framework. Secondly, a big data quality prediction scheme is developed based on the distributed parallel data modeling approach. As an example, the basic Semi-Supervised Probabilistic Principal Component Regression (SSPPCR) model is deployed to concurrently train a set of local models with split datasets. Meanwhile, Bayesian rule is utilized in a MapReduce way to integrate local models based on their predictive abilities. Two case studies demonstrate the effectiveness of the proposed method for big data quality prediction.","year":2018,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.jprocont.2018.04.004","jcs_value":null,"scimago_value":null},{"author":"Diego Garc\u00eda-Gil and Juli\u00e1n Luengo and Salvador Garc\u00eda and Francisco Herrera","title":"Enabling Smart Data: Noise filtering in Big Data classification","keywords":"Big Data, Smart Data, Classification, Class noise, Label noise.","abstract":"In any knowledge discovery process the value of extracted knowledge is directly related to the quality of the data used. Big Data problems, generated by massive growth in the scale of data observed in recent years, also follow the same dictate. A common problem affecting data quality is the presence of noise, particularly in classification problems, where label noise refers to the incorrect labeling of training instances, and is known to be a very disruptive feature of data. However, in this Big Data era, the massive growth in the scale of the data poses a challenge to traditional proposals created to tackle noise, as they have difficulties coping with such a large amount of data. New algorithms need to be proposed to treat the noise in Big Data problems, providing high quality and clean data, also known as Smart Data. In this paper, two Big Data preprocessing approaches to remove noisy examples are proposed: an homogeneous ensemble and an heterogeneous ensemble filter, with special emphasis in their scalability and performance traits. The obtained results show that these proposals enable the practitioner to efficiently obtain a Smart Dataset from any Big Data classification problem.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.ins.2018.12.002","jcs_value":null,"scimago_value":null},{"author":"Nadine C\u00f4rte-Real and Pedro Ruivo and Tiago Oliveira","title":"Leveraging internet of things and big data analytics initiatives in European and American firms: Is data quality a way to extract business value?","keywords":"Big data analytics, Internet of things, Strategic management, Knowledge-based theory, Dynamics capability theory","abstract":"Big data analytics (BDA) and the Internet of Things (IoT) tools are considered crucial investments for firms to distinguish themselves among competitors. Drawing on a strategic management perspective, this study proposes that BDA and IoT capabilities can create significant value in business processes if supported by a good level of data quality, which will lead to a better competitive advantage. Responses are collected from 618 European and American firms that use IoT and BDA applications. Partial least squares results reveal that better data quality is needed to unlock the value of IoT and BDA capabilities.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.im.2019.01.003","jcs_value":null,"scimago_value":null},{"author":"Zhihan Lv and Liang Qiao","title":"Analysis of healthcare big data","keywords":"Big data, Health care, Privacy security risk, Privacy measures","abstract":"In order to explore the development of healthcare in China and the privacy and security risk factors in medical data under the background of big data, the development status of China\u2019s healthcare sector is analyzed. The questionnaire is used to analyze the privacy and security risk factors of healthcare big data and protection measures are put forward based on the data privacy and security risk factors in the context of cloud services in the literature. The results show that in recent years, the number of health institutions, the number of medical personnel, the assets of medical institutions, the per capita hospitalization cost, and the insured population all show a trend of increasing year by year; while in 2017, the crude mortality rate of malignant tumor patients was the highest in China, and the mortality rate of rural patients was higher than that of urban patients. The results of the questionnaire show that the probability of data analysis, medical treatment process, disease diagnosis process, lack of protective measures, and imperfect access system is all greater than 0.8 when medical care big data is oriented to cloud services. Based on this, two levels of privacy protection measures are proposed: technology and management. It indicates that medical institutions need to pay attention to data privacy protection and grasp the use of digital medical data to provide decision support for subsequent medical data analysis.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.future.2020.03.039","jcs_value":null,"scimago_value":null},{"author":"Emily L. Gill and Stephen R. Master","title":"Big Data Everywhere: The Impact of Data Disjunction in the Direct-to-Consumer Testing Model","keywords":"Big Data, Laboratory medicine, Machine learning, Direct-to-consumer testing, DTC, Harmonization","abstract":null,"year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.cll.2019.11.009","jcs_value":null,"scimago_value":null},{"author":"Renato Arbex and Claudio B. Cunha","title":"Estimating the influence of crowding and travel time variability on accessibility to jobs in a large public transport network using smart card big data","keywords":"Public transport, Accessibility, Smart card data, In-vehicle crowding, Travel time reliability","abstract":"Accessibility metrics are gaining momentum in public transportation planning and policy-making. However, critical user experience issues such as crowding discomfort and travel time unreliability are still not considered in those accessibility indicators. This paper aims to apply a methodology to build spatiotemporal crowding data and estimate travel time variability in a congested public transport network to improve accessibility calculations. It relies on using multiple big data sources available in most transit systems such as smart card and automatic vehicle location (AVL) data. S\u00e3o Paulo, Brazil, is used as a case study to show the impact of crowding and travel time variability on accessibility to jobs. Our results evidence a population-weighted average reduction of 56.8% in accessibility to jobs in a regular workday morning peak due to crowding discomfort, as well as reductions of 6.2% due to travel time unreliability and 59.2% when both are combined. The findings of this study can be of invaluable help to public transport planners and policymakers, as they show the importance of including both aspects in accessibility indicators for better decision making. Despite some limitations due to data quality and consistency throughout the study period, the proposed approach offers a new way to leverage big data in public transport to enhance policy decisions.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.jtrangeo.2020.102671","jcs_value":null,"scimago_value":null},{"author":"Ant\u00f3nio A.C. Vieira and Lu\u00eds Dias and Maribel Y. Santos and Guilherme A.B. Pereira and Jos\u00e9 Oliveira","title":"Are Simulation Tools Ready For Big Data? Computational Experiments with Supply Chain Models Developed in Simio","keywords":"Simulation, Supply Chain, Big Data, Industry 4.0","abstract":"The need and potential benefits for the combined use of Simulation and Big Data in Supply Chains (SCs) has been widely recognized. Having worked on such project, some simulation experiments of the modelled SC system were conducted in SIMIO. Different circumstances were tested, including running the model based on the stored data, on statistical distributions and considering risk situations. Thus, this paper aimed to evaluate such experiments, to evaluate the performance of simulations in these contexts. After analyzing the obtained results, it was found that whilst running the model based on the real data required considerable amounts of computer memory, running the model based on statistical distributions reduced such values, albeit required considerable higher time to run a single replication. In all the tested experiments, the simulation took considerable time to run and was not smooth, which can reduce the stakeholders\u2019 interest in the developed tool, despite its benefits for the decision-making process. For future researches, it would be beneficial to test other simulation tools and other strategies and compare those results to the ones provided in this paper.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.promfg.2020.02.093","jcs_value":null,"scimago_value":null},{"author":"Maria Ijaz Baig and Liyana Shuib and Elaheh Yadegaridehkordi","title":"Big data adoption: State of the art and research challenges","keywords":"Big data adoption, Technology\u2013Organization\u2013Environment, Diffusion of Innovations","abstract":"Big data adoption is a process through which businesses find innovative ways to enhance productivity and predict risk to satisfy customers need more efficiently. Despite the increase in demand and importance of big data adoption, there is still a lack of comprehensive review and classification of the existing studies in this area. This research aims to gain a comprehensive understanding of the current state-of-the-art by highlighting theoretical models, the influence factors, and the research challenges of big data adoption. By adopting a systematic selection process, twenty studies were identified in the domain of big data adoption and were reviewed in order to extract relevant information that answers a set of research questions. According to the findings, Technology\u2013Organization\u2013Environment and Diffusion of Innovations are the most popular theoretical models used for big data adoption in various domains. This research also revealed forty-two factors in technology, organization, environment, and innovation that have a significant influence on big data adoption. Finally, challenges found in the current research about big data adoption are represented, and future research directions are recommended. This study is helpful for researchers and stakeholders to take initiatives that will alleviate the challenges and facilitate big data adoption in various fields.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.ipm.2019.102095","jcs_value":null,"scimago_value":null},{"author":"Abdellah Daissaoui and Azedine Boulmakoul and Lamia Karim and Ahmed Lbath","title":"IoT and Big Data Analytics for Smart Buildings: A Survey","keywords":"Smart buildings, IoT, Big data analytics, Reactif systems, Complex event processing","abstract":"The processes of digital transformation have involved a variety of socio-technical activities, with the objective of increasing productivity, safety and quality of execution, sustainable development, collaborative working and solutions for the sustainable smart city. The major digital trends, changing the building sector and revealing new trends of understanding information technologies to integrate in this sector. Current smart building management systems incorporate a variety of sensors, actuators and dedicated networks. Their objectives are to observe the condition of specific areas and apply appropriate rules to preserve or improve comfort while saving energy. In this paper, we propose a review of related works to IoT, Big Data Analytics in smart buildings.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.procs.2020.03.021","jcs_value":null,"scimago_value":null},{"author":"Benjamin T. Hazen and Christopher A. Boone and Jeremy D. Ezell and L. Allison Jones-Farmer","title":"Data quality for data science, predictive analytics, and big data in supply chain management: An introduction to the problem and suggestions for research and applications","keywords":"Data quality, Statistical process control, Knowledge-based view, Organizational information processing view, Systems theory","abstract":"Today\u05f3s supply chain professionals are inundated with data, motivating new ways of thinking about how data are produced, organized, and analyzed. This has provided an impetus for organizations to adopt and perfect data analytic functions (e.g. data science, predictive analytics, and big data) in order to enhance supply chain processes and, ultimately, performance. However, management decisions informed by the use of these data analytic methods are only as good as the data on which they are based. In this paper, we introduce the data quality problem in the context of supply chain management (SCM) and propose methods for monitoring and controlling data quality. In addition to advocating for the importance of addressing data quality in supply chain research and practice, we also highlight interdisciplinary research topics based on complementary theory.","year":2014,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijpe.2014.04.018","jcs_value":null,"scimago_value":null},{"author":"Andy S. Alic and Jussara Almeida and Giovanni Aloisio and Nazareno Andrade and Nuno Antunes and Danilo Ardagna and Rosa M. Badia and Tania Basso and Ignacio Blanquer and Tarciso Braz and Andrey Brito and Donatello Elia and Sandro Fiore and Dorgival Guedes and Marco Lattuada and Daniele Lezzi and Matheus Maciel and Wagner Meira and Demetrio Mestre and Regina Moraes and Fabio Morais and Carlos Eduardo Pires and N\u00e1dia P. Kozievitch and Walter dos Santos and Paulo Silva and Marco Vieira","title":"BIGSEA: A Big Data analytics platform for public transportation information","keywords":null,"abstract":"Analysis of public transportation data in large cities is a challenging problem. Managing data ingestion, data storage, data quality enhancement, modelling and analysis requires intensive computing and a non-trivial amount of resources. In EUBra-BIGSEA (Europe\u2013Brazil Collaboration of Big Data Scientific Research Through Cloud-Centric Applications) we address such problems in a comprehensive and integrated way. EUBra-BIGSEA provides a platform for building up data analytic workflows on top of elastic cloud services without requiring skills related to either programming or cloud services. The approach combines cloud orchestration, Quality of Service and automatic parallelisation on a platform that includes a toolbox for implementing privacy guarantees and data quality enhancement as well as advanced services for sentiment analysis, traffic jam estimation and trip recommendation based on estimated crowdedness. All developments are available under Open Source licenses (http:\/\/github.org\/eubr-bigsea, https:\/\/hub.docker.com\/u\/eubrabigsea\/).","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.future.2019.02.011","jcs_value":null,"scimago_value":null},{"author":"Yongheng Zhang and Rui Zhang and Yizhong Wang and Hongfei Guo and Ray Y Zhong and Ting Qu and Zhiwu Li","title":"Big data driven decision-making for batch-based production systems","keywords":"Big data, Smart Product-Service System, Sales predict, Economic batch quantity, production plan","abstract":"The era of big data has brought new challenges to chemical enterprises. In order to maximize the benefits, enterprises are considering to implement intelligent service technology into traditional production systems to improve the level of intelligence in business. This paper proposes a service framework based on big data driven prediction, which includes information perception layer, information application layer and big data service layer. In this paper, the composition of big data service layer is described in detail, and a sales predicting method based on neural network is introduced. The salability of products is divided, and the qualitative economic production volume mechanism is finally given. Based on the framework, an intelligent service system for enterprises with the characteristics of mass production is implemented. Experimental results show that the big data service framework can support chemical enterprises to make decisions to reduce costs, and provides an effective method for Smart Product Service System (PSS).","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.procir.2019.05.023","jcs_value":null,"scimago_value":null},{"author":"Gianluca Elia and Gloria Polimeno and Gianluca Solazzo and Giuseppina Passiante","title":"A multi-dimension framework for value creation through big data","keywords":"Big data analytics, Cognitive computing, Framework, Model, Systematic literature review, Value creation","abstract":"Big Data represents a promising area for value creation and frontier research. The potential to extract actionable insights from Big Data has gained increasing attention of both academics and practitioners operating in several industries. Marketing domain has become from the start a field for experiments with Big Data approaches, even if the adoption of Big Data solutions does not always generate effective value for the adopters. Therefore, the gap existing between the potential of value creation embedded in the Big Data paradigm and the current limited exploitation of this value represents an area of investigation that this paper aims to explore. In particular, by following a systematic literature review, this study aims at presenting a framework that outlines the multiple value directions that the Big Data paradigm can generate for the adopting organizations. Eleven distinct value directions have been identified and then grouped in five dimensions (Informational, Transactional, Transformational, Strategic, Infrastructural Value), which constitute the pillars of the proposed framework. Finally, the framework has been also preliminarily applied in three case studies conducted within three Italian based companies operating in different industries (e-commerce, fast-moving consumer goods, and banking) in the final aim to see its applicability in real business scenarios.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.indmarman.2019.08.004","jcs_value":null,"scimago_value":null},{"author":"Ant\u00f3nio AC Vieira and Lu\u00eds MS Dias and Maribel Y Santos and Guilherme AB Pereira and Jos\u00e9 A Oliveira","title":"On the use of simulation as a Big Data semantic validator for supply chain management","keywords":"Simulation, Big Data, Data issues, Semantic validation, Supply chain management, Industry 4.0","abstract":"Simulation stands out as an appropriate method for the Supply Chain Management (SCM) field. Nevertheless, to produce accurate simulations of Supply Chains (SCs), several business processes must be considered. Thus, when using real data in these simulation models, Big Data concepts and technologies become necessary, as the involved data sources generate data at increasing volume, velocity and variety, in what is known as a Big Data context. While developing such solution, several data issues were found, with simulation proving to be more efficient than traditional data profiling techniques in identifying them. Thus, this paper proposes the use of simulation as a semantic validator of the data, proposed a classification for such issues and quantified their impact in the volume of data used in the final achieved solution. This paper concluded that, while SC simulations using Big Data concepts and technologies are within the grasp of organizations, their data models still require considerable improvements, in order to produce perfect mimics of their SCs. In fact, it was also found that simulation can help in identifying and bypassing some of these issues.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.simpat.2019.101985","jcs_value":null,"scimago_value":null},{"author":"Cangyu Jin and Yamine Bouzembrak and Jiehong Zhou and Qiao Liang and Leonieke M. {van den Bulk} and Anand Gavai and Ningjing Liu and Lukas J. {van den Heuvel} and Wouter Hoenderdaal and Hans J.P. Marvin","title":"Big Data in food safety- A review","keywords":null,"abstract":"The massive rise of Big Data generated from smartphones, social media, Internet of Things (IoT), and multimedia, has produced an overwhelming flow of data in either structured or unstructured format. Big Data technologies are being developed and implemented in the food supply chain that gather and analyse these data. Such technologies demand new approaches in data collection, storage, processing and knowledge extraction. In this article, an overview of the recent developments in Big Data applications in food safety are presented. This review shows that the use of Big Data in food safety remains in its infancy but it is influencing the entire food supply chain. Big Data analysis is used to provide predictive insights in several steps in the food supply chain, support supply chain actors in taking real time decisions, and design the monitoring and sampling strategies. Lastly, the main research challenges that require research efforts are introduced.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.cofs.2020.11.006","jcs_value":null,"scimago_value":null},{"author":"Nelson Lozada and Jose Arias-P\u00e9rez and Geovanny Perdomo-Charry","title":"Big data analytics capability and co-innovation: An empirical study","keywords":"Business, Economics, Information science, Big data analytics capabilities, Co-innovation, Big data, Co-creation","abstract":"There are numerous emerging studies addressing big data and its application in different organizational aspects, especially regarding its impact on the business innovation process. This study in particular aims at analyzing the existing relationship between Big Data Analytics Capabilities and Co-innovation. To test the hypothesis model, structural equations by the partial least squares method were used in a sample of 112 Colombian firms. The main findings allow to positively relate Big Data Analytics Capabilities with better and more agile processes of product and service co-creation and with more robust collaboration networks with stakeholders internal and external to the firm.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.heliyon.2019.e02541","jcs_value":null,"scimago_value":null},{"author":"Hadi Gholizadeh and Hamed Fazlollahtabar and Mohammad Khalilzadeh","title":"A robust fuzzy stochastic programming for sustainable procurement and logistics under hybrid uncertainty using big data","keywords":"logistics, Robust fuzzy stochastic programming, Sustainable procurement, Big data, Hybrid uncertainty, \u03b5-Constraint","abstract":"Today, in many organizations, the debate about the difference in core capabilities has become an important factor for market competition. Companies, based on the field of activity, decide to strengthen some of their capabilities, capacities, and expertise. Therefore, the focus of an organization on the strengths and efforts to develop its sustainability will lead to a competitive advantage in the marketplace. Due to changes in environmental factors, organizations have focused on carbon emissions in procurement and transportation that have the highest carbon footprint. This paper proposes a multi-objective, eco-sustainability model for a supply chain. The objectives are to minimize overall costs, maximize the efficiency of transportation vehicles and minimize information fraud in the process of information sharing within supply chain elements. Big data is considered in the amount of information exchanged between customers and other elements of the proposed supply chain; since there are frauds in information sharing then using big data 5Vs the model is adapted to control the cost of information loss leading to customer dissatisfaction. Since uncertainty is inevitable in the real environments, in this research hybrid uncertainty is considered. Because two sources of uncertainty are considered in most of the parameters, thus it is necessary to robustify the decision-making process. The model is a mixed integer nonlinear program including big data for an optimal sustainable procurement and transportation decision. A heuristic method is used to solve the big data problem that makes use of a robust fuzzy stochastic programming approach. The proposed model can prevent disturbances by using a scenario-based stochastic programming approach. An effective hybrid robust fuzzy stochastic method is also employed for controlling uncertainty in parameters and risk taking out of outbound decisions. To solve the multi-objective model, augmented \u03b5-constraint method is utilized. The model performance is investigated in a comprehensive computational study.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.jclepro.2020.120640","jcs_value":null,"scimago_value":null},{"author":"Ashish Kumar Jha and Maher A.N. Agi and Eric W.T. Ngai","title":"A note on big data analytics capability development in supply chain","keywords":"Big data, Analytics, Capability development, Qualitative study, Supply chain","abstract":"Big data analytics (BDA) are gaining importance in all aspects of business management. This is driven by both the presence of large-scale data and management's desire to root decisions in data. Extant research demonstrates that supply chain and operations management functions are among the biggest sources and users of data in the company. Therefore, their decision-making processes would benefit from increased use of BDA technologies. However, there is still a lack of understanding of what determines a company's ability to build BDA capability to gain a competitive advantage. In this study, we attempt to answer this fundamental question by identifying the factors that assist a company in or inhibit it from building its BDA capability and maximizing its gains through BDA technologies. We base our findings on a qualitative analysis of data collected from field visits, interviews with senior management, and secondary resources. We find that, in addition to technical capacity, competitive landscape and intra-firm power dynamics play an important role in building BDA capability and using BDA technologies.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.dss.2020.113382","jcs_value":null,"scimago_value":null},{"author":"Mladen Kezunovic and Pierre Pinson and Zoran Obradovic and Santiago Grijalva and Tao Hong and Ricardo Bessa","title":"Big data analytics for future electricity grids","keywords":"Electricity grids, Analytics, Big data, Decision-making","abstract":"This paper provides a survey of big data analytics applications and associated implementation issues. The emphasis is placed on applications that are novel and have demonstrated value to the industry, as illustrated using field data and practical applications. The paper reflects on the lessons learned from initial implementations, as well as ideas that are yet to be explored. The various data science trends treated in the literature are outlined, while experiences from applying them in the electricity grid setting are emphasized to pave the way for future applications. The paper ends with opportunities and challenges, as well as implementation goals and strategies for achieving impactful outcomes.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.epsr.2020.106788","jcs_value":null,"scimago_value":null},{"author":"Yesheng Cui and Sami Kara and Ka C. Chan","title":"Manufacturing big data ecosystem: A systematic literature review","keywords":"Smart manufacturing, Big data, Cloud computing, Cloud manufacturing, Internet of things, NoSQL","abstract":"Advanced manufacturing is one of the core national strategies in the US (AMP), Germany (Industry 4.0) and China (Made-in China 2025). The emergence of the concept of Cyber Physical System (CPS) and big data imperatively enable manufacturing to become smarter and more competitive among nations. Many researchers have proposed new solutions with big data enabling tools for manufacturing applications in three directions: product, production and business. Big data has been a fast-changing research area with many new opportunities for applications in manufacturing. This paper presents a systematic literature review of the state-of-the-art of big data in manufacturing. Six key drivers of big data applications in manufacturing have been identified. The key drivers are system integration, data, prediction, sustainability, resource sharing and hardware. Based on the requirements of manufacturing, nine essential components of big data ecosystem are captured. They are data ingestion, storage, computing, analytics, visualization, management, workflow, infrastructure and security. Several research domains are identified that are driven by available capabilities of big data ecosystem. Five future directions of big data applications in manufacturing are presented from modelling and simulation to real-time big data analytics and cybersecurity.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.rcim.2019.101861","jcs_value":null,"scimago_value":null},{"author":"Robert Northcott","title":"Big data and prediction: Four case studies","keywords":"Big data, Prediction, Case studies, Explanation, Elections, Weather","abstract":"Has the rise of data-intensive science, or \u2018big data\u2019, revolutionized our ability to predict? Does it imply a new priority for prediction over causal understanding, and a diminished role for theory and human experts? I examine four important cases where prediction is desirable: political elections, the weather, GDP, and the results of interventions suggested by economic experiments. These cases suggest caution. Although big data methods are indeed very useful sometimes, in this paper's cases they improve predictions either limitedly or not at all, and their prospects of doing so in the future are limited too.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.shpsa.2019.09.002","jcs_value":null,"scimago_value":null},{"author":"Jesus Silva and Omar Bonerge {Pineda Lezama} and Ligia Romero and Darwin Solano and Claudia Fern\u00e1ndez","title":"Risk Analysis of Using Big Data in Computer Sciences","keywords":"Data management, data quality, decision making, data analysis","abstract":"Today, as technologies mature and people are encouraged to contribute data to organizations\u2019 databases, more transactions are being captured than ever before. Meanwhile, improvements in data storage technologies have made the cost of evaluating, selecting, and destroying legacy data considerably greater than simply letting it accumulate. On the one hand, the excess of stored data has considerably increased the opportunities to interrelate and analyze them, while the moderate enthusiasm generated by data warehousing and data mining in the 1990s has been replaced by a rampant euphoria about big data and data analytics. But, is this as wonderful as seems? This paper presents a risk analysis of Big Data and Big Data Analytics based on a review of quality factors.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.procs.2019.11.052","jcs_value":null,"scimago_value":null},{"author":"Manwendra K. Tripathi and Randhir Kumar and Rakesh Tripathi","title":"Big-data driven approaches in materials science: A survey","keywords":"Material science, Big data, Machine learning, Data analytics, Predictive Algorithms","abstract":"The data volume is growing rapidly in material science. Every year data volume is getting double in many context of material science. The growing rate of data in material science is demanding for new computational infrastructures that can speed-up material discovery and deployment. In this survey, we are focusing on the challenges in material science due to growing data rate, and how Big Data technology can play a major role in research of material science. This survey includes various disciplines that can be used with Big Data to provide better analysis in the material science research.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.matpr.2020.02.249","jcs_value":null,"scimago_value":null},{"author":"Anita Lee-Post and Ram Pakath","title":"Numerical, secondary Big Data quality issues, quality threshold establishment, & guidelines for journal policy development","keywords":"Data quality, Big data, Secondary data, Numerical data, Quality threshold","abstract":"An IS researcher may obtain Big Data from primary or secondary data sources. Sometimes, acquiring primary Big Data is infeasible due to availability, accessibility, cost, time, and\/or complexity considerations. In this paper, we focus on Big Data-based IS research and discuss ways in which one may, post hoc, establish quality thresholds for numerical Big Data obtained from secondary sources. We also present guidelines for developing journal policies aimed at ensuring the veracity and verifiability of such data when used for research purposes.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.dss.2019.113135","jcs_value":null,"scimago_value":null},{"author":"Haitham Ghallab and Hanan Fahmy and Mona Nasr","title":"Detection outliers on internet of things using big data technology","keywords":"Internet of things, IoT, Big data, Data quality, Outliers Detection, DBSCAN, RDDs","abstract":"Internet of Things (IoT) is a fundamental concept of a new technology that will be promising and significant in various fields. IoT is a vision that allows things or objects equipped with sensors, actuators, and processors to talk and communicate with each other over the internet to achieve a meaningful goal. Unfortunately, one of the major challenges that affect IoT is data quality and uncertainty, as data volume increases noise, inconsistency and redundancy increases within data and causes paramount issues for IoT technologies. And since IoT is considered to be a massive quantity of heterogeneous networked embedded devices that generate big data, then it is very complex to compute and analyze such massive data. So this paper introduces a new model named NRDD-DBSCAN based on DBSCAN algorithm and using resilient distributed datasets (RDDs) to detect outliers that affect the data quality of IoT technologies. NRDD-DBSCAN has been applied on three different datasets of N-dimensions (2-D, 3-D, and 25-D) and the results were promising. Finally, comparisons have been made between NRDD-DBSCAN and previous models such as RDD-DBSCAN model and DBSCAN algorithm, and these comparisons proved that NRDD-DBSCAN solved the low dimensionality issue of RDD-DBSCAN model and also solved the fact that DBSCAN algorithm cannot handle IoT data. So the conclusion is that NRDD-DBSCAN proposed model can detect the outliers that exist in the datasets of N-dimensions by using resilient distributed datasets (RDDs), and NRDD-DBSCAN can enhance the quality of data exists in IoT applications and technologies.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.eij.2019.12.001","jcs_value":null,"scimago_value":null},{"author":"Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie","title":"The role of information governance in big data analytics driven innovation","keywords":"Big data analytics capabilities, Information governance, Incremental innovation, Radical innovation, Environmental uncertainty, FIMIX-PLS","abstract":"The age of big data analytics is now here, with companies increasingly investing in big data initiatives to foster innovation and outperform competition. Nevertheless, while researchers and practitioners started to examine the shifts that these technologies entail and their overall business value, it is still unclear whether and under what conditions they drive innovation. To address this gap, this study draws on the resource-based view (RBV) of the firm and information governance theory to explore the interplay between a firm\u2019s big data analytics capabilities (BDACs) and their information governance practices in shaping innovation capabilities. We argue that a firm\u2019s BDAC helps enhance two distinct types of innovative capabilities, incremental and radical capabilities, and that information governance positively moderates this relationship. To examine our research model, we analyzed survey data collected from 175 IT and business managers. Results from partial least squares structural equation modelling analysis reveal that BDACs have a positive and significant effect on both incremental and radical innovative capabilities. Our analysis also highlights the important role of information governance, as it positively moderates the relationship between BDAC\u2019s and a firm\u2019s radical innovative capability, while there is a nonsignificant moderating effect for incremental innovation capabilities. Finally, we examine the effect of environmental uncertainty conditions in our model and find that information governance and BDACs have amplified effects under conditions of high environmental dynamism.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.im.2020.103361","jcs_value":null,"scimago_value":null},{"author":"Mohamed Aboelmaged and Samar Mouakket","title":"Influencing models and determinants in big data analytics research: A bibliometric analysis","keywords":"Big data analytics, Technology adoption, Literature review, Bibliometric analysis, Theoretical models, Adoption frameworks","abstract":"Incorporating big data analytics into a particular context brings various challenges that rest on the model or framework through which individuals or organisations adopt big data to achieve their objectives. Although these models have recently triggered scholars\u2019 attention in various domains, in-depth knowledge of using each of these models in big data research is still blurred. This study enriches our knowledge on emerging models and theories that shape big data analytics adoption (BDAD) research through a bibliometric analysis of 229 studies (143 journal articles and 86 conference papers) published in indexed sources between 2013 and 2019. As a result, twenty models on BDAD have emerged (e.g., \u201cDynamic Capabilities\u201d, \u201cResource-Based View\u201d, \u201cTechnology Acceptance Model\u201d, \u201cDiffusion of Innovation\u201d, etc.). The analysis reveals that BDAD research to demonstrate attributes suggestive of a topic at an initial stage of development as it is broadly dispersed across different domains employs a wide range of models, some of which overlap. Most of the applied models are generic in nature focusing on variance-based relationships and snapshot prediction with little consensus. There is a conspicuous dearth of process models, firm-level analysis and cultural orientation in contemporary BDAD research. Insights of this bibliometric study could guide rigorous big data research and practice in various contexts. The study concludes with research implications and limitations that offer promising prospects for forthcoming research.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.ipm.2020.102234","jcs_value":null,"scimago_value":null},{"author":"Lilia Sfaxi and Mohamed Mehdi Ben\u00a0Aissa","title":"DECIDE: An Agile event-and-data driven design methodology for decisional Big Data projects","keywords":"Big Data, Methodology, Decisional systems, Agile, Data governance, Data quality","abstract":"Decision making is the lifeblood of the enterprise \u2014 from the mundane to the strategically critical. However, the increasing deluge of data makes it more important than ever to understand and use it effectively in every context. Being \u201cdata driven\u201d is more aspiration than reality in most organizations due to the complexity, volume, variability and velocity of data streams from every customer and employee interaction. The purpose of this paper is to provide a flexible and adaptable methodology for governing, managing and applying data throughout the enterprise, called DECIDE.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.datak.2020.101862","jcs_value":null,"scimago_value":null},{"author":"Mark Birkin","title":"Big Data","keywords":"Administrative data, Crowdsourcing, Data ethics, Data quality, Data sharing, Representation and bias, Social media, Spatial analysis, Value, Variety, Velocity, Volume, Volunteered geographical information","abstract":"Big Data are deeply impactful for research in providing a diverse and rich array of novel sources for academic enquiry. Geographers are benefitting from varied data types including administrative data, social media, volunteered geographic information, and consumer data. The article presents examples and illustrations associated with each of these data types. An additional benefit of Big Data analytics is that it brings geographers into closer contact with real world partners and policy problems. Big Data also attract challenges including representational bias, variable data quality, difficulties of access and ownership, and ethical and legal restrictions in their use. Hence we conclude that Big Data are ripe for fruitful exploitation, but careful investments will be necessary if opportunities are to be realized to the full.","year":2020,"ENTRYTYPE":"incollection","doi":"https:\/\/doi.org\/10.1016\/B978-0-08-102295-5.10616-X","jcs_value":606.0,"scimago_value":555.0},{"author":"Feliks P. Sejahtera Surbakti and Wei Wang and Marta Indulska and Shazia Sadiq","title":"Factors influencing effective use of big data: A research framework","keywords":"Big data, Effective use, Factors, Framework","abstract":"Information systems (IS) research has explored \u201ceffective use\u201d in a variety of contexts. However, it is yet to specifically consider it in the context of the unique characteristics of big data. Yet, organizations have a high appetite for big data, and there is growing evidence that investments in big data solutions do not always lead to the derivation of intended value. Accordingly, there is a need for rigorous academic guidance on what factors enable effective use of big data. With this paper, we aim to guide IS researchers such that the expansion of the body of knowledge on the effective use of big data can proceed in a structured and systematic manner and can subsequently lead to empirically driven guidance for organizations. Namely, with this paper, we cast a wide net to understand and consolidate from literature the potential factors that can influence the effective use of big data, so they may be further studied. To do so, we first conduct a systematic literature review. Our review identifies 41 factors, which we categorize into 7 themes, namely data quality; data privacy and security and governance; perceived organizational benefit; process management; people aspects; systems, tools, and technologies; and organizational aspects. To explore the existence of these themes in practice, we then analyze 45 published case studies that document insights into how specific companies use big data successfully. Finally, we propose a framework for the study of effective use of big data as a basis for future research. Our contributions aim to guide researchers in establishing the relevance and relationships within the identified themes and factors and are a step toward developing a deeper understanding of effective use of big data.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.im.2019.02.001","jcs_value":null,"scimago_value":null},{"author":"Fakhitah Ridzuan and Wan Mohd Nazmee {Wan Zainon}","title":"A Review on Data Cleansing Methods for Big Data","keywords":"data cleansing, big data, data quality","abstract":"Massive amounts of data are available for the organization which will influence their business decision. Data collected from the various resources are dirty and this will affect the accuracy of prediction result. Data cleansing offers a better data quality which will be a great help for the organization to make sure their data is ready for the analyzing phase. However, the amount of data collected by the organizations has been increasing every year, which is making most of the existing methods no longer suitable for big data. Data cleansing process mainly consists of identifying the errors, detecting the errors and corrects them. Despite the data need to be analyzed quickly, the data cleansing process is complex and time-consuming in order to make sure the cleansed data have a better quality of data. The importance of domain expert in data cleansing process is undeniable as verification and validation are the main concerns on the cleansed data. This paper reviews the data cleansing process, the challenge of data cleansing for big data and the available data cleansing methods.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.procs.2019.11.177","jcs_value":null,"scimago_value":null},{"author":"Kumar Rahul and Rohitash Kumar Banyal","title":"Data Life Cycle Management in Big Data Analytics","keywords":"Data life cycle, Data creation, Data usability, Healthcare, Big data","abstract":"Data life cycle management is very much useful for any enterprise or application where data is being used and processed for producing results. Data\u2019s appearance for a certain period time ensures accessibility and usability in the system. Data generated through different sources and it is available in various forms for accessibility. A big data-based application such as the healthcare sector generates lots of data through sensors and other electronic devices which can be further classified into a model for report generations and predictions for various purposes for the benefits of patients and hospitals as well. The data life cycle presents the entire data process in the system. The lifecycle of data starts from creation, store, usability, sharing, and archive and destroy in the system and applications. It defines the data flow in an organization. For the successful implementation of the model, there is a need to maintain the life cycle of data under a secured system of data management. This paper deals with the data life cycle with different steps and various works are done for data management in different sectors and benefits of the data life cycle for industrial and healthcare applications including challenges, conclusions, and future scope.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.procs.2020.06.042","jcs_value":null,"scimago_value":null},{"author":"Xin Li and Rob Law","title":"Network analysis of big data research in tourism","keywords":"Big data, Tourism studies, Co-citation analysis, Network analysis, Research trends","abstract":"This study aims to provide a comprehensive network analysis to understand the current state of big data research in tourism by investigating multi-disciplinary contributions relevant to big data. A comprehensive network analytical method, which includes co-citation, clustering and trend analysis, is applied to systematically analyse publications from 2008 to 2017. Two unique data sets from Web of Science are collected. The first data set focuses on big data research in tourism and hospitality. The second data set involves other disciplines, such as computer science, for a comparison with tourism. Results suggest that applications of social media and user-generated content are gaining momentum, whereas theory-based studies on big data in tourism remain limited. Tourism and other relevant domains have similar concerns with the challenges involved in big data, such as privacy, data quality and appropriate data use. This comparative network analysis has implications for future big data research in tourism.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.tmp.2019.100608","jcs_value":null,"scimago_value":null},{"author":"Huamao Wang and Yumei Yao and Said Salhi","title":"Tension in big data using machine learning: Analysis and applications","keywords":"Big data, Machine learning, Data size, Prediction accuracy, Social media","abstract":"The access of machine learning techniques in popular programming languages and the exponentially expanding big data from social media, news, surveys, and markets provide exciting challenges and invaluable opportunities for organizations and individuals to explore implicit information for decision making. Nevertheless, the users of machine learning usually find that these sophisticated techniques could incur a high level of tensions caused by the selection of the appropriate size of the training data set among other factors. In this paper, we provide a systematic way of resolving such tensions by examining practical examples of predicting popularity and sentiment of posts on Twitter and Facebook, blogs on Mashable, news on Google and Yahoo, the US house survey, and Bitcoin prices. Interesting results show that for the case of big data, using around 20% of the full sample often leads to a better prediction accuracy than opting for the full sample. Our conclusion is found to be consistent across a series of experiments. The managerial implication is that using more is not necessarily the best and users need to be cautious about such an important sensitivity as the simplistic approach may easily lead to inferior solutions with potentially detrimental consequences.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2020.120175","jcs_value":null,"scimago_value":null},{"author":"Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie","title":"Big data analytics and firm performance: Findings from a mixed-method approach","keywords":"Big data analytics, Complexity theory, fsQCA, Business value, Mixed-method, Environmental uncertainty","abstract":"Big data analytics has been widely regarded as a breakthrough technological development in academic and business communities. Despite the growing number of firms that are launching big data initiatives, there is still limited understanding on how firms translate the potential of such technologies into business value. The literature argues that to leverage big data analytics and realize performance gains, firms must develop strong big data analytics capabilities. Nevertheless, most studies operate under the assumption that there is limited heterogeneity in the way firms build their big data analytics capabilities and that related resources are of similar importance regardless of context. This paper draws on complexity theory and investigates the configurations of resources and contextual factors that lead to performance gains from big data analytics investments. Our empirical investigation followed a mixed methods approach using survey data from 175 chief information officers and IT managers working in Greek firms, and three case studies to show that depending on the context, big data analytics resources differ in significance when considering performance gains. Applying a fuzzy-set qualitative comparative analysis (fsQCA) method on the quantitative data, we show that there are four different patterns of elements surrounding big data analytics that lead to high performance. Outcomes of the three case studies highlight the inter-relationships between these elements and outline challenges that organizations face when orchestrating big data analytics resources.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.jbusres.2019.01.044","jcs_value":null,"scimago_value":null},{"author":"Michael R. Mathis and Timur Z. Dubovoy and Matthew D. Caldwell and Milo C. Engoren","title":"Making Sense of Big Data to Improve Perioperative Care: Learning Health Systems and the Multicenter Perioperative Outcomes Group","keywords":null,"abstract":null,"year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1053\/j.jvca.2019.11.012","jcs_value":null,"scimago_value":null},{"author":"Robert X. Gao and Lihui Wang and Moneer Helu and Roberto Teti","title":"Big data analytics for smart factories of the future","keywords":"Digital manufacturing system, Information, Learning","abstract":"Continued advancement of sensors has led to an ever-increasing amount of data of various physical nature to be acquired from production lines. As rich information relevant to the machines and processes are embedded within these \u201cbig data\u201d, how to effectively and efficiently discover patterns in the big data to enhance productivity and economy has become both a challenge and an opportunity. This paper discusses essential elements of and promising solutions enabled by data science that are critical to processing data of high volume, velocity, variety, and low veracity, towards the creation of added-value in smart factories of the future.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.cirp.2020.05.002","jcs_value":null,"scimago_value":null},{"author":"Chubing Zhang and Xinchun Wang and Annie Peng Cui and Shenghao Han","title":"Linking big data analytical intelligence to customer relationship management performance","keywords":"Big data, Data-driven culture, Competitive pressures, Mass customization, Marketing capability, CRM performance","abstract":"This study investigates the driving forces of a firm's assimilation of big data analytical intelligence (BDAI) and how the assimilation of BDAI improve customer relationship management (CRM) performance. Drawing on the resource-based view, this study argues that a firm's data-driven culture and the competitive pressure it faces in the industry motivate a firm's assimilation of BDAI. As a firm resource, BDAI enables an organization to develop superior mass-customization capability, which in turn positively influences its CRM performance. In addition, this study proposes that a firm's marketing capability can moderate the impact of BDAI assimilation on its mass-customization capability. Using survey data collected from 147 business-to-business companies, this study finds support for most of the hypotheses. The findings of this study uncover compelling insights about the dynamics involved in the process of using BDAI to improve CRM performance.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.indmarman.2020.10.012","jcs_value":null,"scimago_value":null},{"author":"Iris Hausladen and Maximilian Schosser","title":"Towards a maturity model for big data analytics in airline network planning","keywords":"Maturity model, Network planning, Big data analytics, Airlines, Case study","abstract":"The evaluation, acquisition and use of newly available big data sources has become a major strategic and organizational challenge for airline network planners. We address this challenge by developing a maturity model for big data readiness for airline network planning. The development of the maturity model is grounded in literature, expert interviews and case study research involving nine airlines. Four airline business models are represented, namely full-service carriers, low-cost airlines, scheduled charter airlines and cargo airlines. The maturity model has been well received with seven change requests in the model development phase. The revised version has been evaluated as exhaustive and useful by airline network planners. The self-assessment of airlines revealed low to medium maturity for most domains. Organizational factors show the lowest average maturity, IT architecture the highest. Full-service carriers seem to be more mature than airlines with different business models.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.jairtraman.2019.101721","jcs_value":null,"scimago_value":null},{"author":"Devarshi Shah and Jin Wang and Q. Peter He","title":"Feature engineering in big data analytics for IoT-enabled smart manufacturing \u2013 Comparison between deep learning and statistical learning","keywords":"Internet-of-Things, Smart manufacturing, Big data, Data analytics, Feature engineering, Deep learning, Statistical learning","abstract":"As IoT-enabled manufacturing is still in its infancy, there are several key research gaps that need to be addressed. These gaps include the understanding of the characteristics of the big data generated from industrial IoT sensors, the challenges they present to process data analytics, as well as the specific opportunities that the IoT big data could bring to advance manufacturing. In this paper, we use an inhouse-developed IoT-enabled manufacturing testbed to study the characteristics of the big data generated from the testbed. Since the quality of the data usually has the most impact on process modeling, data veracity is often the most challenging characteristic of big data. To address that, we explore the role of feature engineering in developing effective machine learning models for predicting key process variables. We compare complex deep learning approaches to a simple statistical learning approach, with different level or extent of feature engineering, to explore their pros and cons for potential industrial IoT-enabled manufacturing applications.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.compchemeng.2020.106970","jcs_value":null,"scimago_value":null},{"author":"Lang Huang and Chao Wu and Bing Wang","title":"Challenges, opportunities and paradigm of applying big data to production safety management: From a theoretical perspective","keywords":"Big data, Production safety management, Big-data-driven, Challenges, Opportunities","abstract":"Big data has caused the scientific community to re-examine the scientific research methodologies and has triggered a revolution in scientific thinking. As a branch of scientific research, production safety management is also exploring methods to take advantage of big data. This research aims to provide a theoretical basis for promoting the application of big data in production safety management. First, four different types of production safety management paradigms were identified, namely small-data-based, static-oriented, interpretation-based and causal-oriented paradigm, and the challenges to these paradigms in the presence of big data were introduced. Second, the opportunities of employing big data in production safety management were identified from four aspects, including better predict the future production safety phenomena, promote production safety management highlight relevance, achieve the balance between deductive and inductive approaches and promote the interdisciplinary development of production safety management. Third, the paradigm shifting trend of production safety management was concluded, and the discipline foundation of the new paradigm was considered as the integration of data science, production management and safety science. Fourth, a new big-data-driven production safety management paradigm was developed, which consists of the logical line of production safety management, the macro-meso-micro data spectrum, the key big data analytics, and the four-dimensional morphology. At last, the strengths (e.g., supporting better-informed safety description, safety inquisition, safety prediction) and future research direction (e.g., theory research focuses on safety-related data mining\/capturing\/cleansing) of the new paradigm were discussed. The research results not only can provide theoretical and practical basis for big-data-driven production safety management, but also can offer advice to managerial consideration and scholarly investigation.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.jclepro.2019.05.245","jcs_value":null,"scimago_value":null},{"author":"Mokhtar Sellami and Haithem Mezni and Mohand Said Hacid","title":"On the use of big data frameworks for big service composition","keywords":"Big data, Big service, Big service composition, Quality of big services, Fuzzy RCA, Spark","abstract":"Over the last years, big data has emerged as a new paradigm for the processing and analysis of massive volumes of data. Big data processing has been combined with service and cloud computing, leading to a new class of services called \u201cBig Services\u201d. In this new model, services can be seen as an abstract layer that hides the complexity of the processed big data. To meet users' complex and heterogeneous needs in the era of big data, service reuse is a natural and efficient means that helps orchestrating available services' operations, to provide customer on-demand big services. However different from traditional Web service composition, composing big services refers to the reuse of, not only existing high-quality services, but also high-quality data sources, while taking into account their security constraints (e.g., data provenance, threat level and data leakage). Moreover, composing heterogeneous and large-scale data-centric services faces several challenges, apart from security risks, such as the big services' high execution time and the incompatibility between providers' policies across multiple domains and clouds. Aiming to solve the above issues, we propose a scalable approach for big service composition, which considers not only the quality of reused services (QoS), but also the quality of their consumed data sources (QoD). Since the correct representation of big services requirements is the first step towards an effective composition, we first propose a quality model for big services and we quantify the data breaches using L-Severity metrics. Then to facilitate processing and mining big services' related information during composition, we exploit the strong mathematical foundation of fuzzy Relational Concept Analysis (fuzzy RCA) to build the big services' repository as a lattice family. We also used fuzzy RCA to cluster services and data sources based on various criteria, including their quality levels, their domains, and the relationships between them. Finally, we define algorithms that parse the lattice family to select and compose high-quality and secure big services in a parallel fashion. The proposed method, which is implemented on top of Spark big data framework, is compared with two existing approaches, and experimental studies proved the effectiveness of our big service composition approach in terms of QoD-aware composition, scalability, and security breaches.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.jnca.2020.102732","jcs_value":null,"scimago_value":null},{"author":"Pan Liu and Yue Long and Hai-Cao Song and Yan-Dong He","title":"Investment decision and coordination of green agri-food supply chain considering information service based on blockchain and big data","keywords":"Big data, Blockchain, Agri-food supply chain, Investment decision, Coordination","abstract":"Researches about the fusion application of Big Data and blockchain have appeared for a long time, many information service providers have launched information service business based on Big Data and blockchain (hereafter, ISBD). However, in the green agri-food area, the ISBD application does not popularized. A vital reason is that many decision makers do not know how to make an optimal investment decision and coordinate chain members after adopting ISBD. The core of this problem is to study the issue of investment decision and coordination in a green agri-food supply chain. To solve this problem, firstly, combining with the status of Chinese agricultural development, we proposed a more suitable supply chain structure in the fusion application environment of Big Data and blockchain. Then, we chose a green agri-food supply chain with one producer and one retailer as research object and revised the demand function. Afterwards, considering the changes of agri-food freshness and greenness, we built and analysed the benefit models of producer and retailer before and after using ISBD, and then a cost-sharing and revenue-sharing contract was put forward to coordinate the supply chain. Findings: 1) When the total investment cost payed by producer and retailer is in a certain range, using ISBD will help chain members gain more benefits. 2) If chain members want to gain more benefits after using ISBD, they should try their best to optimize costs by extracting valuable information. Results can offer a theoretical guidance for producer and retailer in investing in ISBD, pricing decision and supply chain coordination after applying ISBD.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.jclepro.2020.123646","jcs_value":null,"scimago_value":null},{"author":"Zehua Xiang and Minli Xu","title":"Dynamic game strategies of a two-stage remanufacturing closed-loop supply chain considering Big Data marketing, technological innovation and overconfidence","keywords":"Supply chain management, Big Data marketing, Technological innovation, Closed-loop supply chain, Overconfidence","abstract":"In the \u201cInternet+\u201d era, involving third-party Internet recycling platforms (IRPs) has revolutionized the operation models of closed-loop supply chains (CLSCs) in China. This study explores the impact of technological innovation, Big Data marketing and overconfidence on supply chain member decision-making. We propose a two-stage remanufacturing CLSC dynamic model consisting of a manufacturer, an IRP, and a supplier based on differential game theory. By comparing the optimal decisions of each member in three scenarios, we find that the IRP\u2019s overconfident behavior is beneficial to both the manufacturer and the IRP but will damage the supplier's profit. Although a suitable cost-sharing ratio can enable the manufacturer and IRP to achieve a \u201cwin\u2013win\u201d situation, an excessive level of confidence will inhibit the incentives of the cost-sharing strategy, negatively affecting the manufacturer's interests. Interestingly, a cost-sharing contract will become inefficient under certain conditions, i.e., highly efficient level of technological innovation, highly efficient Big Data marketing, and a high level of overconfidence, negatively affecting the manufacturer\u2019s interests. Additionally, technological innovation efficiency and marketing efficiency will have different effects on the IRP's recycling price. A cost-sharing contract and the IRP\u2019s overconfidence will prompt the IRP to exert more efforts on technological innovation and Big Data marketing and to significantly reduce the manufacturing costs and recycling costs for all members. Notably, although the IRP\u2019s overconfidence and cost-sharing strategies may damage the supplier\u2019s profit, the total profit of the CLSC increases.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.cie.2020.106538","jcs_value":null,"scimago_value":null},{"author":"Lingqiang Kong and Zhifeng Liu and Jianguo Wu","title":"A systematic review of big data-based urban sustainability research: State-of-the-science and future directions","keywords":"Big data, Social media data, Urban landscape sustainability, Smart city, Urban planning","abstract":"The future of humanity depends increasingly on the performance of cities. Big data provide new and powerful ways of studying and improving coupled urban environmental, social, and economic systems to achieve urban sustainability. However, the term big data has been defined variably, and its urban applications have so far been sporadic in terms of research topic and location. A comprehensive review of big data-based urban environment, society, and sustainability (UESS) research is much needed. The aim of this study was to summarize the big data-based UESS research using a systematic review approach in combination with bibliometric and thematic analyses. The results showed that the numbers of publications and citations of related articles have been increasing exponentially in recent years. The most frequently used big data in UESS research are human behavior data, and the major analytical methods are of five types: classification, clustering, regression, association rules, and social network analysis. The major research topics of big data-based UESS research include urban mobility, urban land use and planning, environmental sustainability, public health and safety, social equity, tourism, resources and energy utilization, real estate, and retail, accommodation and catering. Big data benefit UESS research by proving a people-oriented perspective, timely and real-time information, and fine-resolution spatial dynamics. In addition, several obstacles were identified to applying big data in UESS research, which are related to data quality and acquisition, data storage and management, data security and privacy, data cleaning and preprocessing, and data analysis and information mining. To move forward, future research should integrate multiple big data sources, develop and utilize new methods such as deep learning and cloud computing, and expand the application fields to focus on the interactions between human activities and urban environments. This review can contribute to understanding the current situation of big data-based UESS research, and provide a reference for studies of this topic in the future.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.jclepro.2020.123142","jcs_value":null,"scimago_value":null},{"author":"Yu Liu and Yangtao Wang and Ke Zhou and Yujuan Yang and Yifei Liu","title":"Semantic-aware data quality assessment for image big data","keywords":"Semantic-aware, Quality assessment, Image big data, IDSTH, SHR","abstract":"Data quality (DQ) assessment is essential for realizing the promise of big data by judging the value of data in advance. Relevance, an indispensable dimension of DQ, focusing on \u201cfitness for requirement\u201d, can arouse the user\u2019s interest in exploiting the data source. It has two-level evaluations: (1) the amount of data that meets the user\u2019s requirements; (2) the matching degree of these relevant data. However, there lack works of DQ assessment at dimension of relevance, especially for unstructured image data which focus on semantic similarity. When we try to evaluate semantic relevance between an image data source and a query (requirement), there are three challenges: (1) how to extract semantic information with generalization ability for all image data? (2) how to quantify relevance by fusing the quantity of relevant data and the degree of similarity comprehensively? (3) how to improve assessing efficiency of relevance in a big data scenario by design of an effective architecture? To overcome these challenges, we propose a semantic-aware data quality assessment (SDQA) architecture which includes off-line analysis and on-line assessment. In off-line analysis, for an image data source, we first transform all images into hash codes using our improved Deep Self-taught Hashing (IDSTH) algorithm which can extract semantic features with generalization ability, then construct a graph using hash codes and restricted Hamming distance, next use our designed Semantic Hash Ranking (SHR) algorithm to calculate the importance score (rank) for each node (image), which takes both the quantity of relevant images and the degree of semantic similarity into consideration, and finally rank all images in descending order of score. During on-line assessment, we first convert the user\u2019s query into hash codes using IDSTH model, then retrieve matched images to collate their importance scores, and finally help the user determine whether the image data source is fit for his requirement. The results on public dataset and real-world dataset show effectiveness, superiority and on-line efficiency of our SDQA architecture.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.future.2019.07.063","jcs_value":null,"scimago_value":null},{"author":"Mohamed Ahzam Amanullah and Riyaz Ahamed Ariyaluran Habeeb and Fariza Hanum Nasaruddin and Abdullah Gani and Ejaz Ahmed and Abdul Salam Mohamed Nainar and Nazihah Md Akim and Muhammad Imran","title":"Deep learning and big data technologies for IoT security","keywords":"Deep learning, Big data, IoT security","abstract":"Technology has become inevitable in human life, especially the growth of Internet of Things (IoT), which enables communication and interaction with various devices. However, IoT has been proven to be vulnerable to security breaches. Therefore, it is necessary to develop fool proof solutions by creating new technologies or combining existing technologies to address the security issues. Deep learning, a branch of machine learning has shown promising results in previous studies for detection of security breaches. Additionally, IoT devices generate large volumes, variety, and veracity of data. Thus, when big data technologies are incorporated, higher performance and better data handling can be achieved. Hence, we have conducted a comprehensive survey on state-of-the-art deep learning, IoT security, and big data technologies. Further, a comparative analysis and the relationship among deep learning, IoT security, and big data technologies have also been discussed. Further, we have derived a thematic taxonomy from the comparative analysis of technical studies of the three aforementioned domains. Finally, we have identified and discussed the challenges in incorporating deep learning for IoT security using big data technologies and have provided directions to future researchers on the IoT security aspects.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.comcom.2020.01.016","jcs_value":null,"scimago_value":null},{"author":"Brita Sedlmayr and Andreas Knapp and Mich\u00e9le K\u00fcmmel and Franziska Bathelt and Martin Sedlmayr","title":"Evaluation eines Zukunftsszenarios zur Nutzung von Big-Data-Anwendungen f\u00fcr die Verbesserung der Versorgung von Menschen mit seltenen Erkrankungen","keywords":"Evaluation, Akzeptanz, Nutzen, Barrieren, Befragung, Szenario, Big Data, Seltene Erkrankungen, Evaluation, Acceptance, Benefits, Barriers, Survey, Scenario, Big data, Rare diseases","abstract":"Zusammenfassung\nHintergrund\nIn Deutschland leben etwa 4 Millionen Menschen mit einer seltenen Erkrankung. Einzelne Studien belegen bereits, dass mit Hilfe von Big Data Verfahren die Diagnostik verbessert und seltene Erkrankungen effektiver erforscht werden k\u00f6nnen. Deutschlandweit existiert aber bisher kein konkretes, umfassendes Konzept f\u00fcr den Einsatz von Big Data zur Versorgung von Menschen mit seltenen Erkrankungen. Im Rahmen des BMG-gef\u00f6rderten Projekts \u201eBIDA-SE\u201c wurde ein erstes Szenario entworfen, wie Big-Data-basierte Anwendungen sinnvoll in der Versorgungspraxis von Menschen mit seltenen Erkrankungen einflie\u00dfen k\u00f6nnen.\nMethode\nZiel der vorliegenden Studie war es, das entwickelte Szenario hinsichtlich der Akzeptanz, des (klinischen) Nutzens, \u00f6konomischer Implikationen sowie Grenzen und Barrieren f\u00fcr dessen mittelfristige Umsetzung zu evaluieren. F\u00fcr die Bewertung des Szenarios wurde im Zeitraum Oktober-November 2019 eine Online-Befragung innerhalb Deutschlands mit insgesamt N\u00a0=\u00a09 \u00c4rzt*innen, N\u00a0=\u00a069 Patient*innen mit seltenen Erkrankungen\/Patientenvertreter*innen, N\u00a0=\u00a014 IT-Expert*innen und N\u00a0=\u00a021 Versorgungsforscher*innen durchgef\u00fchrt. F\u00fcr die Entwicklung des Online-Fragebogens wurden standardisierte, validierte Fragen bereits erprobter Erhebungsinstrumente eingesetzt und eigene Fragen auf Basis einer vorausgegangenen Literaturanalyse konstruiert. Die Auswertung der Befragung erfolgte prim\u00e4r deskriptiv durch eine Analyse von H\u00e4ufigkeiten, Mittelwerten und Standardabweichungen.\nErgebnisse\nDie Ergebnisse zeigen, dass das entwickelte Szenario von allen befragten Gruppen (\u00c4rzt*innen, Patient*innen\/Patientenvertreter*innen, IT-Expert*innen und Versorgungsforscher*innen) mehrheitlich Akzeptanz erf\u00e4hrt. Aus Sicht der \u00c4rzt*innen, Patient*innen\/Patientenvertreter*innen und Versorgungsforscher*innen h\u00e4tte das Szenario das Potential, die Diagnosestellung und Therapieeinleitung zu beschleunigen und die sektoren\u00fcbergreifende Behandlung zu verbessern. Investitionen in die vorgestellte Anwendung w\u00fcrden sich aus Sicht der \u00c4rzt*innen und Versorgungsforscher*innen rentieren. Zur Finanzierung des vorgestellten Szenarios m\u00fcsste jedoch eine Anpassung der Verg\u00fctungssituation erfolgen. Die von allen Gruppen benannten Grenzen und Barrieren f\u00fcr eine mittelfristige Umsetzung des Szenarios lassen sich in sieben Themenfelder mit Handlungsbedarf gruppieren: (1) Finanzierung und Investition, (2) Datenschutz und Datensicherheit, (3) Standards\/Datenquellen\/Datenqualit\u00e4t, (4) Technologieakzeptanz, (5) Integration in den Arbeitsalltag, (6) Wissen um Verf\u00fcgbarkeit sowie (7) Gewohnheiten und Pr\u00e4ferenzen\/Arztrolle.\nDiskussion\nMit der vorliegenden Studie wurde ein erstes fach\u00fcbergreifendes, praxisnahes Szenario unter Nutzung von Big-Data-basierten Anwendungen hinsichtlich Akzeptanz, Nutzen und Grenzen\/Barrieren evaluiert und analysiert, inwiefern dieses Szenario zuk\u00fcnftig im Kontext seltener Erkrankungen implementiert werden kann. Das Szenario erf\u00e4hrt von allen befragten Zielgruppen mehrheitlich eine hohe Akzeptanz und wird mehrheitlich als (klinisch) n\u00fctzlich bewertet, wenngleich noch rechtliche, organisatorische und technische Barrieren f\u00fcr dessen mittelfristige Umsetzung \u00fcberwunden werden m\u00fcssen. Die Evaluationsergebnisse tragen dazu bei, Handlungsempfehlungen abzuleiten, um eine mittelfristige Umsetzung des Szenarios zu gew\u00e4hrleiten und den Zugang zu den Zentren f\u00fcr Seltene Erkrankungen zuk\u00fcnftig zu kanalisieren.\nSchlussfolgerung\nAuf nationaler Ebene wurden zahlreiche Aktivit\u00e4ten angesto\u00dfen, um die Versorgungssituation von Menschen mit seltenen Erkrankungen zu verbessern. Das im Rahmen des Projekts \u201eBIDA-SE\u201c entwickelte Szenario erg\u00e4nzt diese Forschungsaktivit\u00e4ten und verdeutlicht, wie Big-Data-basierte Anwendungen sinnvoll in der Praxis genutzt werden k\u00f6nnen, um die Diagnosestellung und Therapie von Menschen mit seltenen Erkrankungen nachhaltig verbessern zu k\u00f6nnen.\nIntroduction\nIn Germany there are about 4 million people living with a rare disease. Studies have shown that big data applications can improve diagnosis of and research on rare diseases more effectively. However, no concrete comprehensive concept for the use of big data in the care of people with rare diseases has so far been established in Germany. As part of the project \u201cBIDA-SE\u201d, which is funded by the German Ministry of Health, a first scenario has been designed to show how big data applications can be usefully incorporated into the care of people with rare diseases.\nMethods\nThe aim of the present study was to evaluate this scenario with regard to acceptance, (clinical) benefits, economic aspects, and limitations and barriers to its implementation. To evaluate the scenario, an online survey was conducted in Germany in October\/November 2019 amongst a total of N\u00a0=\u00a09 physicians, N\u00a0=\u00a069 patients with rare diseases\/patient representatives, N\u00a0=\u00a014 IT experts and N\u00a0=\u00a021 health care researchers. The online questionnaire consisted of both standardized, validated questions taken from already tested survey instruments and additional questions which were constructed on the basis of a preceding literature analysis. The evaluation of the survey was primarily descriptive, with a calculation of frequencies, mean values and standard deviations.\nResults\nThe results of the evaluation show that the scenario has been accepted by a majority of all groups surveyed (physicians, patients\/patient representatives, IT experts and health care researchers). From the point of view of physicians, patients\/patient representatives and health care researchers, the scenario has the potential to accelerate the diagnosis and initiation of therapy and to improve cross-sectoral treatment. From the physician\u2019s and health care researcher\u2019s perspective, investments in the application presented in the scenario would be profitable. Financing the scenario would, however, require adjusting the reimbursement situation. The limitations and barriers identified by all groups for a medium-term implementation of the scenario can be grouped into seven thematic areas where action is needed: (1) financing and investment, (2) data protection and data security, (3) standards\/data sources\/data quality, (4) acceptance of technology, (5) integration into the daily work routine, (6) knowledge about availability as well as (7) habits and preferences\/physician's role.\nDiscussion\nWith the present study, a first interdisciplinary, practical scenario using big data applications was evaluated with regard to acceptance, benefits and limitations\/barriers. The scenario is widely accepted among the majority of all surveyed target groups and is considered (clinically) useful, although legal, organisational and technical barriers still need to be overcome for its medium-term implementation. The evaluation results contribute to the derivation of recommendations for action to ensure the medium-term implementation of the scenario and to channel access to the Centres for Rare Diseases in the future.\nConclusion\nMany activities have been initiated at a national level to improve the health care situation of people with rare diseases. The scenario developed in the \u201cBIDA-SE\u201d project complements these research activities and illustrates how big data applications can be usefully implemented into practice to improve the diagnosis and therapy of people with rare diseases in a sustainable way.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.zefq.2020.11.002","jcs_value":null,"scimago_value":null},{"author":"Elaheh Yadegaridehkordi and Mehrbakhsh Nilashi and Liyana Shuib and Mohd {Hairul Nizam Bin Md Nasir} and Shahla Asadi and Sarminah Samad and Nor {Fatimah Awang}","title":"The impact of big data on firm performance in hotel industry","keywords":"Firm performance, Big data, Hotel industry, Fuzzy logic, Structural equation modelling","abstract":"Big data has increasingly appeared as a frontier of opportunity in enhancing firm performance. However, it still is in early stages of introduction and many enterprises are still un-decisive in its adoption. The aim of this study is to propose a theoretical model based on integration of Human-Organization-Technology fit and Technology-Organization-Environment frameworks to identify the key factors affecting big data adoption and its consequent impact on the firm performance. The significant factors are gained from the literature and the research model is developed. Data was collected from top managers and\/or owners of SMEs hotels in Malaysia using online survey questionnaire. Structural Equation Modelling (SEM) is used to assess the developed model and Adaptive Neuro-Fuzzy Inference Systems (ANFIS) technique is used to prioritize adoption factors based on their importance levels. The results showed that relative advantage, management support, IT expertise, and external pressure are the most important factors in the technological, organizational, human, and environmental dimensions. The results further revealed that technology is the most important influential dimension. The outcomes of this study can assist the policy makers, businesses and governments to make well-informed decisions in adopting big data.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.elerap.2019.100921","jcs_value":null,"scimago_value":null},{"author":"Marcello M. Mariani and Samuel {Fosso Wamba}","title":"Exploring how consumer goods companies innovate in the digital age: The role of big data analytics companies","keywords":"Big data analytics, Forecasting, Innovation, Online review crowdsourcing, Consumer goods companies, Digital data","abstract":"The advent and development of digital technologies have brought about a proliferation of online consumer reviews (OCRs), i.e., real-time customers\u2019 evaluations of products, services, and brands. Increasingly, e-commerce platforms are using them to gain insights from customer feedback. Meanwhile, a new generation of big data analytics (BDA) companies are crowdsourcing large volumes of OCRs by means of controlled ad hoc online experiments and advanced machine learning (ML) techniques to forecast demand and determine the market potential for new products in several industries. We illustrate how this process is taking place for consumer goods companies by exploring the case of UK digital BDA company, SoundOut. Based on an in-depth qualitative analysis, we develop the consumer goods company innovation (CGCI) conceptual framework, which illustrates how digital BDA firms help consumer goods companies to test new products before they are launched on the market, and innovate. Theoretical and managerial implications are discussed.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.jbusres.2020.09.012","jcs_value":null,"scimago_value":null},{"author":"Evagelos D. Lioutas and Chrysanthi Charatsari and Giuseppe {La Rocca} and Marcello {De Rosa}","title":"Key questions on the use of big data in farming: An activity theory approach","keywords":"Big data, Smart farming, Value, Farmers, Cyber-physical-social systems, Activity theory","abstract":"Big data represent a pioneering development in the field of agriculture. By producing intuition, intelligence, and insights, these data have the potential to recast conventional process-driven agriculture, plotting the course for a smarter, data-driven farming. However, many open issues about the use of big data in agriculture remain unanswered. In this work, conceptualizing smart agricultural systems as cyber-physical-social systems, and building upon activity theory, we aim at highlighting some key questions that need to be addressed. To our view, big data constitute a tool reciprocally produced by all the actors involved in the agrifood supply chains. The constant flux of this tool and the intricate nature of the interactions among the actors who share it complicate the translation of big data into value. Moreover, farmers\u2019 limited capacity to deal with data complexity, along with their dual role as producers and users of big data, impedes the institutionalization of this tool at the farm level. Although the approach used left us with more questions than answers, we suggest that unraveling the institutional arrangements that govern value co-creation, capturing the motivations of farmers and other actors, and detailing the direct and indirect effects that big data (and the technologies used to generate them) have in farms are important preconditions for setting forth rules that facilitate the extraction and equal exchange of value from big data.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.njas.2019.04.003","jcs_value":null,"scimago_value":null},{"author":"Xiang Li and Ning Guo and Quanzheng Li","title":"Functional Neuroimaging in the New Era of Big Data","keywords":"Big data, Neuroimaging, Machine learning, Health informatics, fMRI","abstract":"The field of functional neuroimaging has substantially advanced as a big data science in the past decade, thanks to international collaborative projects and community efforts. Here we conducted a literature review on functional neuroimaging, with focus on three general challenges in big data tasks: data collection and sharing, data infrastructure construction, and data analysis methods. The review covers a wide range of literature types including perspectives, database descriptions, methodology developments, and technical details. We show how each of the challenges was proposed and addressed, and how these solutions formed the three core foundations for the functional neuroimaging as a big data science and helped to build the current data-rich and data-driven community. Furthermore, based on our review of recent literature on the upcoming challenges and opportunities toward future scientific discoveries, we envisioned that the functional neuroimaging community needs to advance from the current foundations to better data integration infrastructure, methodology development toward improved learning capability, and multi-discipline translational research framework for this new era of big data.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.gpb.2018.11.005","jcs_value":null,"scimago_value":null},{"author":"Ant\u00f3nio A.C. Vieira and Lu\u00eds Dias and Maribel Y. Santos and Guilherme A.B. Pereira and Jos\u00e9 Oliveira","title":"Bypassing Data Issues of a Supply Chain Simulation Model in a Big Data Context","keywords":"Simulation, Supply Chain, Big Data, Data issues, Industry 4.0","abstract":"Supply Chains (SCs) are complex and dynamic networks, where certain events may cause severe problems. To avoid them, simulation can be used, allowing the uncertainty of these systems to be considered. Furthermore, the data that is generated at increasingly high volumes, velocities and varieties by relevant data sources allow, on one hand, the simulation model to capture all the relevant elements. While developing such solution, due to the inherent use of simulation, several data issues were identified and bypassed, so that the incorporated elements comprise a coherent SC simulation model. Thus, the purpose of this paper is to present the main issues that were faced, and discuss how these were bypassed, while working on a SC simulation model in a Big Data context and using real industrial data from an automotive electronics SC. This paper highlights the role of simulation in this task, since it worked as a semantic validator of the data. Moreover, this paper also presents the results that can be obtained from the developed model.","year":2020,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.promfg.2020.02.033","jcs_value":null,"scimago_value":null},{"author":"Todd R. McNutt and Kevin L. Moore and Binbin Wu and Jean L. Wright","title":"Use of Big Data for Quality Assurance in Radiation Therapy","keywords":null,"abstract":"The application of big data to the quality assurance of radiation therapy is multifaceted. Big data can be used to detect anomalies and suboptimal quality metrics through both statistical means and more advanced machine learning and artificial intelligence. The application of these methods to clinical practice is discussed through examples of guideline adherence, contour integrity, treatment delivery mechanics, and treatment plan quality. The ultimate goal is to apply big data methods to direct measures of patient outcomes for care quality. The era of big data and machine learning is maturing and the implementation for quality assurance promises to improve the quality of care for patients.","year":2019,"ENTRYTYPE":"article","doi":"https:\/\/doi.org\/10.1016\/j.semradonc.2019.05.006","jcs_value":null,"scimago_value":null},{"author":"El Alaoui, Imane and Gahi, Youssef and Messoussi, Rochdi","title":"Big Data Quality Metrics for Sentiment Analysis Approaches","keywords":"Big data quality metrics, Sentiment analysis, Big data","abstract":"In a world increasingly connected, and in which information flows quickly and affects a very large number of people, sentiment analysis has seen a spectacular development over the past ten years. This is due to the fact that the explosion of social networks has allowed anyone with internet access to publicly express his opinion. Moreover, the emergence of big data has brought enormous opportunities and powerful storage and analytics tools to the field of sentiment analysis. However, big data introduces new variables and constraints that could radically affect the traditional models of sentiment analysis. Therefore, new concerns, such as big data quality, have to be addressed to get the most out of big data. To the best of our knowledge, no contributions have been published so far which address big data quality in SA throughout its different processes. In this paper, we first highlight the most important big data quality metrics to consider in any big data project. Then, we show how these metrics could be specifically considered in SA approaches and this for each phase in the big data value chain.","year":2019,"ENTRYTYPE":"inproceedings","doi":"10.1145\/3341620.3341629","jcs_value":null,"scimago_value":null},{"author":"Reda, Oumaima and Sassi, Imad and Zellou, Ahmed and Anter, Samir","title":"Towards a Data Quality Assessment in Big Data","keywords":"Data Quality, Data Quality evaluation, Big Data, Quality Models","abstract":"In recent years, as more and more data sources have become available and the volumes of data potentially accessible have increased, the assessment of data quality has taken a central role whether at the academic, professional or any other sector. Given that users are often concerned with the need to filter a large amount of data to better satisfy their requirements and needs, and that data analysis can be based on inaccurate, incomplete, ambiguous, duplicated and of poor quality, it makes everyone wonder what the results of these analyses will really be like. However, there is a very complex process involved in the identification of new, valid, potentially useful and meaningful data from a large data collection and various information systems, and is critically dependent on a number of measures to be developed to ensure data quality. To this end, the main objective of this paper is to introduce a general study on data quality related with big data, by providing what other researchers came up with on that subject. The paper will be finalized by a comparative study between the different existing data quality models.","year":2020,"ENTRYTYPE":"inproceedings","doi":"10.1145\/3419604.3419803","jcs_value":null,"scimago_value":null},{"author":"Baldassarre, Maria Teresa and Caballero, Ismael and Caivano, Danilo and Rivas Garcia, Bibiano and Piattini, Mario","title":"From Big Data to Smart Data: A Data Quality Perspective","keywords":"Data Quality, Smart Data, Big Data","abstract":"Big Data (BD) solutions are designed to better support decision-making processes in order to optimize organizational performance. These BD solutions use company\u2019s core business data, using typically large datasets. However, data that doesn\u2019t meet adequate quality levels will lead to BD solutions that will not produce useful results, and consequently may not be used to make adequate business decisions. For a long time, companies have collected and stored large amounts of data without being able to exploit the advantage of exploring it. Nowadays, and thanks to the Big Data explosion, organizations have begun to recognize the need for estimating the value of their data and, vice-versa, managing data accordingly to their value. This need of managing the Value of data has led to the concept of Smart Data. It not only involves the datasets, but also the set of technologies, tools, processes and methodologies that enable all the Values from the data to the End-users (Business, data scientist, BI\u2026). Consequently, Smart data is data actionable. We discovered that data quality is one of the most important issues when it comes to \u201csmartizing\u201d data. In this paper, we introduce a methodology to make data smarter, taking as a reference point, the quality level of the data itself.","year":2018,"ENTRYTYPE":"inproceedings","doi":"10.1145\/3281022.3281026","jcs_value":null,"scimago_value":null},{"author":"Emmanuel, Isitor and Stanier, Clare","title":"Defining Big Data","keywords":"Big Data, Big Data characteristics, Data Quality Dimensions, Data Quality","abstract":"As Big Data becomes better understood, there is a need for a comprehensive definition of Big Data to support work in fields such as data quality for Big Data. Existing definitions of Big Data define Big Data by comparison with existing, usually relational, definitions, or define Big Data in terms of data characteristics or use an approach which combines data characteristics with the Big Data environment. In this paper we examine existing definitions of Big Data and discuss the strengths and limitations of the different approaches, with particular reference to issues related to data quality in Big Data. We identify the issues presented by incomplete or inconsistent definitions. We propose an alternative definition and relate this definition to our work on quality in Big Data.","year":2016,"ENTRYTYPE":"inproceedings","doi":"10.1145\/3010089.3010090","jcs_value":null,"scimago_value":null},{"author":"Cuzzocrea, Alfredo and Sacc\\`{a}, Domenico and Ullman, Jeffrey D.","title":"Big Data: A Research Agenda","keywords":"OLAP over big data, privacy of big data, big data posting, big data","abstract":"Recently, a great deal of interest for Big Data has risen, mainly driven from a widespread number of research problems strongly related to real-life applications and systems, such as representing, modeling, processing, querying and mining massive, distributed, large-scale repositories (mostly being of unstructured nature). Inspired by this main trend, in this paper we discuss three important aspects of Big Data research, namely OLAP over Big Data, Big Data Posting, and Privacy of Big Data. We also depict future research directions, hence implicitly defining a research agenda aiming at leading future challenges in this research field.","year":2013,"ENTRYTYPE":"inproceedings","doi":"10.1145\/2513591.2527071","jcs_value":null,"scimago_value":null},{"author":"Neves, Pedro and Bernardino, Jorge","title":"Big Data Issues","keywords":"SaaS, DBMS, Big Data, PaaS, IaaS","abstract":"Big Data is a new trend regarded by both academics and business areas as an interesting concept. The paradigm includes the storage and processing of petabyte-size datasets, boosting knowledge discovery over data and providing organizations with competitive advantage over their contenders. This paper comes to provide an overview of the concept, answering questions that one has when faced with the term for the first time: What is Big Data and what are its advantages? How does it work? How is it accepted among enterprises? How to deploy a Big Data solution?","year":2015,"ENTRYTYPE":"inproceedings","doi":"10.1145\/2790755.2790785","jcs_value":null,"scimago_value":null},{"author":"Baru, Chaitan and Bhandarkar, Milind and Nambiar, Raghunath and Poess, Meikel and Rabl, Tilmann","title":"Big Data Benchmarking","keywords":"mbds","abstract":"We provide a summary of the outcomes from the Workshop on Big Data Benchmarking (WBDB2012) held on May 8-9, 2012 in San Jose, CA. The workshop discussed a number of issues related to big data benchmarking definitions and benchmark processes, and was attended by 60 invitees representing 45 different organizations from industry and academia. Attendees were selected based on their experience and expertise in one or more areas of big data, database systems, performance benchmarking, and big data applications. The participants concluded that there exists both a need and an opportunity for defining benchmarks to capture the end-to-end aspects of big data applications. The metrics for such benchmarks would need to include metrics for performance as well as price\/performance, and consider several costs including total system cost, setup cost, and energy costs. The next Workshop on Big Data Benchmarking is scheduled to be held on December 17-18, 2012 in Pune, India.","year":2012,"ENTRYTYPE":"inproceedings","doi":"10.1145\/2378356.2378368","jcs_value":null,"scimago_value":null},{"author":"Novikov, Boris and Vassilieva, Natalia and Yarygina, Anna","title":"Querying Big Data","keywords":"computer systems and technologies, query processing, query languages, big data","abstract":"The term \"Big Data\" became a buzzword and is widely used in both research and industrial worlds. Typically the concept of big data assumes a variety of different sources of information and velocity of complex analytical processing, rather than just a huge and growing volume of data. All variety, velocity, and volume create new research challenges, as nearly all techniques and tools commonly used in data processing have to be re-considered. Variety and uncertainty of big data require a mixture of exact and similarity search and grouping of complex objects based on different attributes. High-level declarative query languages are important in this context due to expressiveness and potential for optimization.In this talk we are mostly interested in an algebraic layer for complex query processing which resides between user interface (most likely, graphical) and execution engine in layered system architecture. We analyze the applicability of existing models and query languages. We describe a systematic approach to similarity handling of complex objects, simultaneous application of different similarity measures and querying paradigms, complex searching and querying, combined semi-structured and unstructured search. We introduce the adaptive abstract operations based on the concept of fuzzy set, which are needed to support uniform handling of different kinds of similarity processing. To ensure an efficient implementation, approximate algorithms with controlled quality are required to enable quality versus performance trade-off for timeliness of similarity processing. Uniform and adaptive operations enable high-level declarative definition of complex queries and provide options for optimization.","year":2012,"ENTRYTYPE":"inproceedings","doi":"10.1145\/2383276.2383278","jcs_value":null,"scimago_value":null},{"author":"Dong, Xin Luna and Srivastava, Divesh","title":"Big Data Integration","keywords":null,"abstract":"The Big Data era is upon us: data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through society. Since the value of data explodes when it can be linked and fused with other data, addressing the big data integration (BDI) challenge is critical to realizing the promise of Big Data.BDI differs from traditional data integration in many dimensions: (i) the number of data sources, even for a single domain, has grown to be in the tens of thousands, (ii) many of the data sources are very dynamic, as a huge amount of newly collected data are continuously made available, (iii) the data sources are extremely heterogeneous in their structure, with considerable variety even for substantially similar entities, and (iv) the data sources are of widely differing qualities, with significant differences in the coverage, accuracy and timeliness of data provided. This tutorial explores the progress that has been made by the data integration community on the topics of schema mapping, record linkage and data fusion in addressing these novel challenges faced by big data integration, and identifies a range of open problems for the community.","year":2013,"ENTRYTYPE":"article","doi":"10.14778\/2536222.2536253","jcs_value":null,"scimago_value":null},{"author":"Benjamins, V. Richard","title":"Big Data: From Hype to Reality?","keywords":"Analytics, Business, Big Data","abstract":"The traditional world of relational databases and enterprise data warehouses is being challenged by growth in data volumes, the rise of unstructured and semi-structured data, and the desire to extract more valuable business insights. In order to remain competitive: we are entering the world of 'BIG DATA'. Scale-out, commodity hardware-based solutions based on the map-reduce programming model for parallel processing on large hardware are emerging to address these BIG DATA requirements that have challenged traditional technologies. The focus of this talk is on the potential business value to be created in this area by describing the opportunities and risks arising from the recent emergence of BIG DATA Analytics technology for companies. The role businesses can play in BIG DATA is also under discussion, and finally Telefonica's experience is explained in applying BIG DATA technology, both internally for enhancement of its own business processes and externally, where we are applying the technology to benefit our customers directly.","year":2014,"ENTRYTYPE":"inproceedings","doi":"10.1145\/2611040.2611042","jcs_value":null,"scimago_value":null},{"author":"Gross, Thomas","title":"Big Data: Little Software?","keywords":null,"abstract":"The steps of accessing, storing, and transmitting \"Big Data\" raise many interesting problems. But big data sets also amplify any system or software inefficiencies when large data sets require processing. So the efficiency of the generate code (and the runtime system) is crucial if we want to see widespread use of applications based on big data.Adaptive software exploits platform and data properties to custom-tailor program executions to the current environment. However, modern platforms have many features that make it difficult to support adaptive software. Multi-core systems with a non-uniform memory architecture expose various asymmetries and complicate the runtime system's task of data management, yet even modest multi-processors exhibit NUMA properties. Processor features like prefetchers are difficult to model by a compiler and may influence the execution in unexpected ways. Finally, performance monitoring units are supposed to allow a (just-in-time) compiler to obtain the information needed to adapt the generated code. But current performance monitoring units are incomplete and, worse, subject to change over time. An adaptive software system needs performance data that is readily available, reliable, and stable.In this talk I will discuss our experiences in modeling modern systems and argue for portable performance monitoring units that allow higher levels of the software tool chain to rely on live performance data.","year":2013,"ENTRYTYPE":"inproceedings","doi":"10.1145\/2485732.2485757","jcs_value":null,"scimago_value":null},{"author":"Johnson, Jeffrey and Denning, Peter and Delic, Kemal A. and Sousa-Rodrigues, David","title":"Big Data: Big Data or Big Brother? That is the Question Now.","keywords":null,"abstract":"This ACM Ubiquity Symposium presented some of the current thinking about big data developments across four topical dimensions: social, technological, application, and educational. While 10 articles can hardly touch the expanse of the field, we have sought to cover the most important issues and provide useful insights for the curious reader. More than two dozen authors from academia and industry provided shared their points of view, their current focus of interest and their outlines of future research. Big digital data has changed and will change the world in many ways. It will bring some big benefits in the future, but combined with big AI and big IoT devices creates several big challenges. These must be carefully addressed and properly resolved for the future benefit of humanity.","year":2018,"ENTRYTYPE":"article","doi":"10.1145\/3158352","jcs_value":null,"scimago_value":null},{"author":"Lukesh, Susan S.","title":"Big Data","keywords":null,"abstract":null,"year":2014,"ENTRYTYPE":"article","doi":"10.1145\/2614512.2629588","jcs_value":null,"scimago_value":null},{"author":"Cron, Andrew and Nguyen, Huy L. and Parameswaran, Aditya","title":"Big Data","keywords":null,"abstract":null,"year":2012,"ENTRYTYPE":"article","doi":"10.1145\/2331042.2331045","jcs_value":null,"scimago_value":null},{"author":"CACM Staff","title":"Big Data","keywords":null,"abstract":null,"year":2017,"ENTRYTYPE":"article","doi":"10.1145\/3079064","jcs_value":null,"scimago_value":null},{"author":"Srivastava, Divesh","title":"Big Data Integration","keywords":null,"abstract":"The Big Data era is upon us: data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Since the value of data explodes when it can be linked and fused with other data, addressing the big data integration (BDI) challenge is critical to realizing the promise of Big Data. BDI differs from traditional data integration in many dimensions: (i) the number of data sources, even for a single domain, has grown to be in the tens of thousands, (ii) many of the data sources are very dynamic, as a huge amount of newly collected data are continuously made available, (iii) the data sources are extremely heterogeneous in their structure, with considerable variety even for substantially similar entities, and (iv) the data sources are of widely differing qualities, with significant differences in the coverage, accuracy and timeliness of data provided. This talk explores the progress that has been made by the data integration community in addressing these novel challenges faced by big data integration, and identifies a range of open problems for the community.","year":2013,"ENTRYTYPE":"inproceedings","doi":null,"jcs_value":null,"scimago_value":null},{"author":"Ramachandra, Karthik and Sudarshan, S.","title":"Big Data: From Querying to Transaction Processing","keywords":null,"abstract":"The term Big Data has been used and abused extensively in the past few years, and means different things to different people. A commonly used notion says Big Data is about \"volume\" (of data), \"velocity\" (rate at which data is inserted\/updated) and \"variety\" (of data types). In this tutorial, we use the term Big Data to refer to any data processing need that requires a high degree of parallelism. In other words, we focus primarily on the \"volume\" and \"velocity\" aspects.As part of this tutorial, we will cover some aspects of Big Data management, in particular scalable storage, scalable query processing, and scalable transaction processing.This is an introductory tutorial for those who are not familiar with the areas that we will be covering. The focus will be conceptual; it is not meant as a tutorial on how to use any specific system.","year":2013,"ENTRYTYPE":"inproceedings","doi":null,"jcs_value":null,"scimago_value":null},{"author":"Masabo, Emmanuel and Kaawaase, Kyanda Swaib and Sansa-Otim, Julianne","title":"Big Data: Deep Learning for Detecting Malware","keywords":"machine learning, malware detection, big data analytics, deep learning","abstract":"Malicious software, commonly known as malware are constantly getting smarter with the capabilities of undergoing self-modifications. They are produced in big numbers and widely deployed very fast through the Internet-capable devices. This is therefore a big data problem and remains challenging in the research community. Existing detection methods should be enhanced in order to effectively deal with today's malware. In this paper, we propose a novel real-time monitoring, analysis and detection approach that is achieved by applying big data analytics and machine learning in the development of a general detection model. The learnings achieved through big data render machine learning more efficient. Using the deep learning approach, we designed and developed a scalable detection model that brings improvement to the existing solutions. Our experiments achieved an accuracy of 97% and ROC of 0.99.","year":2018,"ENTRYTYPE":"inproceedings","doi":"10.1145\/3195528.3195533","jcs_value":null,"scimago_value":null},{"author":"Davoudian, Ali and Liu, Mengchi","title":"Big Data Systems: A Software Engineering Perspective","keywords":"Big Data systems, Big Data, requirements engineering, software reference architecture, quality assurance, software engineering","abstract":"Big Data Systems (BDSs) are an emerging class of scalable software technologies whereby massive amounts of heterogeneous data are gathered from multiple sources, managed, analyzed (in batch, stream or hybrid fashion), and served to end-users and external applications. Such systems pose specific challenges in all phases of software development lifecycle and might become very complex by evolving data, technologies, and target value over time. Consequently, many organizations and enterprises have found it difficult to adopt BDSs. In this article, we provide insight into three major activities of software engineering in the context of BDSs as well as the choices made to tackle them regarding state-of-the-art research and industry efforts. These activities include the engineering of requirements, designing and constructing software to meet the specified requirements, and software\/data quality assurance. We also disclose some open challenges of developing effective BDSs, which need attention from both researchers and practitioners.","year":2020,"ENTRYTYPE":"article","doi":"10.1145\/3408314","jcs_value":null,"scimago_value":null},{"author":"Mani, Murali and Fei, Si","title":"Effective Big Data Visualization","keywords":"Big Data, Data Visualization, Data Analytics","abstract":"In the last several years, big data analytics has found an increasing role in our everyday lives. Data visualization has long been accepted as an integral part of data analytics. However, data visualization systems are not equipped to handle the complexities typically found in big data. Our work examines effective ways of visualizing big data, while also realizing that most visualization processes are interactive. During an interactive visualization session, an analyst issues several visualization requests, each of which builds on prior visualizations. In our approach, we integrate a distributed data processing system that can effectively process big data with a visualization system that can provide effective interactive visualization but for smaller amounts of data. The analyst's current request is used to infer contextual information about the analyst such as their expertise and tolerance for delay. This information is used to carefully determine additional data that can be sent to the visualization system for decreasing the response time for future requests, thus providing a better experience for the analyst and increasing their productivity.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1145\/3105831.3105857","jcs_value":null,"scimago_value":null},{"author":"Fan, Wenfei","title":"Data Quality: From Theory to Practice","keywords":null,"abstract":"Data quantity and data quality, like two sides of a coin, are equally important to data management. This paper provides an overview of recent advances in the study of data quality, from theory to practice. We also address challenges introduced by big data to data quality management.","year":2015,"ENTRYTYPE":"article","doi":"10.1145\/2854006.2854008","jcs_value":null,"scimago_value":null},{"author":"Pflugfelder, Ehren Helmut","title":"Big Data, Big Questions","keywords":null,"abstract":"One significant concern I have for the future of technical communication, a concern I often share with my students, involves the impact of \"big data.\" Though the term is frequently used with a sneer, or at least a slightly unsettled laugh, the methods for retrieving information from large data sets are improving as I write this. One significant question the field faces is: \"what new relationships will develop and what new work will technical communicators be responsible for in emergent big data projects, in coming years?\"","year":2013,"ENTRYTYPE":"article","doi":"10.1145\/2524248.2524253","jcs_value":null,"scimago_value":null},{"author":"Castelluccia, Daniela and Caldarola, Enrico G. and Boffoli, Nicola","title":"Environmental Big Data: A Systematic Mapping Study","keywords":"Data Management, Systematic Mapping, Big Data, Environment, Data Integration","abstract":"Big data sets and analytics are increasingly being used by government agencies, non-governmental organizations, and privatecompanies to forward environmental protection. Improving energy efficiency, promoting environmental justice, tracking climate change, and monitoring water quality are just a few of the objectives being furthered by the use of Big Data. The authors provide a more detailed analysis of the emerging evidence-based insights on Environmental Big Data (EBD), by applying the well-defined method of systematic mapping. The analysis of results throws light on the current open issues of Environmental Big Data. Moreover, different facets of the study can be combined nto answer more specific research questions. The report reveals the need for more empirical research able to provide new metrics measuring efficiency and effectiveness of the proposed analytics and new methods and tools supporting data processing workflow in EBD","year":2017,"ENTRYTYPE":"article","doi":"10.1145\/3011286.3011307","jcs_value":null,"scimago_value":null},{"author":"Abell\\'{o}, Alberto","title":"Big Data Design","keywords":"nosql, database design, big data","abstract":"It is widely accepted today that Relational databases are not appropriate in highly distributed shared-nothing architectures of commodity hardware, that need to handle poorly structured heterogeneous data. This has brought the blooming of NoSQL systems with the purpose of mitigating such problem, specially in the presence of analytical workloads. Thus, the change in the data model and the new analytical needs beyond OLAP take us to rethink methods and models to design and manage these newborn repositories. In this paper, we will analyze state of the art and future research directions.","year":2015,"ENTRYTYPE":"inproceedings","doi":"10.1145\/2811222.2811235","jcs_value":null,"scimago_value":null},{"author":"Zhou, Ming and Cao, Menglin and Park, Taeho and Pyeon, Jae-Ho","title":"Clarifying Big Data: The Concept and Its Applications","keywords":null,"abstract":"This paper clarifies the concept of Big Data with a discussion of its managerial implications and presents its defining characteristics differentiating Big Data with traditional analytics. This paper also introduces the concept of Big Data in the context of three industries, namely, finance, supply chain and marketing and discusses how this concept can be applied in the business world. With regard to this concept, fundamental yet critical discussions were made for any further understanding of Big Data. Finally, this paper contributes to our current knowledge of Big Data by relating and contrasting Big Data to traditional analysis while presenting context specific discussions for its applications. Although technical aspects of Big Data were not covered in this paper, this paper focused on serving as a business discussion for the concept of Big Data. For future work, business contents must be related to technical capabilities and solutions in order to provide a better understanding of Big Data.","year":2015,"ENTRYTYPE":"inproceedings","doi":"10.1145\/2837060.2837068","jcs_value":null,"scimago_value":null},{"author":"Mockus, Audris","title":"Engineering Big Data Solutions","keywords":"Data Engineering, Operational Data, Game Theory, Statistics, Data Quality, Analytics, Data Science","abstract":"Structured and unstructured data in operational support tools have long been prevalent in software engineering. Similar data is now becoming widely available in other domains. Software systems that utilize such operational data (OD) to help with software design and maintenance activities are increasingly being built despite the difficulties of drawing valid conclusions from disparate and low-quality data and the continuing evolution of operational support tools. This paper proposes systematizing approaches to the engineering of OD-based systems. To prioritize and structure research areas we consider historic developments, such as big data hype; synthesize defining features of OD, such as confounded measures and unobserved context; and discuss emerging new applications, such as diverse and large OD collections and extremely short development intervals. To sustain the credibility of OD-based systems more research will be needed to investigate effective existing approaches and to synthesize novel, OD-specific engineering principles.","year":2014,"ENTRYTYPE":"inproceedings","doi":"10.1145\/2593882.2593889","jcs_value":null,"scimago_value":null},{"author":"Statchuk, Craig and Iles, Michael and Thomas, Fenny","title":"Big Data and Analytics","keywords":null,"abstract":"Business Analytics is maturing and moving towards mass adoption. The emergence of big data increases the need for innovative tools and methodologies. Of particular interest is the established Business Intelligence market segment, built on structured data and reporting. How does big data affect methodologies like ETL, modeling and report authoring? Business Intelligence is at a crossroads between less formal data analysis at scale and business imperatives like regulatory reporting that runs an enterprise. This paper highlights new technologies and services that move the methodologies of old into the data-centric world of high volume and velocity that defines the modern information landscape.","year":2013,"ENTRYTYPE":"inproceedings","doi":null,"jcs_value":null,"scimago_value":null},{"author":"Hepworth, Katherine","title":"Big Data Visualization: Promises &amp; Pitfalls","keywords":null,"abstract":"A few weeks ago, I was having dinner with a friend when a controversial subject came up. My friend had an extremely strong opinion about the harm caused by vaccination, and his argument went something like this: \"I've seen the data. There was an infographic laying it all out.\" He couldn't remember specific numbers from the visualization he'd seen or the author of the article. He couldn't even remember the name of the publication, but the data visualization's overall argument was firmly lodged in his mind. His situation is not unique, and it provides telling insights on how we, as humans, perceive and respond to big data visualization.","year":2017,"ENTRYTYPE":"article","doi":"10.1145\/3071088.3071090","jcs_value":null,"scimago_value":null},{"author":"Broder, Andrei and Adamic, Lada and Franklin, Michael and Rijke, Maarten de and Xing, Eric and Yu, Kai","title":"Big Data: New Paradigm or \"Sound and Fury, Signifying Nothing\"?","keywords":"big data","abstract":"The Gartner's 2014 Hype Cycle released last August moves Big Data technology from the Peak of Inflated Expectations to the beginning of the Trough of Disillusionment when interest starts to wane as reality does not live up to previous promises. As the hype is starting to dissipate it is worth asking what Big Data (however defined) means from a scientific perspective: Did the emergence of gigantic corpora exposed the limits of classical information retrieval and data mining and led to new concepts and challenges, the way say, the study of electromagnetism showed the limits of Newtonian mechanics and led to Relativity Theory, or is it all just \"sound and fury, signifying nothing\", simply a matter of scaling up well understood technologies? To answer this question, we have assembled a distinguished panel of eminent scientists, from both Industry and Academia: Lada Adamic (Facebook), Michael Franklin (University of California at Berkeley), Maarten de Rijke (University of Amsterdam), Eric Xing (Carnegie Mellon University), and Kai Yu (Baidu) will share their point of view and take questions from the moderator and the audience.","year":2015,"ENTRYTYPE":"inproceedings","doi":"10.1145\/2684822.2697027","jcs_value":null,"scimago_value":null},{"author":"Giles, C. Lee","title":"Scholarly Big Data: Information Extraction and Data Mining","keywords":"digital libraries, data mining, information retrieval, big data, information extraction, entity resolution","abstract":"Collections of scholarly documents are usually not thought of as big data. However, large collections of scholarly documents often have many millions of publications, authors, citations, equations, figures, etc., and large scale related data and structures such as social networks, slides, data sets, etc. We discuss scholarly big data challenges, insights, methodologies and applications. We illustrate scholarly big data issues with examples of specialized search engines and recommendation systems that use information extraction and data mining in various areas such as computer science, chemistry, archaeology, acknowledgements, reference recommendation, collaboration recommendation, and others.","year":2013,"ENTRYTYPE":"inproceedings","doi":"10.1145\/2505515.2527109","jcs_value":null,"scimago_value":null},{"author":"Menon, Aravind","title":"Big Data @ Facebook","keywords":"mbds","abstract":"The Facebook Data Infrastructure supports a wide range of applications, including both external facing products and services and internal applications. This paper focuses on the Data Warehousing and Analytics platform of Facebook that provides support for batch-oriented analytics applications. Facebook's data infrastructure is built largely on top of open-source technologies such as Apache Hadoop, HDFS, MapReduce and Hive, and provides a rich set of tools for different users to perform analytics queries on Facebook data. As the Facebook user base continues to grow, we continue to enhance our data platform in order to deal with the challenges of scaling with increasing amounts of data.","year":2012,"ENTRYTYPE":"inproceedings","doi":"10.1145\/2378356.2378364","jcs_value":null,"scimago_value":null},{"author":"Kechadi, M-Tahar","title":"Healthcare Big Data: Challenges and Opportunities","keywords":"Big data, Ecosystems, Data Mining, Sensor Data, System Design, Healthcare Data, Data Analytics","abstract":"In healthcare sector huge quantities of data about patients and their medical conditions have been gathered through clinical databases and various other healthcare processes. Currently, it records nearly all aspects of care, including patient personal information, clinical trials, hospital records, diagnosis, medication, test results, imaging data, costs, administrative reports, etc. Like in other application domains, the big data revolution holds also great promise in the area of healthcare, as the available data about individual patients is very rich, and hides crucial knowledge that can be exploited to improve patients' care while reducing its cost. For instance, in 2012 worldwide collected healthcare data was estimated to be in the region of 500 petabytes and it is expected to grow 50 times more in 2020 (25 Exabytes). Turning this massive amount of data into knowledge that can be used to identify needs, predict and prevent critical patients' conditions, and help practitioners to make rapid and accurate decisions is not only a desire but is of urgent and crucial necessity. Therefore, healthcare organisations must have the ability to manage and analyse their data in a rapid and efficient manner to answer several critical questions related to diseases, treatments, patients' behaviours, and care management. However, building such system faces huge challenges: 1) data complexity, 2) Privacy, security, ethical, legal, and social issues, and 3) Interoperability, portability, and compatibility. We will discuss all these challenges and the requirements of healthcare ecosystem. This will lead us to describe some innovative methodologies of how to build such ecosystem to face the healthcare challenges of the next decade or so.","year":2016,"ENTRYTYPE":"inproceedings","doi":"10.1145\/3010089.3010143","jcs_value":null,"scimago_value":null},{"author":"Ianni, Michele and Masciari, Elio and Mazzeo, Giuseppe M. and Zaniolo, Carlo","title":"Efficient Big Data Clustering","keywords":"Spark, Clustering, Big Data","abstract":"The need to support advanced analytics on Big Data is driving data scientist' interest toward massively parallel distributed systems and software platforms, such as Map-Reduce and Spark, that make possible their scalable utilization. However, when complex data mining algorithms are required, their fully scalable deployment on such platforms faces a number of technical challenges that grow with the complexity of the algorithms involved. Thus algorithms, that were originally designed for a sequential nature, must often be redesigned in order to effectively use the distributed computational resources. In this paper, we explore these problems, and then propose a solution which has proven to be very effective on the complex hierarchical clustering algorithm CLUBS+. By using four stages of successive refinements, CLUBS+ delivers high-quality clusters of data grouped around their centroids, working in a totally unsupervised fashion. Experimental results confirm the accuracy and scalability of CLUBS+.","year":2018,"ENTRYTYPE":"inproceedings","doi":"10.1145\/3216122.3216154","jcs_value":null,"scimago_value":null},{"author":"Pouyanfar, Samira and Yang, Yimin and Chen, Shu-Ching and Shyu, Mei-Ling and Iyengar, S. S.","title":"Multimedia Big Data Analytics: A Survey","keywords":"Big data analytics, 5V challenges, multimedia databases, multimedia analysis, machine learning, survey, retrieval, mobile multimedia, indexing, data mining","abstract":"With the proliferation of online services and mobile technologies, the world has stepped into a multimedia big data era. A vast amount of research work has been done in the multimedia area, targeting different aspects of big data analytics, such as the capture, storage, indexing, mining, and retrieval of multimedia big data. However, very few research work provides a complete survey of the whole pine-line of the multimedia big data analytics, including the management and analysis of the large amount of data, the challenges and opportunities, and the promising research directions. To serve this purpose, we present this survey, which conducts a comprehensive overview of the state-of-the-art research work on multimedia big data analytics. It also aims to bridge the gap between multimedia challenges and big data solutions by providing the current big data frameworks, their applications in multimedia analyses, the strengths and limitations of the existing methods, and the potential future directions in multimedia big data analytics. To the best of our knowledge, this is the first survey that targets the most recent multimedia management techniques for very large-scale data and also provides the research studies and technologies advancing the multimedia analyses in this big data era.","year":2018,"ENTRYTYPE":"article","doi":"10.1145\/3150226","jcs_value":null,"scimago_value":null},{"author":"Miao, Xin","title":"Big Data and Smart Grid","keywords":"Smart Grid, privacy protection, data exploration, architecture, safety, Big Data, power consumption","abstract":"Big Data brings the challenge for Smart Grid. By using the method of SWOT, the double-edged sword effect of Big Data for the Smart Grid has been analyzed. Big Data provides both opportunities and challenges. The benefits and opportunities are which, Big Data bringing data view, changing thinking methods and tools, expanding the application scene, providing better service to the society, enhancing the value of the opportunity. At the same time, Big Data will lead to the challenges in Smart Grid, for example, because of security challenges of Big Data itself, Big Data more concentrated, cause safety challenges in Smart Grid is more serious; the energy consumption challenges of Big Data; Big Data privacy threat Smart Grid. From the viewpoint of information theory of Shannon, the following conclusions can be reached: the electric power consumption is positively correlated closely with the volume of Big Data; the energy consumption of data grows exponentially.","year":2014,"ENTRYTYPE":"inproceedings","doi":"10.1145\/2640087.2644175","jcs_value":null,"scimago_value":null},{"author":"Mohammad, Atif and Mcheick, Hamid and Grant, Emanuel","title":"Big Data Architecture Evolution: 2014 and Beyond","keywords":"cloud computing, big data","abstract":"This paper aims at developing the Big Data Architecture, and its relation with Analytics, Cloud Services as well as Business Intelligence. The chief aim from all mentioned is to enable the Enterprise Architecture and the Vision of an Organizational target to utilize all the data they are ingesting and regressing data for their short-term or long-terms analytical needs, while making sure that they are addressing during the design phase of such data architecture for both directly and indirectly related stakeholder. Since all stakeholders have their relative interests to utilize the transformed data-sets. This paper also identifies most of the Big Data Architecture, threat analysis within a Big Data System and Big Data Analytic Roadmaps, in terms of smaller components by conducting a gap-analysis that has significant importance as Baseline Big Data Architecture, targeting the end resultant Architectures, once the distillation process of main Big Data Architecture is completed by the Data Architects.","year":2014,"ENTRYTYPE":"inproceedings","doi":"10.1145\/2656346.2656358","jcs_value":null,"scimago_value":null},{"author":"Thuraisingham, Bhavani","title":"Big Data Security and Privacy","keywords":"privacy, security, big data","abstract":"This paper describes the issues surrounding big data security and privacy and provides a summary of the National Science Foundation sponsored workshop on this topic held in Dallas, Texas on September 16-17, 2014. Our goal is to build a community in big data security and privacy to explore the challenging research problems.","year":2015,"ENTRYTYPE":"inproceedings","doi":"10.1145\/2699026.2699136","jcs_value":null,"scimago_value":null},{"author":"Sultan, Kashif and Ali, Hazrat","title":"Where Big Data Meets 5G?","keywords":"mobile communication, 5G networks, data analtyics, big data","abstract":"Due to massive increase in data collection from wireless devices, wireless sensor networks, and network operators, data processing has become a challenge. The massive data can broadly be categorized into raw data and right data. Future generation network (5G) can be optimized if right data is extracted efficiently from such a massive raw data. Such a solution is provided through big data analytics. In this article, we discuss big data analytics solution for 5G network. We also outline existing big data architectures proposed for the network optimization. We propose a generalized flow structure for big data based analytics in 5G. Finally, we summarize our article by highlighting some challenges for big data analytics in 5G.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1145\/3018896.3025151","jcs_value":null,"scimago_value":null},{"author":"Miloslavskaya, Natalia and Senatorov, Mikhail and Tolstoy, Alexander and Zapechnikov, Sergey","title":"Big Data Information Security Maintenance","keywords":"Secure Infrastructure, Big Data, Information Security","abstract":"The need to protect big data, particularly those relating to information security maintenance (ISM) of an enterprise's IT infrastructure (ITI), and their processing is shown. Related worldwide experience of addressing big data ISM issues is summarized. An attempt to formulate a big data ISM problem statement is undertaken. An infrastructure for big data ISM is proposed. The importance of big data visualization is discussed.","year":2014,"ENTRYTYPE":"inproceedings","doi":"10.1145\/2659651.2659655","jcs_value":null,"scimago_value":null},{"author":"Endert, Alex and Szymczak, Samantha and Gunning, Dave and Gersh, John","title":"Modeling in Big Data Environments","keywords":null,"abstract":"Human-Centered Big Data Research (HCBDR) is an area of work that focuses on the methodologies and research areas focused on understanding how humans interact with \"big data\". In the context of this paper, we refer to \"big data\" in a holistic sense, including most (if not all) the dimensions defining the term, such as complexity, variety, velocity, veracity, etc. Simply put, big data requires us as researchers of to question and reconsider existing approaches, with the opportunity to illuminate new kinds of insights that were traditionally out of reach to humans.","year":2014,"ENTRYTYPE":"inproceedings","doi":"10.1145\/2609876.2609890","jcs_value":null,"scimago_value":null},{"author":"Cuzzocrea, Alfredo and Damiani, Ernesto","title":"Pedigree-Ing Your Big Data: Data-Driven Big Data Privacy in Distributed Environments","keywords":null,"abstract":"This paper introduces a general framework for supporting data-driven privacy-preserving big data management in distributed environments, such as emerging Cloud settings. The proposed framework can be viewed as an alternative to classical approaches where the privacy of big data is ensured via security-inspired protocols that check several (protocol) layers in order to achieve the desired privacy. Unfortunately, this injects considerable computational overheads in the overall process, thus introducing relevant challenges to be considered. Our approach instead tries to recognize the \"pedigree\" of suitable summary data representatives computed on top of the target big data repositories, hence avoiding computational overheads due to protocol checking. We also provide a relevant realization of the framework above, the so-called Data-dRIven aggregate-PROvenance privacy-preserving big Multidimensional data (DRIPROM) framework, which specifically considers multidimensional data as the case of interest.","year":2018,"ENTRYTYPE":"inproceedings","doi":"10.1109\/CCGRID.2018.00100","jcs_value":null,"scimago_value":null},{"author":"Argenta, Chris and Benson, Jordan and Bos, Nathan and Paletz, Susannah B. F. and Pike, William and Wilson, Aaron","title":"Sensemaking in Big Data Environments","keywords":null,"abstract":"We report on the sensemaking breakout group at the Human Centered Big Data Research (HCBDR-2014) workshop. The authors are a multi-disciplinary team of invited researchers and stakeholders who participated in this breakout session. This report includes an overview of our discussions on the many research challenges associated with sensemaking within a big data environment. Specifically, we focused on key topics that fit squarely in the intersection of the sensemaking and big data research, as other communities already exist for decision making and big data technologies independently. As part of this effort, our group developed and proposed a framework around which this community can target and structure future research. This framework is intended to allow the community to systematically identify areas where innovative research might make large contributions to sensemaking in a big data environment.","year":2014,"ENTRYTYPE":"inproceedings","doi":"10.1145\/2609876.2609889","jcs_value":null,"scimago_value":null},{"author":"Gopalkrishnan, Vivekanand and Steier, David and Lewis, Harvey and Guszcza, James","title":"Big Data, Big Business: Bridging the Gap","keywords":null,"abstract":"Business analytics, occupying the intersection of the worlds of management science, computer science and statistical science, is a potent force for innovation in both the private and public sectors. The successes of business analytics in strategy, process optimization and competitive advantage has led to data being increasingly recognized as a valuable asset in many organizations. In recent years, thanks to a dramatic increase in the volume, variety and velocity of data, the loosely defined concept of \"Big Data\" has emerged as a topic of discussion in its own right -- with different viewpoints in both the business and technical worlds. From our perspective, it is important for discussions of \"Big Data\" to start from a well-defined business goal, and remain moored to fundamental principles of both cost\/benefit analysis as well as core statistical science. This note discusses some business case considerations for analytics projects involving \"Big Data\", and proposes key questions that businesses should ask. With practical lessons from Big Data deployments in business, we also pose a number of research challenges that may be addressed to enable the business analytics community bring best data analytic practices when confronted with massive data sets.","year":2012,"ENTRYTYPE":"inproceedings","doi":"10.1145\/2351316.2351318","jcs_value":null,"scimago_value":null},{"author":"Villanustre, Flavio","title":"Industrial Big Data Analytics: Lessons from the Trenches","keywords":"distributed algorithms, declarative programming, dataflow programming, big data, abstraction models","abstract":"Big Data Analytics in particular and Data Science in general have become key disciplines in the last decade. The convergence of Information Technology, Statistics and Mathematics, to explore and extract information from Big Data have challenged the way many industries used to operate, shifting the decision making process in many organizations. A new breed of Big Data platforms has appeared, to fulfill the needs to process data that is large, complex, variable and rapidly generated. The author describes the experience in this field from a company that provides Big Data analytics as its core business.","year":2015,"ENTRYTYPE":"inproceedings","doi":null,"jcs_value":null,"scimago_value":null},{"author":"Villa, Adam H.","title":"Big Data: Motivating the Development of an Advanced Database Systems Course","keywords":null,"abstract":"The creation of massive data sets, commonly referred to as Big Data, has motivated the development of new database systems and techniques for managing, monitoring, querying, and analyzing data [5]. As data sizes grow, so do the technologies developed to the meet this new demand, which in turn generates new employment opportunities for graduating students. Preparing students for these new positions requires the integration of these new techniques and methodologies into the curriculum. The growth of Big Data is motivating the development of advanced database courses. This paper presents an approach to creating such a course using flexible modules.","year":2016,"ENTRYTYPE":"article","doi":null,"jcs_value":null,"scimago_value":null},{"author":"Kantarcioglu, Murat","title":"Securing Big Data: New Access Control Challenges and Approaches","keywords":"encrypted data processing, nosql databases, intrusion detection, access control, privacy, security","abstract":"Recent cyber attacks have shown that the leakage\/stealing of big data may result in enormous monetary loss and damage to organizational reputation, and increased identity theft risks for individuals. Furthermore, in the age of big data, protecting the security and privacy of stored data is paramount for maintaining public trust, and getting the full value from the collected data. In this talk, we first discuss the unique security and privacy challenges arise due to big data and the NoSQL systems designed to analyze big data. Also we discuss our proposed SecureDL system that is built on top of existing NoSQL databases such as Hadoop and Spark and designed as a data access broker where each request submitted by a user app is automatically captured. These captured requests are logged, analyzed and then modified (if needed) to conform with security and privacy policies (e.g.,[5]), and submitted to underlying NoSQL database. Furthermore, SecureDL can allow organizations to audit their big data usage to prevent data misuse and comply with various privacy regulations[2]. SecureDL is totally transparent from the user point of view and does not require any change to the user's code and\/or the underlying NoSQL database systems. Therefore, it can be deployed on existing NoSQL databases.Later on, we discuss how to add additional security layer for protecting big data using encryption techniques (e.g., [1, 3, 4]). Especially, we discuss our work on leveraging the modern hardware based trusted execution environments (TEEs) such as Intel SGX for secure encrypted data processing. We also discuss how to provide a simple, secure and high level language based framework that is suitable for enabling generic data analytics for non-security experts who do not have security concepts such as \"oblivious execution''. Our proposed framework allows data scientists to perform the data analytic tasks with TEEs using a Python\/Matlab like high level language; and automatically compiles programs written in our language to optimal execution code by managing issues such as optimal data block sizes for I\/O, vectorized computations to simplify much of the data processing, and optimal ordering of operations for certain tasks. Using these design choices, we show how to provide guarantees for efficient and secure big data analytics over encrypted data.","year":2019,"ENTRYTYPE":"inproceedings","doi":"10.1145\/3322431.3326330","jcs_value":null,"scimago_value":null},{"author":"Tahsin, Anika and Hasan, Md. Manzurul","title":"Big Data &amp; Data Science: A Descriptive Research on Big Data Evolution and a Proposed Combined Platform by Integrating R and Python on Hadoop for Big Data Analytics and Visualization","keywords":"Data Science, R, Python, Big Data, Hadoop","abstract":"In this technological era, Big Data is a new glorified term in where Data Science is the secret sauce of it. Undoubtedly, the digitalization of data is not the whole story; it is just a beginning of Data Science area of study. There was a time when the main focus was on building framework and processing of this data. After Hadoop HDFS and MapReduce resolved this issue already typically the concentration will follow to the next level. In terms of this, Big Data on Data Science becoming the most hyped solving area. At the moment of zettabytes data, R, Python, Hadoop all are in progressing phase in where integration among individual framework and tools will be highlighted and newest data handling tools are integrating with latest technology in terms of analytics competence. There will be a positivity when this integration will expose a new horizon for researchers and develop the preeminent solution based on the challenges.","year":2020,"ENTRYTYPE":"inproceedings","doi":"10.1145\/3377049.3377051","jcs_value":null,"scimago_value":null},{"author":"De Francisci Morales, Gianmarco and Bifet, Albert and Khan, Latifur and Gama, Joao and Fan, Wei","title":"IoT Big Data Stream Mining","keywords":"IoT, data streams, big data, data science","abstract":"The challenge of deriving insights from the Internet of Things (IoT) has been recognized as one of the most exciting and key opportunities for both academia and industry. Advanced analysis of big data streams from sensors and devices is bound to become a key area of data mining research as the number of applications requiring such processing increases. Dealing with the evolution over time of such data streams, i.e., with concepts that drift or change completely, is one of the core issues in IoT stream mining. This tutorial is a gentle introduction to mining IoT big data streams. The first part introduces data stream learners for classification, regression, clustering, and frequent pattern mining. The second part deals with scalability issues inherent in IoT applications, and discusses how to mine data streams on distributed engines such as Spark, Flink, Storm, and Samza.","year":2016,"ENTRYTYPE":"inproceedings","doi":"10.1145\/2939672.2945385","jcs_value":null,"scimago_value":null},{"author":"Yang, Tianbao and Lin, Qihang and Jin, Rong","title":"Big Data Analytics: Optimization and Randomization","keywords":"optimization, randomized approximation, machine learning, randomized reduction","abstract":"As the scale and dimensionality of data continue to grow in many applications of data analytics (e.g., bioinformatics, finance, computer vision, medical informatics), it becomes critical to develop efficient and effective algorithms to solve numerous machine learning and data mining problems. This tutorial will focus on simple yet practically effective techniques and algorithms for big data analytics. In the first part, we plan to present the state-of-the-art large-scale optimization algorithms, including various stochastic gradient descent methods, stochastic coordinate descent methods and distributed optimization algorithms, for solving various machine learning problems. In the second part, we will focus on randomized approximation algorithms for learning from large-scale data. We will discuss i) randomized algorithms for low-rank matrix approximation; ii) approximation techniques for solving kernel learning problems; iii) randomized reduction methods for addressing the high-dimensional challenge. Along with the description of algorithms, we will also present some empirical results to facilitate understanding of different algorithms and comparison between them.","year":2015,"ENTRYTYPE":"inproceedings","doi":"10.1145\/2783258.2789989","jcs_value":null,"scimago_value":null},{"author":"Heer, Jeffrey and Kandel, Sean","title":"Interactive Analysis of Big Data","keywords":null,"abstract":"New user interfaces can transform how we work with big data, and raise exciting research problems that span human-computer interaction, machine learning, and distributed systems.","year":2012,"ENTRYTYPE":"article","doi":"10.1145\/2331042.2331058","jcs_value":null,"scimago_value":null},{"author":"Liu, He and Huang, Fupeng and Li, Han and Liu, Weiwei and Wang, Tongxun","title":"A Big Data Framework for Electric Power Data Quality Assessment","keywords":"Big Data;Data integrity;Power grids;History;Real-time systems;Sensors;data quality;electric power data;data quality assessment;big data;framework","abstract":"Since a low-quality data may influence the effectiveness and reliability of applications, data quality is required to be guaranteed. Data quality assessment is considered as the foundation of the promotion of data quality, so it is essential to access the data quality before any other data related activities. In the electric power industry, more and more electric power data is continuously accumulated, and many electric power applications have been developed based on these data. In China, the power grid has many special characteristic, traditional big data assessment frameworks cannot be directly applied. Therefore, a big data framework for electric power data quality assessment is proposed. Based on big data techniques, the framework can accumulate both the real-time data and the history data, provide an integrated computation environment for electric power big data assessment, and support the storage of different types of data.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1109\/WISA.2017.29","jcs_value":null,"scimago_value":null},{"author":"Taleb, Ikbal and Serhani, Mohamed Adel","title":"Big Data Pre-Processing: Closing the Data Quality Enforcement Loop","keywords":"Big Data;Optimization;Data models;Quality assessment;Big Data;Data Quality Evaluation;Data Quality Rules Discovery;Big Data Pre-Processing","abstract":"In the Big Data Era, data is the core for any governmental, institutional, and private organization. Efforts were geared towards extracting highly valuable insights that cannot happen if data is of poor quality. Therefore, data quality (DQ) is considered as a key element in Big data processing phase. In this stage, low quality data is not penetrated to the Big Data value chain. This paper, addresses the data quality rules discovery (DQR) after the evaluation of quality and prior to Big Data pre-processing. We propose a DQR discovery model to enhance and accurately target the pre-processing activities based on quality requirements. We defined, a set of pre-processing activities associated with data quality dimensions (DQD's) to automatize the DQR generation process. Rules optimization are applied on validated rules to avoid multi-passes pre-processing activities and eliminates duplicate rules. Conducted experiments showed an increased quality scores after applying the discovered and optimized DQR's on data.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigDataCongress.2017.73","jcs_value":null,"scimago_value":null},{"author":"Loetpipatwanich, Sakda and Vichitthamaros, Preecha","title":"Sakdas: A Python Package for Data Profiling and Data Quality Auditing","keywords":"Data integrity;Pipelines;Data visualization;Big Data;Syntactics;Software;Python;Data Quality Management;Data Profiling;Data Quality Auditing;Python Package;Data Pipeline","abstract":"Data Profiling and data quality management become a more significant part of data engineering, which an essential part of ensuring that the system delivers quality information to users. In the last decade, data quality was considered to need more managing. Especially in the big data era that the data comes from many sources, many data types, and an enormous amount. Thus it makes the managing of data quality is more difficult and complicated. The traditional system was unable to respond as needed. The data quality managing software for big data was developed but often found in a high-priced, difficult to customize as needed, and mostly provide as GUI, which is challenging to integrate with other systems. From this problem, we have developed an opensource package for data quality managing. By using Python programming language, Which is a programming language that is widely used in the scientific and engineering field today. Because it is a programming language that is easy to read syntax, small, and has many additional packages to integrate. The software developed here is called \u201cSakdas\u201d this package has been divided into three parts. The first part deals with data profiling provide a set of data analyses to generate a data profile, and this profile will help to define the data quality rules. The second part deals with data quality auditing that users can set their own data quality rules for data quality measurement. The final part deals with data visualizing that provides data profiling and data auditing report to improve the data quality. The results of the profiling and auditing services, the user can specify both the form of a report for self-review. Or in the form of JSON for use in post-process automation.","year":2020,"ENTRYTYPE":"inproceedings","doi":"10.1109\/IBDAP50342.2020.9245455","jcs_value":null,"scimago_value":null},{"author":"HongJu, Xiao and Fei, Wang and FenMei, Wang and XiuZhen, Wang","title":"Some key problems of data management in army data engineering based on big data","keywords":"Data engineering;Data analysis;Data integration;Big Data;Distributed databases;Data models;Uncertainty;army data engineering;data management;data integration;data analysis;representation of data analysis results;data quality","abstract":"This paper analyzed the challenges of data management in army data engineering, such as big data volume, data heterogeneous, high rate of data generation and update, high time requirement of data processing, and widely separated data sources. We discussed the disadvantages of traditional data management technologies to deal with these problems. We also highlighted the key problems of data management in army data engineering including data integration, data analysis, representation of data analysis results, and evaluation of data quality.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICBDA.2017.8078796","jcs_value":null,"scimago_value":null},{"author":"Peng, Zhibin and Chen, Yuefeng and Zhang, Zehong and Qiu, Queling and Han, Xiaoqiang","title":"Implementation of Water Quality Management Platform for Aquaculture Based on Big Data","keywords":"Data visualization;Aquaculture;Data mining;Big Data;Neural networks;Data models;Predictive models;aquaculture;big data;water quality warning;data visualization","abstract":"In order to ensure the quality and quantity of aquaculture, aquaculture farmers need to grasp the water quality in time. However, most farmers have to collect water quality data manually at present, and cannot store and reuse that information rapidly. This paper aims to use SpringBoot framework and JPA framework to build a big data platform of acquisition automation and visualization, which realizes the data analysis and display of heterogeneous water quality and breeding information. The platform can make the water quality prediction and real-time warning. Meanwhile, it realizes the management of robots, users and breeding experts. The application of this platform will bring better social benefits to aquaculture farmers.","year":2020,"ENTRYTYPE":"inproceedings","doi":"10.1109\/CIBDA50819.2020.00024","jcs_value":null,"scimago_value":null},{"author":"Patel, Jayesh","title":"An Effective and Scalable Data Modeling for Enterprise Big Data Platform","keywords":"Data models;Big Data;Business;Analytical models;Computational modeling;Lakes;Solid modeling;Big Data;Big Data Lake;Scalable Data Modeling;Hadoop;Spark;Business Intelligence;Big Data Analytics","abstract":"The enormous growth of the internet, enterprise applications, social media, and IoT devices in the current time caused a huge spike in enterprise data growth. Big data platform provided scalable storage to manage enterprise data growth and served easier data access to decision-makers, stakeholders and business users. It is a well-known challenge to classify, organize and store all this data and process it to provide business insights. Due to nature, variety, velocity, volume and value of data make it difficult to effectively process big data. Enterprises face challenges to apply complex business rules, to generate insights and to support data-driven decisions in a timely fashion. As big data lake integrates streams of data from a bunch of business units, stakeholders usually analyze enterprise-wide data from various data models. Data models are a vital component of Big data platform. Users may do complex processing, run queries and perform big table joins to generate required metrics depending on the available data models. It is usually a time consuming and resource-intensive process to find the value from data. It is a no-brainer that big data platform in the enterprise needs high-quality data modeling methods to reach an optimal mix of cost, performance, and quality. This paper addresses these challenges by proposing an effective and scalable way to organize and store data in Big Data Lake. It presents some of the basic principles and methodology to build scalable data models in a distributed environment. It also describes how it overcomes common challenges and presents findings.","year":2019,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData47090.2019.9005614","jcs_value":null,"scimago_value":null},{"author":"Taleb, Ikbal and Serhani, Mohamed Adel and Dssouli, Rachida","title":"Big Data Quality Assessment Model for Unstructured Data","keywords":"Big Data;Data integrity;Data mining;Feature extraction;Data models;Measurement;Quality assessment;Big Data;Data Quality;Unstructured Data;Quality of Unstructured Big Data","abstract":"Big Data has gained an enormous momentum the past few years because of the tremendous volume of generated and processed Data from diverse application domains. Nowadays, it is estimated that 80% of all the generated data is unstructured. Evaluating the quality of Big data has been identified to be essential to guarantee data quality dimensions including for example completeness, and accuracy. Current initiatives for unstructured data quality evaluation are still under investigations. In this paper, we propose a quality evaluation model to handle quality of Unstructured Big Data (UBD). The later captures and discover first key properties of unstructured big data and its characteristics, provides some comprehensive mechanisms to sample, profile the UBD dataset and extract features and characteristics from heterogeneous data types in different formats. A Data Quality repository manage relationships between Data quality dimensions, quality Metrics, features extraction methods, mining methodologies, data types and data domains. An analysis of the samples provides a data profile of UBD. This profile is extended to a quality profile that contains the quality mapping with selected features for quality assessment. We developed an UBD quality assessment model that handles all the processes from the UBD profiling exploration to the Quality report. The model provides an initial blueprint for quality estimation of unstructured Big data. It also, states a set of quality characteristics and indicators that can be used to outline an initial data quality schema of UBD.","year":2018,"ENTRYTYPE":"inproceedings","doi":"10.1109\/INNOVATIONS.2018.8605945","jcs_value":null,"scimago_value":null},{"author":"Arruda, Darlan and Madhavji, Nazim H.","title":"QualiBD: A Tool for Modelling Quality Requirements for Big Data Applications","keywords":"Tools;Data models;Containers;Software;Real-time systems;Big Data applications;Big Data Applications;Quality Requirements;Big Data Goal-oriented Requirements Language;Requirements Modelling Tool","abstract":"The development of Big Data applications is not well-explored, to our knowledge. Embracing Big Data in system building, questions arise as to how to elicit, specify, analyse, model, and document Big Data quality requirements. In our ongoing research, we explore a requirements modelling language for Big Data software applications. In this paper, we introduce QualiBD, a modelling tool that implements the proposed goal-oriented requirements language that facilitates the modelling of Big Data quality requirements.","year":2019,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData47090.2019.9006294","jcs_value":null,"scimago_value":null},{"author":"Norman, Ryan and Bolin, Jason and Powell, Edward T. and Amin, Sanket and Nacker, John","title":"Using Big Data Analytics to Create a Predictive Model for Joint Strike Fighter","keywords":"Big Data;Knowledge management;Tools;US Department of Defense;Cloud computing;Computer architecture;Data analysis;Big Data;Data Analytics;Knowledge Management;Data Management;Virtualization;Cloud Computing;Predictive Maintainance;Department of Defense;Test and Evaluation","abstract":"The amount of information needed to acquire knowledge on today's acquisition systems is growing exponentially due to more complex, higher resolution, software-intensive acquisition systems that need to operate in System-of-Systems (SoS), Family-of-Systems (FoS), Joint, and Coalition environments. Unfortunately, the tools and methods necessary to rapidly collect, aggregate, and analyze this information have not evolved as a whole in conjunction with this increased system complexity and, therefore, has made analysis and evaluation increasingly deficient and ineffective. The Test Resource Management Center's (TRMC's) vision is to build a DoD test and evaluation (T&E) knowledge management (KM) and analysis capability that leverages commercial big data analysis and cloud computing technologies to improve evaluation quality and reduce decision-making time. An evaluation revolution, starting with the Joint Strike Fighter (JSF) program, is underway to ensure the T&E community can support the demands of next-generation weapon systems.The true product of T&E is knowledge ascertained through the collection of information about a system or item under test. However, the T&E community's ability to provide this knowledge is hampered by more complex systems, more complex environments, and the need to be more agile in support of strategic initiatives, such as agile acquisition and the 3rd Offset Strategy. This increased complexity and need for speed cause delayed analysis and problems that go undetected during T&E. The primary reason for these shortfalls is antiquated tools and processes that make data hard to locate, aggregate, and convert into knowledge. In short, DoD has not evolved its evaluation infrastructure as its weapon systems have evolved.Conversely, commercial entities, such as medical observation and diagnosis, electric power distribution, retail, and industrial manufacturing, have embraced agility in their methodologies while modernizing analytics capabilities to keep up with the massive influx of data. Raw physical sensors could provide data, higher-quality image or video cameras, radio frequency identification (RFID) devices, faster data collectors, more detailed point-of-sale information or digitized records, and ultimately is providing more data to analysts in size and complexity than ever before. As more data has become available, an interrelated phenomenon is the desire of analysts to ask more detailed questions about their consumers and their business infrastructure. To drive the process of implementing big data analytics, businesses have begun establishing analytics centers which either take pre-defined business cases and apply methods to address them or implement existing knowledge within the data architecture to create a higher level of awareness to business groups or the company at-large. To meet these demands, data storage and computation architectures have become more sophisticated, dozens of technologies were developed for large-scale processing (such as Apache Hadoop or GreenPlum), and streaming architectures which allow data to be processed and actioned on in real-time as it is collected have become commonplace. The net result of these commercial best practices is a solid foundation for the DoD to transform how it uses data to achieve faster, better, and smarter decisions throughout the acquisition lifecycle.","year":2018,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData.2018.8622388","jcs_value":null,"scimago_value":null},{"author":"Pan, Xing and Zhang, Manli and Chen, Xi","title":"A Method of Quality Improvement Based on Big Quality Warranty Data Analysis","keywords":"Warranties;Data mining;Product design;Quality assessment;Databases;Big Data;Reliability engineering;quality warranty data;big data analysis;association rules;quality improvement;PDCA","abstract":"Quality warranty data includes big data of product use and customer services, which is foundation of product quality and reliability improvement. This paper presents a method of quality warranty data analysis, which is based on the big data analysis technology. By means of the method of association rules mining, it distinguishes the association rules of failure modes while feeding back the information to the process of product design, production, and usage. To achieve product fault location and fault disposal, the key factors such as fault type and fault cause are analyzed. Meanwhile, this paper adopted the principles of PDCA circulation to propose a procedure of product quality improvement. The quality improvement procedure based on quality warranty data analysis provides a comprehensive and systematic quality improvement for different stages and different types of products. Finally, a case study of household appliances in China is given to illustrate the method.","year":2018,"ENTRYTYPE":"inproceedings","doi":"10.1109\/QRS-C.2018.00115","jcs_value":null,"scimago_value":null},{"author":"Peng, Fang and Wang, Honggang and Zhuang, Li and Wang, Minnan and Yang, Chengyue","title":"Methods of enterprise electronic file content information mining under big data environment","keywords":"Scalability;Big Data;Information age;Search problems;Data mining;Electronic countermeasures;Software engineering;Big data environment;Enterprise electronic documents;Data mining","abstract":"As the product of the digital age, big data technology and computer information technology can greatly improve the efficiency and quality of file management and promote the development of enterprises. Based on this, this paper first analyzes the current status of enterprise archives management; Secondly, this paper discusses the countermeasures of information mining of electronic documents of innovative enterprises in the digital age. Text information mining is beneficial to improve the efficiency of text information search and utilization, aiming at the existing problems of traditional methods, the text information mining method is proposed.","year":2020,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICBASE51474.2020.00008","jcs_value":null,"scimago_value":null},{"author":"Li, Mingda and Wang, Hongzhi and Li, Jianzhong","title":"Mining conditional functional dependency rules on big data","keywords":"Big Data;Training;Data mining;Cleaning;Sampling methods;Heuristic algorithms;Fault tolerance;data mining;conditional functional dependency;big data;data quality","abstract":"Current Conditional Functional Dependency (CFD) discovery algorithms always need a well-prepared training dataset. This condition makes them difficult to apply on large and low-quality datasets. To handle the volume issue of big data, we develop the sampling algorithms to obtain a small representative training set. We design the fault-tolerant rule discovery and conflict-resolution algorithms to address the low-quality issue of big data. We also propose parameter selection strategy to ensure the effectiveness of CFD discovery algorithms. Experimental results demonstrate that our method can discover effective CFD rules on billion-tuple data within a reasonable period.","year":2020,"ENTRYTYPE":"article","doi":"10.26599\/BDMA.2019.9020019","jcs_value":null,"scimago_value":null},{"author":"Hongxun, Tian and Honggang, Wang and Kun, Zhou and Mingtai, Shi and Haosong, Li and Zhongping, Xu and Taifeng, Kang and Jin, Li and Yaqi, Cai","title":"Data quality assessment for on-line monitoring and measuring system of power quality based on big data and data provenance theory","keywords":"Data integrity;Power quality;Monitoring;Power measurement;Redundancy;Big Data;Business;power qualiy;data quality;big data;data provenance;data assessment","abstract":"Currently, on-line monitoring and measuring system of power quality has accumulated a huge amount of data. In the age of big data, those data integrated from various systems will face big data application problems. This paper proposes a data quality assessment system method for on-line monitoring and measuring system of power quality based on big data and data provenance to assess integrity, redundancy, accuracy, timeliness, intelligence and consistency of data set and single data. Specific assessment rule which conforms to the situation of on-line monitoring and measuring system of power quality will be devised to found data quality problems. Thus it will provide strong data support for big data application of power quality.","year":2018,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICCCBDA.2018.8386521","jcs_value":null,"scimago_value":null},{"author":"O\u2019Shea, Enda and Khan, Rafflesia and Breathnach, Ciara and Margaria, Tiziana","title":"Towards Automatic Data Cleansing and Classification of Valid Historical Data An Incremental Approach Based on MDD","keywords":"Java;Quality assurance;Semantics;Big Data;Tools;Syntactics;Cleaning;Data Collection;Data Analytics;Model-Driven Development;Historical Data;Data Parsing;Data Cleaning;Ethics;Data Assurance;Data Quality","abstract":"The project Death and Burial Data: Ireland 1864-1922 (DBDIrl) examines the relationship between historical death registration data and burial data to explore the history of power in Ireland from 1864 to 1922. Its core Big Data arises from historical records from a variety of heterogeneous sources, some aspects are pre-digitized and machine readable. A huge data set (over 4 million records in each source) and its slow manual enrichment (ca 7,000 records processed so far) pose issues of quality, scalability, and creates the need for a quality assurance technology that is accessible to non-programmers. An important goal for the researcher community is to produce a reusable, high-level quality assurance tool for the ingested data that is domain specific (historic data), highly portable across data sources, thus independent of storage technology.This paper outlines the step-wise design of the finer granular digital format, aimed for storage and digital archiving, and the design and test of two generations of the techniques, used in the first two data ingestion and cleaning phases.The first small scale phase was exploratory, based on metadata enrichment transcription to Excel, and conducted in parallel with the design of the final digital format and the discovery of all the domain-specific rules and constraints for the syntax and semantic validity of individual entries. Excel embedded quality checks or database-specific techniques are not adequate due to the technology independence requirement. This first phase produced a Java parser with an embedded data cleaning and evaluation classifier, continuously improved and refined as insights grew. The next, larger scale phase uses a bespoke Historian Web Application that embeds the Java validator from the parser, as well as a new Boolean classifier for valid and complete data assurance built using a Model-Driven Development technique that we also describe. This solution enforces property constraints directly at data capture time, removing the need for additional parsing and cleaning stages. The new classifier is built in an easy to use graphical technology, and the ADD-Lib tool it uses is a modern low-code development environment that auto-generates code in a large number of programming languages. It thus meets the technology independence requirement and historians are now able to produce new classifiers themselves without being able to program. We aim to infuse the project with computational and archival thinking in order to produce a robust data set that is FAIR compliant (Free Accessible Inter-operable and Re-useable).","year":2020,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData50022.2020.9378148","jcs_value":null,"scimago_value":null},{"author":"Ning, Xiuli and Xu, Yingcheng and Gao, Xiaohong and Li, Ying","title":"Missing data of quality inspection imputation algorithm base on stacked denoising auto-encoder","keywords":"Filling;Algorithm design and analysis;Noise reduction;Training;Clustering algorithms;Inspection;Big Data;Big data of quality inspection;Stacked denoising auto-encoder;Filling algorithm","abstract":"Analyzing and processing big data of quality inspection is the key factor in ensuring product quality and People's property security. Big data of quality inspection collected by social network and E-commerce is missing in most cases. And the incompleteness of data brings huge challenge for analyzing and processing. Therefore, the algorithm of data filling based on stacked denoising auto-encoder is proposed in this text. As the experiment shows that the algorithm proposed in this text is effective in dealing with big data of quality inspection.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICBDA.2017.8078781","jcs_value":null,"scimago_value":null},{"author":"Beecks, Christian and Uysal, Merih Seran and Seidl, Thomas","title":"Gradient-based signatures for big multimedia data","keywords":"Multimedia communication;Multimedia databases;Adaptation models;Data models;Indexing;Big data;Query processing;Big Multimedia Data;Content-based Information Access;Gradient-based Signatures;Feature Signatures","abstract":"With the continuous increase of heterogeneous multimedia data, the question of how to access big multimedia data efficiently has become of crucial importance. In order to provide fast access to complex multimedia data, we propose to approximate content-based features of multimedia objects by means of generative models. The proposed gradient-based signatures epitomize a high quality content-based approximation of multimedia objects and facilitate efficient indexing and query processing at large scale.","year":2015,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData.2015.7364093","jcs_value":null,"scimago_value":null},{"author":"Juneja, Ashish and Das, Nripendra Narayan","title":"Big Data Quality Framework: Pre-Processing Data in Weather Monitoring Application","keywords":"Big Data;Data integrity;Meteorology;Monitoring;Data mining;Organizations;Big Data;Big Data Quality;Data Quality;preprocessing;pre-processing","abstract":"Big Data has become an imminent part of all industries and business sectors today. All organizations in any sector like energy, banking, retail, hardware, networking, etc all generate huge quantum of heterogenous data which if mined, processed and analyzed accurately can reveal immensely useful patterns for business heads to apply to generate and grow their businesses. Big Data helps in acquiring, processing and analyzing large amounts of heterogeneous data to derive valuable results. Quality of information is affected by size, speed and format in which data is generated. Hence, Quality of Big Data is of great relevance and importance. We propose addressing various aspects of the raw data to improve its quality in the pre-processing stage, as the raw data may not usable as-is. We are exploring process like Cleansing to fix as much data as feasible, Noise filters to remove bad data, as well sub-processes for Integration and Filtering along with Data Transformation\/Normalization. We evaluate and profile the Big Data during acquisition stage, which is adapted to expectations to avoid cost overheads later while also improving and leading to accurate data analysis. Hence, it is imperative to improve Data quality even it is absorbed and utilized in an industry's Big Data system. In this paper, we propose a Pre-Processing Framework to address quality of data in a weather monitoring and forecasting application that also takes into account global warming parameters and raises alerts\/notifications to warn users and scientists in advance.","year":2019,"ENTRYTYPE":"inproceedings","doi":"10.1109\/COMITCon.2019.8862267","jcs_value":null,"scimago_value":null},{"author":"Jin, Li and Haosong, Li and Zhongping, Xu and Ting, Wang and Shuai, Wang and Yutong, Wei and Dongliang, Hu and Chunting, Kang and Jia, Wu and Dan, Su","title":"Research on Wide-area Distributed Power Quality Data Fusion Technology of Power Grid","keywords":"Power quality;Data integration;Distributed databases;Monitoring;Power grids;Computer architecture;Data models;Power Quality;Wide Area Distribution;Data Integration","abstract":"With the advancement of the \"big operation\" system construction, the online monitoring system for power quality has been integrated, and various power quality data have been incorporated into relevant organizations for unified management. Power quality management has a larger range of data, more types, and higher frequency. It needs to realize the unified storage management and efficient access of massive heterogeneous power quality data for the characteristics of data applications and the collection and aggregation of these effective data. This paper proposes a new type of grid wide-area distributed power quality data integration architecture, which is designed for multi-source, heterogeneous, distributed data integration technology and wide-area distributed data storage technology to solve the big data source problem and realize the sharing of power quality data information of the whole network.","year":2019,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICCCBDA.2019.8725668","jcs_value":null,"scimago_value":null},{"author":"Becker, David and King, Trish Dunn and McMullen, Bill","title":"Big data, big data quality problem","keywords":"Big data;Biomedical imaging;Personnel;Complexity theory;Bioinformatics;Process control;Instruments;Big Data;Data Quality;Returns to Scale","abstract":"A USAF sponsored MITRE research team undertook four separate, domain-specific case studies about Big Data applications. Those case studies were initial investigations into the question of whether or not data quality issues encountered in Big Data collections are substantially different in cause, manifestation, or detection than those data quality issues encountered in more traditionally sized data collections. The study addresses several factors affecting Big Data Quality at multiple levels, including collection, processing, and storage. Though not unexpected, the key findings of this study reinforce that the primary factors affecting Big Data reside in the limitations and complexities involved with handling Big Data while maintaining its integrity. These concerns are of a higher magnitude than the provenance of the data, the processing, and the tools used to prepare, manipulate, and store the data. Data quality is extremely important for all data analytics problems. From the study's findings, the \"truth about Big Data\" is there are no fundamentally new DQ issues in Big Data analytics projects. Some DQ issues exhibit return-s-to-scale effects, and become more or less pronounced in Big Data analytics, though. Big Data Quality varies from one type of Big Data to another and from one Big Data technology to another.","year":2015,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData.2015.7364064","jcs_value":null,"scimago_value":null},{"author":"Wong, Ka Yee. and Wong, Raymond K.","title":"Big Data Quality Prediction on Banking Applications: Extended Abstract","keywords":"Big Data;Data models;Banking;Neural networks;Industries;Data integrity;Computational modeling;Big Data;Data Noise;Data Quality;Prediction and Machine Learning","abstract":"Big data has been transformed into knowledge by information systems to add value in businesses. Enterprises relying on it benefit from risk management to a certain extent. The value, however, depends on the quality of data. The quality needs to be verified before any use of the data. Specifically, measuring the quality by simulating the real life situation and even forecast it accurately turns into a hot topic. In recent years, there have been numerous researches on the measurement and assessment of data quality. These are yet to utilize a scientific computational method for the measurement and prediction. Current methods either fail to make an accurate prediction or do not consider the correlation and time sequence factors of the data. To address this, we design a model to extend machine learning technique to business applications predicting this. Firstly, we implement the model to detect data noises from a risk dataset according to an international data quality standard from banking industry and then estimate their impacts with Gaussian and Bayesian methods. Secondly, we direct sequential learning in multiple deep neural networks for the prediction with an attention mechanism. The model is experimented with various network methodologies to show the predictive power of machine learning technique and is evaluated by validation data to confirm the model effectiveness. The model is scalable to apply to any industries utilizing big data other than the banking industry.","year":2020,"ENTRYTYPE":"inproceedings","doi":"10.1109\/DSAA49011.2020.00119","jcs_value":null,"scimago_value":null},{"author":"Rao, Dhana and Gudivada, Venkat N and Raghavan, Vijay V.","title":"Data quality issues in big data","keywords":"Databases;Big data;Measurement;Context;Cleaning;Biology;Computers;Data quality;big data;biological data;information quality","abstract":"Though the issues of data quality trace back their origin to the early days of computing, the recent emergence of Big Data has added more dimensions. Furthermore, given the range of Big Data applications, potential consequences of bad data quality can be for more disastrous and widespread. This paper provides a perspective on data quality issues in the Big Data context. it also discusses data integration issues that arise in biological databases and attendant data quality issues.","year":2015,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData.2015.7364065","jcs_value":null,"scimago_value":null},{"author":"Taleb, Ikbal and Kassabi, Hadeel T. El and Serhani, Mohamed Adel and Dssouli, Rachida and Bouhaddioui, Chafik","title":"Big Data Quality: A Quality Dimensions Evaluation","keywords":"Big data;Measurement;Feature extraction;Quality assessment;Social network services;Companies;Context;Big Data;data quality dimensions;data quality evaluation;Big data sampling","abstract":"Data is the most valuable asset companies are proud of. When its quality degrades, the consequences are unpredictable, can lead to complete wrong insights. In Big Data context, evaluating the data quality is challenging, must be done prior to any Big data analytics by providing some data quality confidence. Given the huge data size, its fast generation, it requires mechanisms, strategies to evaluate, assess data quality in a fast, efficient way. However, checking the Quality of Big Data is a very costly process if it is applied on the entire data. In this paper, we propose an efficient data quality evaluation scheme by applying sampling strategies on Big data sets. The Sampling will reduce the data size to a representative population samples for fast quality evaluation. The evaluation targeted some data quality dimensions like completeness, consistency. The experimentations have been conducted on Sleep disorder's data set by applying Big data bootstrap sampling techniques. The results showed that the mean quality score of samples is representative for the original data, illustrate the importance of sampling to reduce computing costs when Big data quality evaluation is concerned. We applied the Quality results generated as quality proposals on the original data to increase its quality.","year":2016,"ENTRYTYPE":"inproceedings","doi":"10.1109\/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0122","jcs_value":null,"scimago_value":null},{"author":"Ogudo, Kingsley A. and Nestor, Dahj Muwawa Jean","title":"Modeling of an Efficient Low Cost, Tree Based Data Service Quality Management for Mobile Operators Using in-Memory Big Data Processing and Business Intelligence use Cases","keywords":"Quality of service;Big Data;Business intelligence;Structured Query Language;Tools;Sparks;Service Quality Management;In-Memory Big Data;Business Intelligence;Service Quality Index;Over The Top Application (OTT);Data Traffic and ROI","abstract":"Network Operators are shifting their business interest towards Data services in a geometric progression manner, as Data services is becoming the major source of Telco revenue. The wide use of Data platforms; such as WhatsApp, Skype, Hangout and other Over the Top (OTT) voice applications over the traditional voice services is a clear indication that Network Operators need to adjust their business model and needs. And couple with the adoption of Smartphones usage which grows continuously year by year, this means more subscribers to manage, large amount of transactions generated, more network resources to be added and evidently more human technical expertise required to ensure good service quality. That has led to high investment on Robust Service Quality Management (SQM) and Customer Experience Management (CEM) to stay competitive in the market. The high investment is justified by the integration of Big Data Solutions, Machine Learning capabilities and good visualization of insight data. However, the Return on Investment (ROI) of the expensive systems are not as conspicuous as the provided functionalities and business rules. Therefore, in this paper an efficient model for low cost SQM system is presented, exploring the advantages of In-Memory Big Data processing and low cost business Intelligence tools to showcase how a good Service Quality Management can be implemented with no big investment.","year":2018,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICABCD.2018.8465410","jcs_value":null,"scimago_value":null},{"author":"Gonzalez-Alonso, P. and Vilar, R. and Lupia\u00f1ez-Villanueva, F.","title":"Meeting Technology and Methodology into Health Big Data Analytics Scenarios","keywords":"Big Data;Organizations;Medical services;Tools;Medical diagnostic imaging;Design methodology;Healthcare;health analytics;big data analytics;big data analytical frameworks;translational medicine","abstract":"Health organizations are collecting more data from a wider array of sources at greater speed every day. The analysis of this vast amount of data creates new opportunities to deliver modern personalized health and social care services. Big Data Analytics and underlying technologies have the potential to process and analyze these data to extract meaningful insights for improving quality of care, efficiency and sustainability of health and social care systems. Health organizations face therefore a new scenario where analytical tools must accommodate both traditional business intelligence and Big Data approaches, resulting in important technological and methodological challenges to be tackled. In this paper, we present a methodological approach to address the introduction of Big Data Analytics technologies into an integrated care provider.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1109\/CBMS.2017.71","jcs_value":null,"scimago_value":null},{"author":"Zhang, Pengcheng and Xiong, Fang and Gao, Jerry and Wang, Jimin","title":"Data quality in big data processing: Issues, solutions and open problems","keywords":"Big Data;Data integrity;Data analysis;Social network services;Data preprocessing;Internet;Big Data;Big data processing;Data Quality;Recommendation system;Prediction system","abstract":"With the rapid development of social networks, Internet of things, Cloud computing as well as other technologies, big data age is arriving. The increasing number of data has brought great value to the public and enterprises. Meanwhile how to manage and use big data better has become the focus of all walks of life. The 4V characteristics of big data have brought a lot of issues to the big data processing. The key to big data processing is to solve data quality issue, and to ensure data quality is a prerequisite for the successful application of big data technique. In this paper, we use recommendation systems and prediction systems as typical big data applications, and try to find out the data quality issues during data collection, data preprocessing, data storage and data analysis stages of big data processing. According to the elaboration and analysis of the proposed issues, the corresponding solutions are also put forward. Finally, some open problems to be solved in the future are also raised.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1109\/UIC-ATC.2017.8397554","jcs_value":null,"scimago_value":null},{"author":"Wu, Donghui","title":"A big data analytics framework for forecasting rare customer complaints: A use case of predicting MA members' complaints to CMS","keywords":"Decision trees;Training;Training data;Big Data;Prediction algorithms;Ethics;Contracts;CMS Star Ratings;customer complaints;big data analytics;ensemble methods;ethics;call center data;medical claims","abstract":"Centers for Medicare & Medicaid Services (CMS) publishes Medicare Part C Star Ratings each year to measure the quality of care of Medicare Advantage (MA) contracts. One of the key measures is Complaints about the Health Plan, which is captured in Complaints Tracking Module (CTM). Complaints resulted in CTM are rare events: for MA contracts with 2-5 star ratings, number of complaints for every 1,000 members range from .10 to 1.84 over last 5 years. Reducing number of complaints is extremely important to MA plans as they impact CMS reimbursements to MA plans. Forecasting and reducing complaints is an extremely technically challenging task, and involves ethics considerations in patients' rights and privacy. In this research, we constructed a big data analytics framework for forecasting rare customer complaints. First, we built a big data ingestion pipelines on a Hadoop platform: a) Ingest MA plan's customer complaints data from CTM from past 3 years. b) Ingest health plan's call center data for MA members from past 3 years, including both structured data and unstructured text script for the calls. c) Ingest MA members' medical claims, including members' demographics and enrollment history. d) Ingest MA members' pharmacy claims. e) Integrate and unified data from above sources, and enrich the data with additional engineered features into a big wide table, one row per member for analysis and modeling. Second, we designed a unique decision tree based Large Ensemble with Over-Sampling (LEOS) algorithm, which mimics random forest but with extreme oversampling of target class to increase bias, and leverages the parallel computing of Hadoop clusters by generating thousands of fixed size training data sets, and for each such dataset training a decision trees with similar fixed tree structure, and ensemble them. Third, we validated our framework and LEOS learning algorithm with real data, and also discussed ethics issues we encountered in handling data and applying findings from research.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData.2017.8258406","jcs_value":null,"scimago_value":null},{"author":"Muniswamaiah, Manoj and Agerwala, Tilak and Tappert, Charles C.","title":"Federated Query processing for Big Data in Data Science","keywords":"Databases;Machine learning algorithms;Engines;Machine learning;Task analysis;Protocols;Big Data;Machine learning;big data;federated query;database;query optimizer;in-memory data;data science","abstract":"As the number of databases continues to grow data scientists need to use data from different sources to run machine learning algorithms for analysis. Data science results depend upon the quality of data been extracted. The objective of this research paper is to implement a federated query processing framework which extracts data from different data sources and stores the result datasets in a common in-memory data format. This helps data scientists to perform their analysis and execute machine learning algorithms using different data engines without having to convert the data into their native data format and improve the performance.","year":2019,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData47090.2019.9005530","jcs_value":null,"scimago_value":null},{"author":"Kumar, Jitendra and Crow, Michael C. and Devarakonda, Ranjeet and Giansiracusa, Michael and Guntupally, Kavya and Olatt, Joseph V. and Price, Zach and Shanafield, Harold A. and Singh, Alka","title":"Provenance\u2013aware workflow for data quality management and improvement for large continuous scientific data streams","keywords":"Data integrity;Instruments;Atmospheric measurements;Dictionaries;Big Data;Portals;scientific data workflows;data quality;provenance;atmospheric science","abstract":"Data quality assessment, management and improvement is an integral part of any big data intensive scientific research to ensure accurate, reliable, and reproducible scientific discoveries. The task of maintaining the quality of data, however, is non-trivial and poses a challenge for a program like the Department of Energy's Atmospheric Radiation Measurement (ARM) that collects data from hundreds of instruments across the world, and distributes thousands of streaming data products that are continuously produced in near-real-time for an archive 1.7 Petabyte in size and growing. In this paper, we present a computational data processing workflow to address the data quality issues via an easy and intuitive web-based portal that allows reporting of any quality issues for any site, facility or instruments at a granularity down to individual variables in the data files. This portal allows instrument specialists and scientists to provide corrective actions in the form of symbolic equations. A parallel processing framework applies the data improvement to a large volume of data in an efficient, parallel environment, while optimizing data transfer and file I\/O operations; corrected files are then systematically versioned and archived. A provenance tracking module tracks and records any change made to the data during its entire life cycle which are communicated transparently to the scientific users. Developed in Python using open source technologies, this software architecture enables fast and efficient management and improvement of data in an operational data center environment.","year":2019,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData47090.2019.9006358","jcs_value":null,"scimago_value":null},{"author":"Serhani, Mohamed Adel and El Kassabi, Hadeel T. and Taleb, Ikbal and Nujum, Alramzana","title":"An Hybrid Approach to Quality Evaluation across Big Data Value Chain","keywords":"Big data;Metadata;Measurement;Quality assessment;Quality of service;Unified modeling language;Big Data;Quality assessment;Metadata;Quality metrics;quality Metadata;Quality of process;Hybrid quality assessment","abstract":"While the potential benefits of Big Data adoption are significant, and some initial successes have already been realized, there remain many research and technical challenges that must be addressed to fully realize this potential. The Big Data processing, storage and analytics, of course, are major challenges that are most easily recognized. However, there are additional challenges related for instance to Big Data collection, integration, and quality enforcement. This paper proposes a hybrid approach to Big Data quality evaluation across the Big Data value chain. It consists of assessing first the quality of Big Data itself, which involve processes such as cleansing, filtering and approximation. Then, assessing the quality of process handling this Big Data, which involve for example processing and analytics process. We conduct a set of experiments to evaluate Quality of Data prior and after its pre-processing, and the Quality of the pre-processing and processing on a large dataset. Quality metrics have been measured to access three Big Data quality dimensions: accuracy, completeness, and consistency. The results proved that combination of data-driven and process-driven quality evaluation lead to improved quality enforcement across the Big Data value chain. Hence, we recorded high prediction accuracy and low processing time after we evaluate 6 well-known classification algorithms as part of processing and analytics phase of Big Data value chain.","year":2016,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigDataCongress.2016.65","jcs_value":null,"scimago_value":null},{"author":"Nobles, Alicia L. and Vilankar, Ketki and Wu, Hao and Barnes, Laura E.","title":"Evaluation of data quality of multisite electronic health record data for secondary analysis","keywords":"Documentation;Data warehouses;Medical services;Big data;Databases;Cleaning;Medical diagnostic imaging;electronic health records;data quality;big data;multiple data vendors;metrics","abstract":"Currently, a large amount of data is amassed in electronic health records (EHRs). However, EHR systems are largely information silos, that is, uses of these systems are often confined to management of patient information and analytics specific to a clinician's practice. A growing trend in healthcare is combining multiple databases to support epidemiological research. The College Health Surveillance Network is the first national data warehouse containing EHR data from 31 different student health centers. Each member university contributes to the data warehouse by uploading select EHR data including patient demographics, diagnoses, and procedures to a common server on a monthly basis. In this paper, we focus on the data quality dimensions from a subsample of the data comprised of over 5.7 million patient visits for approximately 980,000 patients with 4,465 unique diagnoses from 23 of those universities. We examine the data for measures of completeness, consistency, and availability for secondary use for epidemiological research. Additionally, clinical documentation practices and EHR vendor were evaluated as potential contributors to data quality. We found that overall about 70% of the data in the data warehouse is available for secondary use, and identified clinical documentation practices that are correlated to a reduction in data quality. This suggests that automated quality control and proactive clinical documentation support could reduce ad-hoc data cleaning needs resulting in greater data availability for secondary use.","year":2015,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData.2015.7364060","jcs_value":null,"scimago_value":null},{"author":"Mao, Yifan and Huang, Shasha and Cui, Shuo and Wang, HaiFeng and Zhang, Junyan and Ding, Wenhao","title":"Multi dimensional data distribution monitoring based on OLAP","keywords":"Measurement;Analytical models;Standards organizations;Data integration;Systems architecture;Financial management;Information age;data link;OLAP;multidimensional data model","abstract":"With the rapid development of the Internet, society is gradually entering the information age, and various data in enterprises have become the most important strategic core resources of all enterprises. The operation and decision-making of enterprises all require a large amount of data analysis. Nowadays, many companies do not pay enough attention to the monitoring of data asset distribution. In addition, various internal systems such as financial management and ERP systems are relatively independent. Each system has its own data organization standard, which makes it difficult to conduct a unified management of data. This also directly leads to the one-sided and subjective problem of enterprise managers' distribution of data assets. With the construction of the data center of each enterprise, the data of each system is aggregated to the center through data integration technology. Therefore, all enterprises need to build a multi-dimensional data distribution monitoring model around data links to comprehensively monitor the status of various data distributions across the company's entire network, and improve data service capabilities and sharing capabilities as well as the company's operational capabilities. This article uses OLAP technology to construct a multi-dimensional data distribution monitoring model for the data link in the process of power enterprise data integration. This article first selects the dimensions and metrics that need to be monitored in the multidimensional data, and then constructs the conceptual model, logical model and physical model of the multidimensional data using on line analytical processing technology. Finally, an example analysis of OLAP system architecture based on B\/S structure is realized. The overall data distribution of the enterprise can be grasped by analyzing the various dimensions of the data link, such as System type, location distribution, and time.","year":2020,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ITCA52113.2020.00070","jcs_value":null,"scimago_value":null},{"author":"Li, Lianzhi","title":"Evaluation Model of Education Service Quality Satisfaction in Colleges and Universities Dependent on Classification Attribute Big Data Feature Selection Algorithm","keywords":"Education;Correlation;Data models;Encyclopedias;Big Data;Mutual information;Compounds;Classification Attribute Big Data Feature Selection Algorithm;Education Service Quality in Colleges and Universities;Education Service in Colleges and Universities;Satisfaction Evaluation","abstract":"In view of the insufficiency in the education service quality in colleges and universities, a kind of evaluation model of the education service quality satisfaction in the colleges and universities that is dependent on the classification attribute big data feature selection algorithm is put forward in this paper based on the existing work. On the basis of detailed description of the model components, further study on the evaluation method of the proposed model for the education service quality satisfaction in the colleges and universities is carried out. Under the guidance of the evaluation model of the education service quality satisfaction in the colleges and universities, the method for the construction of the evaluation model of the education service quality satisfaction in the colleges and universities is studied with the orientation to the education service resources in the colleges and universities under the open big data environment. In addition, experimental verification is carried out on the basis of the evaluation data in the 360 Encyclopedia on the education service quality satisfaction in the colleges and universities. The experimental results show that the model and method put forward in this paper can effectively evaluate the quality of the education service in the colleges and universities.","year":2019,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICITBS.2019.00160","jcs_value":null,"scimago_value":null},{"author":"Catarci, Tiziana and Scannapieco, Monica and Console, Marco and Demetrescu, Camil","title":"My (fair) big data","keywords":"Big Data;Pipelines;Ontologies;Metadata;Google;Quality-driven policies;Big Data pipeline;ontology-based quality checking;Big Data confidentiality","abstract":"Policy making has the strict requirement to rely on quantitative and high quality information. This paper will address the data quality issue for policy making by showing how to deal with Big Data quality in the different steps of a processing pipeline, with a focus on the integration of Big Data sources with traditional sources. In this respect, a relevant role is played by metadata and in particular by ontologies. Integration systems relying on ontologies enable indeed a formal quality evaluation of inaccuracy, inconsistency and incompleteness of integrated data. The paper will finally describe data confidentiality as a Big Data quality dimension, showing the main issues to be faced for its assurance.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData.2017.8258267","jcs_value":null,"scimago_value":null},{"author":"Taleb, Ikbal and Serhani, Mohamed Adel and Dssouli, Rachida","title":"Big Data Quality: A Survey","keywords":"Big Data;Data integrity;Quality management;Data visualization;Databases;Sensors;Social network services;Big Data, Data Quality, Quality Management framework, Quality of Big Data","abstract":"With the advances in communication technologies and the high amount of data generated, collected, and stored, it becomes crucial to manage the quality of this data deluge in an efficient and cost-effective way. The storage, processing, privacy and analytics are the main keys challenging aspects of Big Data that require quality evaluation and monitoring. Quality has been recognized by the Big Data community as an essential facet of its maturity. Yet, it is a crucial practice that should be implemented at the earlier stages of its lifecycle and progressively applied across the other key processes. The earlier we incorporate quality the full benefit we can get from insights. In this paper, we first identify the key challenges that necessitates quality evaluation. We then survey, classify and discuss the most recent work on Big Data management. Consequently, we propose an across-the-board quality management framework describing the key quality evaluation practices to be conducted through the different Big Data stages. The framework can be used to leverage the quality management and to provide a roadmap for Data scientists to better understand quality practices and highlight the importance of managing the quality. We finally, conclude the paper and point to some future research directions on quality of Big Data.","year":2018,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigDataCongress.2018.00029","jcs_value":null,"scimago_value":null},{"author":"Guan, Zhibin and Ji, Tongkai and Qian, Xu and Ma, Yan and Hong, Xuehai","title":"A Survey on Big Data Pre-processing","keywords":"Big Data;Data integration;Data analysis;Dimensionality reduction;Feature extraction;Data integrity;big data processing;data pre-processing;data cleansing;dimension reduction;data quality","abstract":"In this paper, we briefly introduce some basic concepts and characteristics of big data. We are surrounded by massive amount of data but starving for knowledge. In the era of Big Data, how to quickly obtain high-quality and valuable information from massive amounts of data has become an important research direction. Hence, we focus our attention to the data pre-processing which is a sub-content of the data processing workflow. In this paper, the four phases of data pre-processing, including data cleansing, data integration, data reduction, and data transformation, have been discussed. And different approaches for a variety of purposes have been presented, which show current methods and techniques need to be further modified in order to improve the quality of data before data analysis.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ACIT-CSII-BCD.2017.49","jcs_value":null,"scimago_value":null},{"author":"Li, Yuqian and Li, Peng and Zhu, Feng and Wang, Ruchuan","title":"Design of higher education quality monitoring and evaluation platform based on big data","keywords":"Education;Monitoring;Big Data;Data mining;Servers;Memory;Indexes;big data;monitoring and evaluation;system design","abstract":"Through the continuous collection and in-depth analysis of the quality monitoring data of colleges and universities, we combine the efficiency processing of big data and data evaluation, monitor the status of higher education normally, and construct a higher education quality monitoring and evaluation platform based on Spark. This platform is teaching centered with schools as its basis, including subsystems of data acquisition, data analysis, machine learning, data storage, data analysis and other areas. Through the application of the higher education quality monitoring platform, we can understand the current situation of the development of higher education scientifically, and provide the basis for the macro-decision of education administration department.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICCSE.2017.8085513","jcs_value":null,"scimago_value":null},{"author":"Rueda, Diego F. and Vergara, Dahyr and Reniz, David","title":"Big Data Streaming Analytics for QoE Monitoring in Mobile Networks: A Practical Approach","keywords":"Quality of experience;Big Data;Monitoring;Streaming media;Real-time systems;Tools;Big Data Analytics;customer experience management;mobile networks;quality of experience;streaming data processing","abstract":"Traditionally, Mobile Network Operators (MNOs) use a set of Key Performance Indicators (KPIs) to measure the quality offered to their customers. However, these KPIs do not reflect the quality perceived by the customers because they are high-level and network-based metrics. Instead, Quality of Experience (QoE) monitoring of the most common mobile applications can help MNOs to determine when and where customer experience is degraded. In this paper, a customized tool based on Big Data Streaming is proposed to solve the needs of customer experience monitoring in a real-life MNO and to overcome the challenges of processing a large amount of data collected in 3G and 4G mobile networks. Moreover, real-life case studies of value creation through Big Data Analytics for telecommunication industry are also defined. Results show that the streaming data processing enables new opportunities for the MNO to take actions focused on customer experience improvement in near real-time.","year":2018,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData.2018.8622590","jcs_value":null,"scimago_value":null},{"author":"Xiang, Zheng and Xiaofang, Liu and Weigang, Gao","title":"Analysis of the Application of Military Big Data in Equipment Quality Information Management","keywords":"Big Data;Military equipment;Information management;Distributed databases;Data analysis;Maintenance engineering;military big data;equipment quality information;management","abstract":"This At present, big data has risen to the national strategy. Big data is fully integrated into the military field, becoming the driving force of military scientific research, the core element of construction management, and an important resource for war success. This paper mainly expounds the basic connotation of big data technology and military big data, and analyzes the application of military big data in equipment quality information management, and proposes information collection, storage, analysis, processing, exchange and feedback on equipment quality information management. The countermeasures provide methods and basis for military big data in equipment information management.","year":2019,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICCCBDA.2019.8725744","jcs_value":null,"scimago_value":null},{"author":"Abdallah, Mohammad","title":"Big Data Quality Challenges","keywords":"Big Data;Quality Measurement;Quality Model;Quality Assurance","abstract":"Big Data, is a growing technique these days. There are many uses of Big Data; Artificial Intelligence, Health Care, Business, and many more. For that reason, it becomes necessary to deal with this massive volume of data with caution and care in a term to make sure that the data used and produced is in high quality. Therefore, the Big Data quality is must, and its rules have to be satisfied. In this paper, the main Big Data Quality Factors, which need to be measured, is presented in the perspective of the data itself, the data management, data processing, and data users. This research highlights the quality factors that may be used later to create different Big Data quality models.","year":2019,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICBDCI.2019.8686099","jcs_value":null,"scimago_value":null},{"author":"Arruda, Darlan and Laigner, Rodrigo","title":"Requirements Engineering Practices and Challenges in the Context of Big Data Software Development Projects: Early Insights from a Case Study","keywords":"Conferences;Big Data;Tools;Software;Data models;Requirements engineering;System analysis and design;Big Data Systems;Requirements Engineering;Case Study;Big Data Challenges;Big Data Requirements","abstract":"This paper reports on the results of an exploratory case study on a large-scale Big Data systems development project in the Oil&Gas domain within a non-profit organisation. The aim of this study was to investigate the RE practices and challenges in such projects, currently bereft in the scientific literature. This investigation was focused on: (a) RE practices; (b) sources and distribution of requirements; (c) the role of Big Data characteristics and technologies in RE and systems design; and (d) RE challenges in engineering Big Data Systems. The main results show that (a) there is a lack of specific project tailored RE practices, tools, and frameworks for elicitation, specification and modelling, analysis, and prioritisation of requirements; (b) 40% of the system's requirements are considered Big Data-related from which 75% are identified from internal sources; (c) Big Data characteristics and technologies play an important role in defining quality requirements and system's architecture; (d) five challenges in eliciting, documenting, and analysing Big Data related requirements were identified and discussed. The findings suggest academics and practitioners opportunities to engage in further research in this area.","year":2020,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData50022.2020.9377734","jcs_value":null,"scimago_value":null},{"author":"Priebe, Torsten and Markus, Stefan","title":"Business information modeling: A methodology for data-intensive projects, data science and big data governance","keywords":"Business;Data models;Terminology;Big data;Data warehouses;Wheels;Standards organizations;Data Modeling;Project Methodology;Data Governance;Metadata;Information Catalog","abstract":"This paper discusses an integrated methodology to structure and formalize business requirements in large data-intensive projects, e.g. data warehouses implementations, turning them into precise and unambiguous data definitions suitable to facilitate harmonization and assignment of data governance responsibilities. We place a business information model in the center - used end-to-end from analysis, design, development, testing to data quality checks by data stewards. In addition, we show that the approach is suitable beyond traditional data warehouse environments, applying it also to big data landscapes and data science initiatives - where business requirements analysis is often neglected. As proper tool support has turned out to be inevitable in many real-world settings, we also discuss software requirements and their implementation in the Accurity Glossary tool. The approach is evaluated based on a large banking data warehouse project the authors are currently involved in.","year":2015,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData.2015.7363987","jcs_value":null,"scimago_value":null},{"author":"Han, Weiguo and Jochum, Matthew","title":"A Machine Learning Approach for Data Quality Control of Earth Observation Data Management System","keywords":"Data integrity;Machine learning;Earth;Satellites;Big Data;Process control;Monitoring;Big Data;Machine Learning;Earth Observation Data;Data management;Data Quality;Random Forest","abstract":"In the big data era, innovative technologies like cloud computing, artificial intelligence, and machine learning are increasingly utilized in the large-scale data management systems of many industry sectors to make them more scalable and intelligent. Applying them to automate and optimize earth observation data management is a hot topic. To improve data quality control mechanisms, a machine learning method in combination with built-in quality rules is presented in this paper to evolve processes around data quality and enhance management of earth observation data. The rules of quality check are set up to detect the common issues, including data completeness, data latency, bad data, and data duplication, and the machine learning model is trained, tested, and deployed to address these quality issues automatically and reduce manual efforts.","year":2020,"ENTRYTYPE":"inproceedings","doi":"10.1109\/IGARSS39084.2020.9323615","jcs_value":null,"scimago_value":null},{"author":"Blanquer, Ignacio and Meira, Wagner","title":"EUBra-BIGSEA, A Cloud-Centric Big Data Scientific Research Platform","keywords":"Big Data;Data analysis;Data privacy;Quality of service;Data models;Security;Biological system modeling;Cloud-Computing,-Big-Data,-Quality-of-Service,-Data-Analytics","abstract":"This paper describes the achievements of project EUBra-BIGSEA, which has delivered programming models and data analytics tools for the development of distributed Big Data applications. As framework components, multiple data models are supported (e.g. data streams, multidimensional data, etc.) and efficient mechanisms to ensure privacy and security, on top of a QoS-aware layer for the smart and rapid provisioning of resources in a cloud-based environment.","year":2018,"ENTRYTYPE":"inproceedings","doi":"10.1109\/DSN-W.2018.00023","jcs_value":null,"scimago_value":null},{"author":"Taleb, Ikbal and Dssouli, Rachida and Serhani, Mohamed Adel","title":"Big Data Pre-processing: A Quality Framework","keywords":"Big data;Data integration;Accuracy;Distributed databases;Data analysis;Business;Big Data;Data Quality;pre-processing","abstract":"With the abundance of raw data generated from various sources, Big Data has become a preeminent approach in acquiring, processing, and analyzing large amounts of heterogeneous data to derive valuable evidences. The size, speed, and formats in which data is generated and processed affect the overall quality of information. Therefore, Quality of Big Data (QBD) has become an important factor to ensure that the quality of data is maintained at all Big data processing phases. This paper addresses the QBD at the pre-processing phase, which includes sub-processes like cleansing, integration, filtering, and normalization. We propose a QBD model incorporating processes to support Data quality profile selection and adaptation. In addition, it tracks and registers on a data provenance repository the effect of every data transformation happened in the pre-processing phase. We evaluate the data quality selection module using large EEG dataset. The obtained results illustrate the importance of addressing QBD at an early phase of Big Data processing lifecycle since it significantly save on costs and perform accurate data analysis.","year":2015,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigDataCongress.2015.35","jcs_value":null,"scimago_value":null},{"author":"Juddoo, Suraj and George, Carlisle","title":"Discovering Most Important Data Quality Dimensions Using Latent Semantic Analysis","keywords":"Data integrity;Big Data;Semantics;Industries;Bibliographies;Libraries;Production;Big Data;data quality;health data;data quality dimensions;latent semantic analysis","abstract":"Big Data quality is a field which is emerging. Many authors nowadays agree that data quality is still very relevant, even for Big Data uses. However, there is a lack of frameworks or guidelines about how to carry out those big data quality initiatives. The starting point of any data quality work is to determine the properties of data quality, termed as data quality dimensions (DQDs). Even those dimensions lack precise rigour in terms of definition from existing literature. This current research aims to contribute towards identifying the most important DQDs for big data in the health industry. It is a continuation of a previous work, which already identified five most important DQDs, using a human judgement based technique known as inner hermeneutic cycle. To remove potential bias coming from the human judgement aspect, this research uses the same set of literature but applies a statistical technique known to extract knowledge from a set of documents known as latent semantic analysis. The results confirm only 2 similar most important DQDs, namely accuracy and completeness.","year":2018,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICABCD.2018.8465129","jcs_value":null,"scimago_value":null},{"author":"Guntupally, Kavya and Devarakonda, Ranjeet and Kehoe, Kenneth","title":"Spring Boot based REST API to Improve Data Quality Report Generation for Big Scientific Data: ARM Data Center Example","keywords":"Springs;Databases;Service-oriented architecture;Data integrity;Tools;Servers;Big Data;auto configuration;CRUD;java framework;service-oriented architecture;REST;spring boot","abstract":"Web application technologies are growing rapidly with continuous innovation and improvements. This paper focuses on the popular Spring Boot [1] java-based framework for building web and enterprise applications and how it provides the flexibility for service-oriented architecture (SOA). One challenge with any Spring-based applications is its level of complexity with configurations. Spring Boot makes it easy to create and deploy stand-alone, production-grade Spring applications with very little Spring configuration. Example, if we consider Spring Model-View-Controller (MVC) framework [2], we need to configure dispatcher servlet, web jars, a view resolver, and component scan among other things. To solve this, Spring Boot provides several Auto Configuration options to setup the application with any needed dependencies. Another challenge is to identify the framework dependencies and associated library versions required to develop a web application. Spring Boot offers simpler dependency management by using a comprehensive, but flexible, framework and the associated libraries in one single dependency, which provides all the Spring related technology that you need for starter projects as compared to CRUD web applications. This framework provides a range of additional features that are common across many projects such as embedded server, security, metrics, health checks, and externalized configuration. Web applications are generally packaged as war and deployed to a web server, but Spring Boot application can be packaged either as war or jar file, which allows to run the application without the need to install and\/or configure on the application server. In this paper, we discuss how Atmospheric Radiation Measurement (ARM) Data Center (ADC) at Oak Ridge National Laboratory, is using Spring Boot to create a SOA based REST [4] service API, that bridges the gap between frontend user interfaces and backend database. Using this REST service API, ARM scientists are now able to submit reports via a user form or a command line interface, which captures the same data quality or other important information about ARM data.","year":2018,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData.2018.8621924","jcs_value":null,"scimago_value":null},{"author":"Khaleel, Majida Yaseen and Hamad, Murtadha M.","title":"Data Quality Management for Big Data Applications","keywords":"Distributed databases;Metadata;Big Data;Standards;File systems;Data integrity;Big Data;data quality;unstructured Data Distributed data file system;and statistical model.","abstract":"Currently, as a result of the continuous increase of data, one of the key issues is the development of systems and applications to deal with storage, management and processing of big numbers of data. These data are found in unstructured ways. Data management with traditional approaches is inappropriate because of the large and complex data sizes. Hadoop is a suitable solution for the continuous increase in data sizes. The important characteristics of the Hadoop are distributed processing, high storage space, and easy administration. Hadoop is better known for distributed file systems. In this paper, we have proposed techniques and algorithms that deal with big data including data collecting, data preprocessing, algorithms for data cleaning, A Technique for Converting Unstructured Data to Structured Data using metadata, distributed data file system (fragmentation algorithm) and Quality assurance algorithms by using the model is the statistical model to evaluate the highest educational institutions. We concluded that Metadata accelerates query response required and facilitates query execution, metadata will be content for reports, fields and descriptions. Total time access for three complex queries in distributed processing it is 00: 03: 00 per second while in nondistributed processing it is at 00: 15: 77 per second, average is approximately five minutes per second. Quality assurance note values (T-test) is 0.239 and values (T-dis) is 1.96, as a result of dealing with scientific sets and humanities sets. In the comparison law, it can be deduced that if the t-test is smaller than the t-dis; so there is no difference between the mean of the scientific and humanities samples, the values of C.V for both scientific is (8.585) and humanities sets is (7.427), using the law of homogeneity know whether any sets are more homogeneous whenever the value of a small C.V was more homogeneous however the humanity set is more homogeneity.","year":2019,"ENTRYTYPE":"inproceedings","doi":"10.1109\/DeSE.2019.00072","jcs_value":null,"scimago_value":null},{"author":"Shanmugam, Srinivasan and Seshadri, Gokul","title":"Aspects of Data Cataloguing for Enterprise Data Platforms","keywords":"Context;Business;Metadata;Big data;Electronic mail;Reliability;Indexes;Enterprise data;Data catalogue;Metadata of data;Data context;Data quality;Data governance;Data as a service","abstract":"As the adoption of enterprise Big Data platforms mature, the necessity to maintain systematic catalogue of data being processed and managed by these platforms becomes imperative. Enterprise data catalogues serve as centralized repositories of storing such metadata about data being handled by such platforms, enabling better data governance, security and control. Standardized approaches, methodologies and tools for data catalogues are being discussed and evolved. This paper introduces some key variables, attributes and indexes that need to be handled in such cataloguing solutions -- such as data contexts, data-system relationships, data quality, reliability, sensitivity and accessibility. The paper also discusses specific approaches on how each of these aspects can be adopted and applied to different enterprise contexts effectively.","year":2016,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigDataSecurity-HPSC-IDS.2016.52","jcs_value":null,"scimago_value":null},{"author":"Assefi, Mehdi and Behravesh, Ehsun and Liu, Guangchi and Tafti, Ahmad P.","title":"Big data machine learning using apache spark MLlib","keywords":"Sparks;Big Data;Libraries;Data models;Computer architecture;Machine learning algorithms;Apache Spark MLlib;Big Data Machine Learning;Big Data Analytics;Machine Learning","abstract":"Artificial intelligence, and particularly machine learning, has been used in many ways by the research community to turn a variety of diverse and even heterogeneous data sources into high quality facts and knowledge, providing premier capabilities to accurate pattern discovery. However, applying machine learning strategies on big and complex datasets is computationally expensive, and it consumes a very large amount of logical and physical resources, such as data file space, CPU, and memory. A sophisticated platform for efficient big data analytics is becoming more important these days as the data amount generated in a daily basis exceeds over quintillion bytes. Apache Spark MLlib is one of the most prominent platforms for big data analysis which offers a set of excellent functionalities for different machine learning tasks ranging from regression, classification, and dimension reduction to clustering and rule extraction. In this contribution, we explore, from the computational perspective, the expanding body of the Apache Spark MLlib 2.0 as an open-source, distributed, scalable, and platform independent machine learning library. Specifically, we perform several real world machine learning experiments to examine the qualitative and quantitative attributes of the platform. Furthermore, we highlight current trends in big data machine learning research and provide insights for future work.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData.2017.8258338","jcs_value":null,"scimago_value":null},{"author":"Snineh, Sidi Mohamed and Bouattane, Omar and Youssfi, Mohamed and Daaif, Abdelaziz","title":"Towards a multi-agents model for errors detection and correction in big data flows","keywords":"Big Data;Multi-agent systems;Companies;Task analysis;Error correction;Data integrity;Real-time systems;Multi-agents;Big Data;Data quality;detection and correction of data errors","abstract":"The quality of data for decision-making will always be a major factor for companies that want to remain competitors. In addition, the era of Big Data has brought new challenges for the processing, management, storage of data and in particular the challenge represented by the veracity of these data which is one of the 5Vs that characterizes Big Data. This characteristic that defines the quality or reliability of the data and its sources must be verified in the future systems of each company. In this paper, we present an approach that helps to improve the quality of Big Data by the distributed execution of algorithms for detecting and correcting data errors. The idea is to have a multi-agents model for errors detection and correction in big data flow. This model linked to a repository specific to each company. This repository contains the most frequent errors, metadata, error types, error detection algorithms and error correction algorithms. Each agent of this model represents an algorithm and will be deployed in multiple instances when needed. The use of these agents will go through two steps. In the first step, the detection agents and error correction agents manage each flow entering the system in real time. In the second step, all the processed data flows in first step will be a dataset to which the error detection and correction agents are applied in batch in order to process other types of errors. Among architectures who allow this processing type, we have chosen Lambda architecture.","year":2019,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICDS47004.2019.8942297","jcs_value":null,"scimago_value":null},{"author":"Chang, Yue Shan and Lin, Kuan-Ming and Tsai, Yi-Ting and Zeng, Yu-Ren and Hung, Cheng-Xiang","title":"Big data platform for air quality analysis and prediction","keywords":"Semantics;Big Data;Air quality;Data mining;Urban areas;Monitoring;Government;Air Quality;Big Data;Prediction;Cloud Environment","abstract":"With the advance of industry, air quality (AQ) is increasingly becoming worse. There are increasingly AQ monitors device have been deployed around country for monitoring air-quality all year long. To estimate and predict AQ, such as PM (particulate matter) 2.5, become an important issue for government to improve people's quality of life. As we can know, there are many factors can affect the AQ, such as traffic, factory exhaust emissions, weather, incineration of garbage, and so on. In most well-developed countries, these pollution sources are monitored for future environmental policy making. In this paper, we will propose a semantic ETL (Extract-Transform-Load) framework on cloud platform for AQ prediction. In the platform, we exploit ontology to concretize the relationship of PM 2.5 from various data sources and to merge those data with the same concept but different naming into the unified database. We implement the ETL framework on the cloud platform, which includes computing nodes and storage nodes. The computing nodes are used to execute data mining algorithms for predicting, and storage modes are used to store retrieved, preprocessed, and analyzed data. We utilize restful web service as the front end API to retrieve analyzed data, and finally we exploit browser to show the visualized result to demonstrate the estimation and prediction. It shows that the big data access framework on the cloud platform can work well for air quality analysis.","year":2018,"ENTRYTYPE":"inproceedings","doi":"10.1109\/WOCC.2018.8372743","jcs_value":null,"scimago_value":null},{"author":"Immonen, Anne and P\u00e4\u00e4kk\u00f6nen, Pekka and Ovaska, Eila","title":"Evaluating the Quality of Social Media Data in Big Data Architecture","keywords":"Big data;Social network services;Computer architecture;Meta data;Online services;architecture;big data;metadata;quality attribute;quality of data;Architecture;big data;metadata;quality attribute;quality of data","abstract":"The use of freely available online data is rapidly increasing, as companies have detected the possibilities and the value of these data in their businesses. In particular, data from social media are seen as interesting as they can, when properly treated, assist in achieving customer insight into business decision making. However, the unstructured and uncertain nature of this kind of big data presents a new kind of challenge: how to evaluate the quality of data and manage the value of data within a big data architecture? This paper contributes to addressing this challenge by introducing a new architectural solution to evaluate and manage the quality of social media data in each processing phase of the big data pipeline. The proposed solution improves business decision making by providing real-time, validated data for the user. The solution is validated with an industrial case example, in which the customer insight is extracted from social media data in order to determine the customer satisfaction regarding the quality of a product.","year":2015,"ENTRYTYPE":"article","doi":"10.1109\/ACCESS.2015.2490723","jcs_value":null,"scimago_value":null},{"author":"Juddoo, Suraj and George, Carlisle","title":"A Qualitative Assessment of Machine Learning Support for Detecting Data Completeness and Accuracy Issues to Improve Data Analytics in Big Data for the Healthcare Industry","keywords":"Industries;Machine learning algorithms;Data integrity;Clustering algorithms;Medical services;Big Data;Tools;Big Data;Data Quality;Data Inaccuracy;Data incompleteness;Machine Learning","abstract":"Tackling Data Quality issues as part of Big Data can be challenging. For data cleansing activities, manual methods are not efficient due to the potentially very large amount of data.. This paper aims to qualitatively assess the possibilities for using machine learning in the process of detecting data incompleteness and inaccuracy, since these two data quality dimensions were found to be the most significant by a previous research study conducted by the authors. A review of existing literature concludes that there is no unique machine learning algorithm most suitable to deal with both incompleteness and inaccuracy of data. Various algorithms are selected from existing studies and applied against a representative big (healthcare) dataset. Following experiments, it was also discovered that the implementation of machine learning algorithms in this context encounters several challenges for Big Data quality activities. These challenges are related to the amount of data particualar machine learning algorithms can scale to and also to certain data type restrictions imposed by some machine learning algorithms. The study concludes that 1) data imputation works better with linear regression models, 2) clustering models are more efficient to detect outliers but fully automated systems may not be realistic in this context. Therefore, a certain level of human judgement is still needed.","year":2020,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ELECOM49001.2020.9297009","jcs_value":null,"scimago_value":null},{"author":"Haryadi, Adiska Fardani and Hulstijn, Joris and Wahyudi, Agung and van der Voort, Haiko and Janssen, Marijn","title":"Antecedents of big data quality: An empirical examination in financial service organizations","keywords":"Big data;Industries;Insurance;Companies;Finance;big data;data quality;big data quality;antecedents;finance","abstract":"Big data has been acknowledged for its enormous potential. In contrast to the potential, in a recent survey more than half of financial service organizations reported that big data has not delivered the expected value. One of the main reasons for this is related to data quality. The objective of this research is to identify the antecedents of big data quality in financial institutions. This will help to understand how data quality from big data analysis can be improved. For this, a literature review was performed and data was collected using three case studies, followed by content analysis. The overall findings indicate that there are no fundamentally new data quality issues in big data projects. Nevertheless, the complexity of the issues is higher, which makes it harder to assess and attain data quality in big data projects compared to the traditional projects. Ten antecedents of big data quality were identified encompassing data, technology, people, process and procedure, organization, and external aspects.","year":2016,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData.2016.7840595","jcs_value":null,"scimago_value":null},{"author":"Li, Tao and He, Yihai and Zhu, Chunling","title":"Big Data Oriented Macro-Quality Index Based on Customer Satisfaction Index and PLS-SEM for Manufacturing Industry","keywords":"Indexes;Mathematical model;Data models;Manufacturing industries;Computational modeling;Big data;Customer satisfaction;macro-quality evaluation;big data;macro-quality index;customer satisfaction index;PLS-SEM","abstract":"The aim of this paper was to develop a novel macro-quality index driven by big quality data regarding the macro-quality management demands of manufacturing enterprises in Industry 4.0. Firstly, the connotation of big data of macro-quality management in manufacturing industry is expounded, which is the collection of the quality data in product lifecycle including the product quality data, process quality data and organizational ability data. Secondly, referring to the customer satisfaction index theory, a new big data oriented macro-quality index computation model based on the partial least square-structural equation modeling(PLS-SEM) theory is proposed, and the partial least square(PLS) is adopted to estimate the path parameters. Finally, a case study of macro-quality situation evaluation for the manufacturing industry of a city in China is presented. The final result shows that the proposed macro-quality index model is applicable and predictable.","year":2016,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICIICII.2016.0052","jcs_value":null,"scimago_value":null},{"author":"Huang, Xiaotao and Qin, Niannian and Zhang, Xiaofang and Wang, Fen","title":"Experimental teaching design and practice on big data course","keywords":"Big Data;Education;Data mining;Clustering algorithms;Data visualization;Classification algorithms;Industries;Experimental teaching;Big data course;Big data processing lifecycle;Big data project;Big data","abstract":"With the rapid development of big data technology and the rapid growth of big data industry market, big data talent demand is also a substantial increase in China. In order to cultivate more talented people satisfying the needs of the community, we have designed the big data course for undergraduates. The big data course stresses not only on many theories but also lots of practice. The project of \u201cbig data talent development trend analysis\u201d is designed in the experimental teaching on big data. By doing this project, students can master all the technologies of big data processing lifecycle, including data collection, data preprocessing, data mining and data visualization. We evaluate students who master big data core technology with a multi-evaluation method and design the experiment evaluation system on big data. Through our two years' practice, the results show that all these designs have achieved the good effect and improved the teaching quality.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICCSE.2017.8085555","jcs_value":null,"scimago_value":null},{"author":"Haug, Frank S.","title":"Bad big data science","keywords":"Big data;Metadata;Distributed databases;Stakeholders;Data science;Big Data;Data Quality;Data Warehousing;Distributed Database;Metadata","abstract":"As hardware and software technologies have improved, our definition of a \u201cmanageable amount of data\u201d has increased in its scope dramatically. The term \u201cbig data\u201d can be applied to any of several different projects and technologies sharing the ultimate goal of supporting analysis on these large, heterogeneous, and evolving data sets. The term \u201cdata science\u201d refers to the statistical, technical, and domain-specific knowledge required to ensure that the analysis is done properly. Techniques for managing some common causes for bad data and invalid analysis have been used in other areas, such as data warehousing and distributed database. However, big data projects face special challenges when trying to combine big data and data science without producing inaccurate, misleading, or invalid results. This paper discusses potential causes for \u201cbad big data science\u201d, focusing primarily on the data quality of the input data, and suggests methods for minimizing them based on techniques originally developed for data warehousing and distributed database projects.","year":2016,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData.2016.7840935","jcs_value":null,"scimago_value":null},{"author":"Wang, Jinghan and Zhang, Jinnan and Yuan, XueGuang and Tang, Yu and Hao, Hongyu and Zuo, Yong and Tan, Zebin and Qiao, Min and Cao, Yang Hua and Ai, Lingmei and Wan, Yihang and Chen, Hao","title":"Air quality data analysis and forecasting platform based on big data","keywords":"Air quality;Big Data;Data mining;Photonics;Optical fiber communication;Telecommunications;Clustering algorithms;Air quality;Big data;Data mining;Data visualization","abstract":"Nowadays, with the continuous development of big data technology, various industries use big data technology to process and mine massive data, and realize the value of data efficiently. In terms of air quality data processing, big data technology can also play a certain advantage. The platform is based on big data technology to design an air quality data analysis and prediction platform including data layer, business layer, interaction layer and visualization platform. Data is cleaned, calibrated, and stored in the data layer to ensure data consistency, integrity, and security. The air quality data is analyzed and predicted at the business layer. The interaction layer includes the functions of algorithm management, data query, and the data visualization platform provides intuitive information display. This design is a significant application for fully exploiting environmental data information. It has powerful data processing functions and scalability, which is a reliable data analysis and prediction platform.","year":2019,"ENTRYTYPE":"inproceedings","doi":"10.1109\/CAC48633.2019.8996332","jcs_value":null,"scimago_value":null},{"author":"Challa, Jagat Sesh and Goyal, Poonam and Nikhil, S. and Mangla, Aditya and Balasubramaniam, Sundar S. and Goyal, Navneet","title":"DD-Rtree: A dynamic distributed data structure for efficient data distribution among cluster nodes for spatial data mining algorithms","keywords":"Data structures;Clustering algorithms;Data mining;Indexing;Distributed databases;Algorithm design and analysis;Data mining;data distribution;spatial locality;neighborhood queries;k-NN queries;density based clustering","abstract":"Parallelizing data mining algorithms has become a necessity as we try to mine ever increasing volumes of data. Spatial data mining algorithms like Dbscan, Optics, Slink, etc. have been parallelized to exploit a cluster infrastructure. The efficiency achieved by existing algorithms can be attributed to spatial locality preservation using spatial indexing structures like k-d-tree, quad-tree, grid files, etc. for distributing data among cluster nodes. However, these indexing structures are static in nature, i.e., they need to scan the entire dataset to determine the partitioning coordinates. This results in high data distribution cost when the data size is large. In this paper, we propose a dynamic distributed data structure, DD-Rtree, which preserves spatial locality while distributing data across compute nodes in a shared nothing environment. Moreover, DD-Rtree is dynamic, i.e., it can be constructed incrementally making it useful for handling big data. We compare the quality of data distribution achieved by DD-Rtree with one of the recent distributed indexing structure, SD-Rtree. We also compare the efficiency of queries supported by these indexing structures along with the overall efficiency of DBSCAN algorithm. Our experimental results show that DD-Rtree achieves better data distribution and thereby resulting in improved overall efficiency.","year":2016,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData.2016.7840586","jcs_value":null,"scimago_value":null},{"author":"Shao, Jun and Xu, Daqi and Feng, Chun and Chi, Mingmin","title":"Big data challenges in China centre for resources satellite data and application","keywords":"Satellites;Remote sensing;Big Data;Monitoring;Land observing satellites;data center;big data;data challenges","abstract":"China Centre for Resources Satellite Data and Application (abbreviate as CRESDA) is a core platform to store, process, analyze, and distribute land observing satellite data in China. It can provide high quality and effective services for the State Council and the relevant departments of government and local authorities. In the era of big data, the data center benefits from big data opportunities as well as suffering from big data challenges. In the paper, the big data challenges of the CRESDA are summarized. In particular, four major challenges are comprised of the 3V dimensions of big data (i.e. Volume, Variety, and Velocity) and one specific challenge (i.e., extensibility) in the data center.","year":2015,"ENTRYTYPE":"inproceedings","doi":"10.1109\/WHISPERS.2015.8075482","jcs_value":null,"scimago_value":null},{"author":"Bai, Zhongxian and Zhuo, Rongqing","title":"Quality Management of Crowd Sensing Data Based on Machine Learning","keywords":"Time series analysis;Temperature sensors;Mobile handsets;Temperature distribution;Databases;Data models;Machine Learning;Crowd Sensing;Big Data Analysis;Abnormal Data Detection Management;Clustering Method","abstract":"Recently, research on crowd sensing data quality management is a new subject area developed based on wireless sensor network related concepts. Crowd sensing data has also experienced many links in the process of network propagation, so it is inevitable that there are abnormal data in the database. Therefore, how to filter these unreliable data to get more real data is particularly important. This paper takes somatosensory temperature as an example, solves the problem of calculating the similarity of unequal long-term sequences by using DTW technology, and then clusters and compares the data in the database to find out the abnormal data. Thereby, the accuracy of the somatosensory temperature database is improved, and relevant users can obtain more accurate information. The experimental results show that when the minimum DTW value exceeds the threshold t = 0.7, the more the number of simulation sequences, the more stable the accuracy rate, and the faster the growth rate of the running time.","year":2020,"ENTRYTYPE":"inproceedings","doi":"10.1109\/CIBDA50819.2020.00049","jcs_value":null,"scimago_value":null},{"author":"Debattista, Jeremy and Lange, Christoph and Scerri, Simon and Auer, S\u00f6ren","title":"Linked 'Big' Data: Towards a Manifold Increase in Big Data Value and Veracity","keywords":"Big data;Semantics;Resource description framework;Data models;Encyclopedias;Vocabulary;linked data;Web of Data;Veracity;Value;Big Data dimension","abstract":"The Web of Data is an increasingly rich source of information, which makes it useful for Big Data analysis. However, there is no guarantee that this Web of Data will provide the consumer with truthful and valuable information. Most research has focused on Big Data's Volume, Velocity, and Variety dimensions. Unfortunately, Veracity and Value, often regarded as the fourth and fifth dimensions, have been largely overlooked. In this paper we discuss the potential of Linked Data methods to tackle all five V's, and particularly propose methods for addressing the last two dimensions. We draw parallels between Linked and Big Data methods, and propose the application of existing methods to improve and maintain quality and address Big Data's veracity challenge.","year":2015,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BDC.2015.34","jcs_value":null,"scimago_value":null},{"author":"Zuo, Yiming and Vernica, Rares and Lei, Yang and Barcelo, Steven and Rogacs, Anita","title":"A Big Data Platform for Surface Enhanced Raman Spectroscopy Data with an Application on Image-Based Sensor Quality Control","keywords":"Quality control;Servers;Cloud computing;Big Data;Visualization;Inspection;Databases;Big data platform, Surface Enhanced Raman Spectroscopy, sensor quality control","abstract":"Surface-enhanced Raman spectroscopy (SERS) significantly enhances the Raman scattering by molecules, enabling detection and identification of small quantities of relevant bio-\/chemical markers in a wide range of applications. In this paper, we present a big data platform with both a local client and cloud server built for acquiring, processing, visualizing and storing SERS sensor data. The local client controls the hardware (i.e., spectrometer and stage) to collect SERS spectra from HP designed sensors, and offers the options to analyze, visualize and save the spectra with meta-data records, including relevant experimental conditions. The cloud server contains remote databases and web interface for centralized data management to users from different locations. Here we describe how this platform was built and demonstrate its use for automated sensor quality control based on sensor images. Sensor quality control is a common practice, employed in sensor production to select high performing sensors. Image-based approach is a natural way to perform sensor quality control without destructing the sensors. Automating this process using the proposed platform can also reduce the time spent and achieve consistent result by avoiding human visual inspection.","year":2019,"ENTRYTYPE":"inproceedings","doi":"10.1109\/MIPR.2019.00093","jcs_value":null,"scimago_value":null},{"author":"Sinaeepourfard, Amir and Garcia, Jordi and Masip-Bruin, Xavier and Mar\u00edn-Tordera, Eva","title":"Towards a Comprehensive Data LifeCycle Model for Big Data Environments","keywords":"Data models;Adaptation models;Biological system modeling;Big Data;Organizations;Data acquisition;Computational modeling;Big Data;Data LifeCycle;Data Management;Data Organization;Data Complexity;Vs Challenges","abstract":"A huge amount of data is constantly being produced in the world. Data coming from the IoT, from scientific simulations, or from any other field of the eScience, are accumulated over historical data sets and set up the seed for future Big Data processing, with the final goal to generate added value and discover knowledge. In such computing processes, data are the main resource, however, organizing and managing data during their entire life cycle becomes a complex research topic. As part of this, Data LifeCycle (DLC) models have been proposed to efficiently organize large and complex data sets, from creation to consumption, in any field, and any scale, for an effective data usage and big data exploitation. 2. Several DLC frameworks can be found in the literature, each one defined for specific environments and scenarios. However, we realized that there is no global and comprehensive DLC model to be easily adapted to different scientific areas. For this reason, in this paper we describe the Comprehensive Scenario Agnostic Data LifeCycle (COSA-DLC) model, a DLC model which: i) is proved to be comprehensive as it addresses the 6Vs challenges (namely Value, Volume, Variety, Velocity, Variability and Veracity, and ii), it can be easily adapted to any particular scenario and, therefore, fit the requirements of a specific scientific field. In this paper we also include two use cases to illustrate the ease of the adaptation in different scenarios. We conclude that the comprehensive scenario agnostic DLC model provides several advantages, such as facilitating global data management, organization and integration, easing the adaptation to any kind of scenario, guaranteeing good data quality levels and, therefore, saving design time and efforts for the scientific and industrial communities.","year":2016,"ENTRYTYPE":"inproceedings","doi":"","jcs_value":null,"scimago_value":null},{"author":"Doku, Ronald and Rawat, Danda B. and Liu, Chunmei","title":"Towards Federated Learning Approach to Determine Data Relevance in Big Data","keywords":"Data models;Blockchain;Machine learning;Mobile handsets;Data privacy;Cryptography;Big Data;Federated Learning Approach, Data Relevance, Big Data Analytics, Machine Learning","abstract":"In the past few years, data has proliferated to astronomical proportions; as a result, big data has become the driving force behind the growth of many machine learning innovations. However, the incessant generation of data in the information age poses a needle in the haystack problem, where it has become challenging to determine useful data from a heap of irrelevant ones. This has resulted in a quality over quantity issue in data science where a lot of data is being generated, but the majority of it is irrelevant. Furthermore, most of the data and the resources needed to effectively train machine learning models are owned by major tech companies, resulting in a centralization problem. As such, federated learning seeks to transform how machine learning models are trained by adopting a distributed machine learning approach. Another promising technology is the blockchain, whose immutable nature ensures data integrity. By combining the blockchain's trust mechanism and federated learning's ability to disrupt data centralization, we propose an approach that determines relevant data and stores the data in a decentralized manner.","year":2019,"ENTRYTYPE":"inproceedings","doi":"10.1109\/IRI.2019.00039","jcs_value":null,"scimago_value":null},{"author":"Faroukhi, Abou Zakaria and El Alaoui, Imane and Gahi, Youssef and Amine, Aouatif","title":"Big Data Value Chain: A Unified Approach for Integrated Data Quality and Security","keywords":"Big Data;Security;Data integrity;Data models;Organizations;Decision making;Reliability;Big Data Value Chain;Data Management;Data Quality;Data Security;Process Integration;Orchestration","abstract":"Big Data has grown significantly in recent years. This growth has led organizations to adopt Big Data Value Chains (BDVC) as the appropriate framework for unlocking the value to make suitable decisions. Despite its promising opportunities, Big Data raises new concerns such as data quality and security that could radically impact the effectiveness of the BDVC. These two essential aspects have become an urgent need for any Big Data project to provide meaningful datasets and reliable insights. In this contribution, we highlight the importance of considering data quality and security requirements. Then, we propose a coherent, unified framework that extends BDVC with security and quality aspects. Through quality and security reports, the model can self-evaluate and arrange tasks according to orchestration and monitoring process, allowing the BDVC to evolve at the organization pace and to align strategically with its objectives as well as to federate a sustainable ecosystem.","year":2020,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICECOCS50124.2020.9314391","jcs_value":null,"scimago_value":null},{"author":"Scavuzzo, Marco and Di Nitto, Elisabetta and Ardagna, Danilo","title":"[Journal First] Experiences and Challenges in Building a Data Intensive System for Data Migration","keywords":"Big Data;Databases;Quality of service;Tools;Fault tolerance;Fault tolerant systems;Software;Data intensive applications;Experiment driven action research;Big data;Data migration","abstract":"Recent analyses[2, 4, 5] report that many sectors of our economy and society are more and more guided by data-driven decision processes (e.g., health care, public administrations, etc.). As such, Data Intensive (DI) applications are becoming more and more important and critical. They must be fault-tolerant, they should scale with the amount of data, and be able to elastically leverage additional resources as and when these last ones are provided [3]. Moreover, they should be able to avoid data drops introduced in case of sudden overloads and should offer some Quality of Service (QoS) guarantees. Ensuring all these properties is, per se, a challenge, but it becomes even more difficult for DI applications, given the large amount of data to be managed and the significant level of parallelism required for its components. Even if today some technological frameworks are available for the development of such applications (for instance, think of Spark, Storm, Flink), we still lack solid software engineering approaches to support their development and, in particular, to ensure that they offer the required properties in terms of availability, throughput, data loss, etc. In fact, at the time of writing, identifying the right solution can require several rounds of experiments and the adoption of many different technologies. This implies the need for highly skilled persons and the execution of experiments with large data sets and a large number of resources, and, consequently, a significant amount of time and budget. To experiment with currently available approaches, we performed an action research experiment focusing on developing- testing-reengineering a specific DI application, Hegira4Cloud, that migrates data between widely used NoSQL databases, including so-called Database as a Service (DaaS), as well as on-premise databases. This is a representative DI system because it has to handle large volumes of data with different structures and has to guarantee that some important characteristics, in terms of data types and transactional properties, are preserved. Also, it poses stringent requirements in terms of correctness, high performance, fault tolerance, and fast and effective recovery. In our action research, we discovered that the literature offered some high level design guidelines for DI applications, as well as some tools to support modelling and QoS analysis\/simulation of complex architectures, however the available tools were not yet. suitable to support DI systems. Moreover, we realized that the available big data frameworks we could have used were not flexible enough to cope with all possible application-specific aspects of our system. Hence, to achieve the desired level of performance, fault tolerance and recovery, we had to adopt a time-consuming, experiment-based approach [1, 6], which, in our case, consisted of three iterations: (1) the design and implementation of a Mediation Data Model capable of managing data extracted from different databases, together with a first monholitical prototype of Hegira4Cloud; (2) the improvement of performance of our prototype when managing and transferring huge amounts of data; (3) the introduction of fault-tolerant data extraction and management mechanisms, which are independent from the targeted databases. Among the others, an important issue that has forced us to reiterate in the development of Hegira4Cloud concerned the DaaS we interfaced with. In particular these DaaS, which are well-known services with a large number of users: (1) were missing detailed information regarding the behaviour of their APIs; (2) did not offer a predictable service; (3) were suffering of random downtimes not correlated with the datasets we were experimenting with. In this journal first presentation, we describe our experience and the issues we encountered that led to some important decisions during the software design and engineering process. Also, we analyse the state of the art of software design and verification tools and approaches in the light of our experience, and identify weaknesses, alternative design approaches and open challenges that could generate new research in these areas. More details can be found in the journal publication.","year":2018,"ENTRYTYPE":"inproceedings","doi":"10.1145\/3180155.3182534","jcs_value":null,"scimago_value":null},{"author":"Chenran, Xiong and Youde, Wu","title":"The Geographic Environment Analysis of Regional Economic Development of Yunnan Province of China Based on the Big Data Technology","keywords":"Transportation;Big data;Smart cities;analysis of big data;Yunnan province of China;regional economic development;geographic environment","abstract":"This paper uses the analysis methods of data mining, data quality and management, semantic engine and prediction in big data to analyze the geographic environment of the Yunnan's economic development from its special location and geographic elements, beholding that the geographic elements lay basic material conditions for its economic development of Yunnan province. Geographic factors on one hand encourage the economic development and on the other hand affect together the economic development of Yunnan province.","year":2015,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICITBS.2015.220","jcs_value":null,"scimago_value":null},{"author":"Benbernou, Salima and Ouziri, Mourad","title":"Enhancing data quality by cleaning inconsistent big RDF data","keywords":"Resource description framework;Ontologies;Sparks;Cleaning;Inference algorithms;Automobiles;Jacobian matrices;Big data;Data quality;Inconsistency;Logical inference;RDF data cleaning;Apache Spark ecosystems","abstract":"We address the problem of dealing with inconsistencies in fusion of big data sources using Resource Description Framework (RDF) and ontologies. We propose a scalable approach ensuring data quality for query answering over big RDF data in a distributed way on a Spark ecosystem. In so doing, the cleaning inconsistent big RDF data approach is built on the following steps (1) modeling consistency rules to detect the inconsistency triples even if it is implicitly hidden including inference and inconsistent rules (2) detecting inconsistency through rule evaluation based on Apache Spark framework to discover the minimally sub-set of inconsistent triples (3) cleaning the inconsistency through finding the best repair for consistent query answering.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData.2017.8257913","jcs_value":null,"scimago_value":null},{"author":"Shrivastava, Shrey and Patel, Dhaval and Bhamidipaty, Anuradha and Gifford, Wesley M. and Siegel, Stuart A. and Ganapavarapu, Venkata Sitaramagiridharganesh and Kalagnanam, Jayant R.","title":"DQA: Scalable, Automated and Interactive Data Quality Advisor","keywords":"Data integrity;Machine learning;Pipelines;Cleaning;Libraries;Buildings;Data quality;machine learning;data cleaning;scalability;automation;data science","abstract":"Fueled with growth in the fields of Internet of Things (IoT) and Big Data, data has become one of the most valuable assets in today's world. While we are leveraging this data for analyzing complex systems using machine learning and deep learning, a considerable amount of time and effort is spent on addressing data quality issues. If undetected, data quality issues can cause large deviations in the analysis, misleading data scientists. To ease the effort of identifying and addressing data quality challenges, we introduce DQA, a scalable, automated and interactive data quality advisor. In this paper, we describe the DQA framework, provide detailed description of its components and the benefits of integrating it in a data science process. We propose a programmatic approach for implementing the data quality framework which automatically generates dynamic executable graphs for performing data validations fine-tuned for a given dataset. We discuss the use of DQA to build a library of validation checks common to many applications. We provide insight into how DQA addresses many persistence and usability issues which currently make data cleaning a laborious task for data scientists. Finally, we provide a case study of how DQA is implemented in a realworld system and describe the benefits realized.","year":2019,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData47090.2019.9006187","jcs_value":null,"scimago_value":null},{"author":"Cao, Rui and Gao, Jing","title":"Research on reliability evaluation of big data system","keywords":"Big Data;Fault trees;Software;Software reliability;Data models;Hardware;big data system;reliability;fault tree;evaluation","abstract":"The application of big data system is now more pervasive. The reliability of the large data system is crucial to both the academic and the industry. However, to date there are few studies on the reliability of the big data system, and lack of evaluation model. This paper uses the fault tree to model the reliability of the big data system on the cloud. The type of faults is summarized and the cause of fault is analyzed by experiments. The fault tree analysis (FTA) is used to evaluate the reliability of the big data system, which can provide reference for the fault processing and quality assurance of big data system.","year":2018,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICCCBDA.2018.8386523","jcs_value":null,"scimago_value":null},{"author":"Rettig, Laura and Khayati, Mourad and Cudr\u00e9-Mauroux, Philippe and Pi\u00f3rkowski, Michal","title":"Online anomaly detection over Big Data streams","keywords":"Entropy;Measurement;Correlation;Big data;Data structures;Yttrium;Sparks","abstract":"Data quality is a challenging problem in many real world application domains. While a lot of attention has been given to detect anomalies for data at rest, detecting anomalies for streaming applications still largely remains an open problem. For applications involving several data streams, the challenge of detecting anomalies has become harder over time, as data can dynamically evolve in subtle ways following changes in the underlying infrastructure. In this paper, we describe and empirically evaluate an online anomaly detection pipeline that satisfies two key conditions: generality and scalability. Our technique works on numerical data as well as on categorical data and makes no assumption on the underlying data distributions. We implement two metrics, relative entropy and Pearson correlation, to dynamically detect anomalies. The two metrics we use provide an efficient and effective detection of anomalies over high velocity streams of events. In the following, we describe the design and implementation of our approach in a Big Data scenario using state-of-the-art streaming components. Specifically, we build on Kafka queues and Spark Streaming for realizing our approach while satisfying the generality and scalability requirements given above. We show how a combination of the two metrics we put forward can be applied to detect several types of anomalies - like infrastructure failures, hardware misconfiguration or user-driven anomalies - in large-scale telecommunication networks. We also discuss the merits and limitations of the resulting architecture and empirically evaluate its scalability on a real deployment over live streams capturing events from millions of mobile devices.","year":2015,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData.2015.7363865","jcs_value":null,"scimago_value":null},{"author":"Stojanovic, Nenad and Dinic, Marko and Stojanovic, Ljiljana","title":"Big data process analytics for continuous process improvement in manufacturing","keywords":"Clustering algorithms;Big data;Process control;Partitioning algorithms;Manufacturing;Six sigma;Analytical models;Big data;manufactoring;quality control","abstract":"One of the most important challenges in manufacturing is the continuous process improvement that requires new insights about the behavior\/quality control of processes in order to understand the optimization\/improvement potential. The paper elaborates on usage of big data-driven clustering for an efficient discovering of real-time unusualities in the process and their route-cause analysis. Our approach extends traditional clustering algorithms (like k-Means) with methods for better understanding the nature of clusters and provides a very efficient big data realization. We argue that this approach paves the way for a new generation of quality management tools based on big data analytics that will extend traditional statistical process control and empower Lean Six Sigma through big data processing. The proposed approach has been applied for improving process control in Whirlpool (washing machine tests, factory in Italy) and we present the most important finding from the evaluation study.","year":2015,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData.2015.7363900","jcs_value":null,"scimago_value":null},{"author":"Ezzine, Imane and Benhlima, Laila","title":"A Study of Handling Missing Data Methods for Big Data","keywords":"Data integrity;Big Data;Data models;Dictionaries;Machine learning algorithms;Machine learning;Measurement;data quality;missing data;big data;functional dependancy;master data;machine learning","abstract":"Improving data quality is not a recent field but in the context of big data this is a challenging area as there is a crucial need for data quality to, for example, increase the accuracy of big data analytics or avoid storing redundant data. Missing data is one of the major problem that faces the quality of data. There are several methods and approaches that have been used in relational databases to handle missing data most of which have been adapted to big data. This paper aims to provide an overview of some methods and approaches for handling missing data in big data contexts.","year":2018,"ENTRYTYPE":"inproceedings","doi":"10.1109\/CIST.2018.8596389","jcs_value":null,"scimago_value":null},{"author":"Jiang, Wei and Ning, Xiuli and Xu, Yingcheng","title":"Review on Big Data Fusion Methods of Quality Inspection for Consumer Goods","keywords":"Big Data;Inspection;Data integration;Semantics;Hidden Markov models;Thesauri;Support vector machines;Big data of quality inspection;data fusion;named entity recognition;entity resolution;data conflict resolution","abstract":"Quality big data comes from a wide range, the problem is how to eliminate the structure barriers between various types of data from different sources, to achieve the effective integration for isolated or fragmented data, and then to mine the information, knowledge and wisdom according to the actual needs. It is urgent for the quality inspection departments to solve the problem of making a decision the first time and take preventive measures. This paper introduced the research status of named entity recognition, solid unified detection, data conflict resolution, data fusion method from the characteristics of quality inspection data. The existence problems are analyzed and the research direction is looking forward of the research on data fusion in this paper.","year":2018,"ENTRYTYPE":"inproceedings","doi":"10.1109\/CSCloud\/EdgeCom.2018.00025","jcs_value":null,"scimago_value":null},{"author":"Micic, Natasha and Neagu, Daniel and Campean, Felician and Zadeh, Esmaeil Habib","title":"Towards a Data Quality Framework for Heterogeneous Data","keywords":"Data integrity;Measurement;Warranties;Metadata;Cleaning;Big Data;Heterogeneous Data Sets;Data Quality;Metadata;Data Cleaning;Data Quality Assessment","abstract":"Every industry has significant data output as a product of their working process, and with the recent advent of big data mining and integrated data warehousing it is the case for a robust methodology for assessing the quality for sustainable and consistent processing. In this paper a review is conducted on Data Quality (DQ) in multiple domains in order to propose connections between their methodologies. This critical review suggests that within the process of DQ assessment of heterogeneous data sets, not often are they treated as separate types of data in need of an alternate data quality assessment framework. We discuss the need for such a directed DQ framework and the opportunities that are foreseen in this research area and propose to address it through degrees of heterogeneity.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1109\/iThings-GreenCom-CPSCom-SmartData.2017.28","jcs_value":null,"scimago_value":null},{"author":"Karafili, Erisa and Lupu, Emil C. and Cullen, Alan and Williams, Bill and Arunkumar, Saritha and Calo, Seraphin","title":"Improving data sharing in data rich environments","keywords":"Big Data;Drones;Data privacy;Electronic mail;Access control;Big data;data sharing;data access;usage control;DSAs;drone systems;military scenario","abstract":"The increasing use of big data comes along with the problem of ensuring correct and secure data access. There is a need to maximise the data dissemination whilst controlling their access. Depending on the type of users different qualities and parts of data are shared. We introduce an alteration mechanism, more precisely a restriction one, based on a policy analysis language. The alteration reflects the level of trust and relations the users have, and are represented as policies inside the data sharing agreements. These agreements are attached to the data and are enforced every time the data are accessed, used or shared. We show the use of our alteration mechanism with a military use case, where different parties are involved during the missions, and they have different relations of trust and partnership.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData.2017.8258270","jcs_value":null,"scimago_value":null},{"author":"Zhang, Pengcheng and Zhou, Xuewu and Li, Wenrui and Gao, Jerry","title":"A Survey on Quality Assurance Techniques for Big Data Applications","keywords":"Big Data applications;Quality assurance;Testing;Data models;Monitoring;Computer architecture;Quality Assurance;Big data;Big data application;MDA;Testing;Verification;Fault tolerance;Monitoring;Prediction","abstract":"With the rapid advance of big data and cloud computing, building high quality big data systems in different application fields has gradually became a popular research topic in academia and industry as well as government agencies. However, more quality problems lead to application errors. Although the current research work has discussed how to ensure the quality of big data applications from several aspects, there is no systematic discussion on how to ensure the quality of large data applications. Therefore, a systematic study on big data application quality assurance is very necessary and critical. This paper focuses on the survey of quality assurance techniques of big data applications, and it introduces big data properties and quality attributes. It mainly discusses the key approaches to ensure the quality of big data applications and they are testing, model-driven architecture (MDA), monitoring, fault tolerance, verification and also prediction techniques. In addition, this paper also discusses the impact of big data characteristics on big data applications.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigDataService.2017.42","jcs_value":null,"scimago_value":null},{"author":"Kumar, Sunil and Singh, Maninder","title":"A novel clustering technique for efficient clustering of big data in Hadoop Ecosystem","keywords":"Clustering algorithms;Big Data;Ocean temperature;Data mining;Meteorology;Partitioning algorithms;Temperature control;clustering;Hadoop;big data;k-means;hierarchical","abstract":"Big data analytics and data mining are techniques used to analyze data and to extract hidden information. Traditional approaches to analysis and extraction do not work well for big data because this data is complex and of very high volume. A major data mining technique known as data clustering groups the data into clusters and makes it easy to extract information from these clusters. However, existing clustering algorithms, such as k-means and hierarchical, are not efficient as the quality of the clusters they produce is compromised. Therefore, there is a need to design an efficient and highly scalable clustering algorithm. In this paper, we put forward a new clustering algorithm called hybrid clustering in order to overcome the disadvantages of existing clustering algorithms. We compare the new hybrid algorithm with existing algorithms on the bases of precision, recall, F-measure, execution time, and accuracy of results. From the experimental results, it is clear that the proposed hybrid clustering algorithm is more accurate, and has better precision, recall, and F-measure values.","year":2019,"ENTRYTYPE":"article","doi":"10.26599\/BDMA.2018.9020037","jcs_value":null,"scimago_value":null},{"author":"Wang, Xin and Zhao, Xinbin and Yu, Liling","title":"Data Mining on the Flight Quality of an Airline based on QAR Big Data","keywords":"Quality assurance;Atmospheric modeling;Big Data;Data models;Time measurement;Safety;Mathematical model;flight quality;pitch;QAR data;normal distribution;t test","abstract":"At present, the airlines have made some achievements in event analysis and investigation by using their quick access record (QAR) data. But where each airline's flight quality is in the industry, and whether there is a problem in itself, the airline can't find. In order to help airlines discover the existing flight quality problems, this article uses the QAR big data of the flight operational quality assurance (FOQA) Station of CAAC, and compares the industry-wide QAR data with the QAR data of individual airlines, and founds that the take-off pitch angle of a certain aircraft of A321 models is too small, by using mathematical statistics t test to verify, found the airline's the take-off pitch angle and the industry's the take-off pitch angle exist significant difference. The correlative speed at rotation and the speed at liftoff are also analyzed, and the significant difference is found. The FOQA Station of CAAC feeds back the problem to the airline and the authority. After the investigation of the airline and the authority, there are problems with the airline. And the airline immediately starts to rectify it.","year":2020,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICCASIT50869.2020.9368701","jcs_value":null,"scimago_value":null},{"author":"Xu, Birong and Wang, Weijiang and Wu, Yuyan and Shi, Yueting and Lu, Chang","title":"Internet of things and big data analytics for smart oil field malfunction diagnosis","keywords":"Oils;Production;Standards;Fluids;Moisture;Big Data;Algorithm design and analysis;internet of thing;warning thresholds scheme;oil field fault diagnosis;big data;6 sigma","abstract":"With the rapid development of information technology and digital communication, the data types are more abundant by integration of various technologies. In this paper, based on the analysis of a large number of historical data of oil and water wells, the changes of some important parameters of the wells can be monitored and then used in the trend prediction and the early warning system. Subsequently, we use 6 Sigma algorithm to process the historical data, and by the big data trend analysis combining with various parameters, we can diagnose six operating conditions, such as sand production, abnormal of moisture content etc. Through experiments, the algorithm is stable and reliable in practical application, and it has great significance to ensure the normal production of oil field and improve the management ability for oil field.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICBDA.2017.8078802","jcs_value":null,"scimago_value":null},{"author":"Gan, Wenting","title":"Design of Network Precision Marketing Based on Big Data Analysis Technology","keywords":"Visualization;Data analysis;Databases;Data integrity;Semantics;Big Data;Internet;Big Data;E-commerce;Internet marketing;Marketing design","abstract":"In the process of big data processing, big data analysis is the core work content. After obtaining a large amount of data, we use related analysis technology to perform data processing and analysis to obtain knowledge. Its related content includes visual analysis, data mining, predictive analysis, semantic analysis and data quality management. How to obtain big data, classify and store according to data types, mine valuable information from big data, and effectively apply big data in the field of precision marketing are hot topics of research. On the basis of researching the key technologies of big data analysis, this paper uses Hadoop big data to analyze and store the massive online logs generated by users' mobile terminals, and calculates and builds user characteristic databases. We use relevant analysis technology to analyze the user's location information, browsing and usage habits, hobbies, and focus content. At the same time, a precise marketing model is established according to user behavior characteristics and attributes, thereby improving the marketing effect of the enterprise.","year":2020,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ECIT50008.2020.00026","jcs_value":null,"scimago_value":null},{"author":"Zhang, Guobao","title":"A data traceability method to improve data quality in a big data environment","keywords":"Data Governance;Data Credibility;Data Traceability","abstract":"In the actual project of data sharing and data governance, the problems of data heterogeneity and low data quality have not been solved. Data quality detection based on syntax rules can effectively find data quality problems, but it had not improved the data quality, especially in a variety of heterogeneous data source environments. This paper designs a data governance model based on data traceability, and we can get data feedback and revision through this model. The proposed method considers the different ownership of data and may form a closed-loop data service chain including effective data validation, data tracking, and data revision, data release. The analysis of the case shows that the method is effective to improve the data quality and meet the requirements of data security also.","year":2020,"ENTRYTYPE":"inproceedings","doi":"10.1109\/DSC50466.2020.00051","jcs_value":null,"scimago_value":null},{"author":"Hee, Kim","title":"Is data quality enough for a clinical decision?: Apply machine learning and avoid bias","keywords":"Data mining;Medical diagnostic imaging;Currencies;Tools;Medical services;Measurement;data quality;decision quality;healthcare;clinical data;EHRs;data quality assurance;stratified sampling;bias","abstract":"This paper provides a practical guideline for the assurance and (re-)usage of clinical data. It proposes a process which aims to provide a systematic data quality assurance even without involving a medical domain expert. Especially when (re-)using clinical data, data quality is an important topic because clinical data are not purposely collected. Therefore, data driven conclusions might be false, because a given dataset is not representative. These false data driven conclusions could even harm the life of patients. Thus, all researchers should adhere to some basic principles that can prevent false conclusions. Twelve empirical experiments were conducted in order to prove that my process is able to assure data quality with respect to the descriptive and predictive analysis. Descriptive results obtained by applying stratified sampling are conflicting in four out of nine population inputs. Sampling is carried based on the top ranked feature drawn by the Contextual Data Quality Assurance (CDQA). Between datasets these features are confirmed by the Mutual Data Quality Assurance (MDQA). Stratified sampled inputs improve predictive results compared to raw data. Both Area Under the Curve (AUC) scores and accuracy scores increase by three percent.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData.2017.8258221","jcs_value":null,"scimago_value":null},{"author":"Guler, Emine Rumeysa and Ozdemir, Suat","title":"Applications of Stream Data Mining on the Internet of Things: A Survey","keywords":"Big Data;Deep learning;Python;Cyber terrorism;Mathematical model;Data mining;Nanoelectromechanical systems;IoT;Big Data Analytics;Deep Learning;Stream Data Mining;Data Processing Platforms","abstract":"In the era of the Internet of Things (IoT), enormous amount of sensing devices collect and\/or generate various sensory data over time for a wide range of fields and applications. Based on the nature of the application, these devices result in big or fast\/real time data streams. The analytics technique on the subject matter used to discover new information, anticipate future predictions and make decisions on important issues makes IoT technology valuable for both the business world and the quality of everyday life. In this study, first of all, the concept of IoT and its architecture and relation with big and streaming data are emphasized. Information discovery process applied to the IoT streaming data is investigated and deep learning frameworks covered by this process are described comparatively. Finally, the most commonly used tools for analyzing IoT stream data are introduced and their characteristics are revealed.","year":2018,"ENTRYTYPE":"inproceedings","doi":"10.1109\/IBIGDELFT.2018.8625289","jcs_value":null,"scimago_value":null},{"author":"Feng, Peilu","title":"Big Data Analysis of E-Commerce Based on the Internet of Things","keywords":"Conferences;Transportation;Big Data;Smart cities;ISO;Big Data;Internet of things;electronic commerce","abstract":"In the era of big data, while providing massive information, it also challenges the development of related activities in the overall environment. In the context of the rapid development of e-commerce, the opportunities of the development of the Internet of things technology are analyzed from the aspects of logistics distribution, quality control and facilities promotion. Electronic commerce is a new form of trade under the development of modern information technology, while cloud computing and the Internet of Things provide related services. Under the exertion of their related functions, the revolutionary improvement of e-commerce mode has been realized, and to a certain extent, it has promoted the development and operation of modern market economy. This article analyzes the development strategy of e-commerce based on Internet of things and cloud computing under the overall environment of big data era.","year":2019,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICITBS.2019.00091","jcs_value":null,"scimago_value":null},{"author":"Zhang, Lanlan and Zou, Du","title":"Product quality prediction of rolling mill in big data environment","keywords":"Data models;Neural networks;Analytical models;Predictive models;Rough surfaces;Surface roughness;Mathematical model;Product quality;BP neural network;data analysis;surface roughness;thickness error","abstract":"With the wide use of rolling mill in iron and steel industry, the quality of rolling mill products has become the primary goal of people. However, due to design defects and manufacturing quality problems, the quality of steel products is seriously affected, and the surface roughness and thickness of steel plate are important quality indicators. In this paper, by analyzing a large number of monitoring data of rolling mill condition and using BP neural network model [1], the discrete system model between monitoring data and \u201csurface roughness\u201d and \u201cthickness error\u201d of rolling steel plate is further established.","year":2020,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICBDIE50010.2020.00015","jcs_value":null,"scimago_value":null},{"author":"Chengzan, Li and Yanfei, Hou and Jianhui, Li and Lili, Zhang","title":"ScienceDB: A Public Multidisciplinary Research Data Repository for eScience","keywords":"Metadata;Business;Data visualization;Data mining;Distributed databases;Collaboration;Computer architecture;research data repository;technical framework;multidiscipline;data recommendation;data collaboration;open data","abstract":"Research data repositories are necessary infrastructures that ensure the data generated for research are accessible, stable, reliable, and reusable. Based on years of accumulated data work experience, the Computer Network Information Center of the Chinese Academy of Sciences has built a multi-disciplinary data repository ScienceDB for research users and teams using its big data storage, analysis and computing environments. This paper firstly introduces the motivation to develop ScienceDB and gives a profile to it. Then the overall technical framework of ScienceDB is introduced, and the key technologies such as the support for multidiscipline extensibility, data collaboration and data recommendation are analyzed deeply. And then this paper presents the functions and features of ScienceDB's current version and discusses some issues such as its data policy, data quality assurance measures, and current application status. Finally, it summarizes and puts forward that it needs to carry out more in-depth research and practice of ScienceDB in order to meet the higher requirements of eScience in terms of thorough data association and fusion, data analysis and mining, data evaluation, and so on.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1109\/eScience.2017.38","jcs_value":null,"scimago_value":null},{"author":"Chen, Zeqiang and Chen, Nengcheng and Gong, Jianya","title":"Design and implementation of the real-time GIS data model and Sensor Web service platform for environmental big data management with the Apache Storm","keywords":"Real-time systems;Storms;Data models;Web services;Big data;Air quality;Soil moisture;real-time GIS data model;Sensor Web;environmental big data;Apache Storm","abstract":"An abstract real-time GIS data model and Sensor Web service platform was proposed to manage real-time environmental data. With the development of sensor technology, more and more sensor networks are deployed to monitor our environment, and then generate environmental big data. How to improve the real-time GIS data model and Sensor Web service platform for real-time environmental big data manage is a problem. In this paper, the Apache Storm is adopted to deal with the question. A design and implementation of the real-time GIS data model and Sensor Web service platform for environmental big data management with Apache Storm is proposed. The main studied contents include integrating the Apache Strom with the Sensor Web service as the Sensor Observation Service, and processing the environmental big data timely. To test the feasibility of the design and implementation, two use cases of real-time air quality monitoring and real-time soil moisture monitoring based on the real-time GIS data model in the Sensor Web service platform are realized and demonstrated. The experimental results show that the implementation of real-time GIS data model and Sensor Web Service Platform with the Apache Storm is an effective way to manage real-time environmental big data.","year":2015,"ENTRYTYPE":"inproceedings","doi":"10.1109\/Agro-Geoinformatics.2015.7248139","jcs_value":null,"scimago_value":null},{"author":"Vidal, Maria-Esther and Jozashoori, Samaneh and Sakor, Ahmad","title":"Semantic Data Integration Techniques for Transforming Big Biomedical Data into Actionable Knowledge","keywords":"Drugs;Data mining;Semantics;Data integration;Unified modeling language;Ontologies;Bioinformatics;Semantic Data Integration;Big Data;Knowledge Graph;Biomedical Data;Natural Language Processing","abstract":"FAIR principles and the Open Data initiatives have motivated the publication of large volumes of data. Specifically, in the biomedical domain, the size of the data has increased exponentially in the last decade, and with the advances in the technologies to collect and generate data, a faster growth rate is expected for the next years. The available collections of data are characterized by the dominant dimensions of big data, i.e., they are not only large in volume, but they can be also heterogeneous and present quality issues. These data complexity problems impact on the typical tasks of data management, and particularly, in the task of integrating big biomedical data sources. We tackle the problem of big data integration and present a knowledge-driven framework able to extract and integrate data collected from structured and unstructured data sources. The proposed framework resorts to Natural Language Processing techniques to extract knowledge from unstructured data and short text. Furthermore, ontologies and controlled vocabularies, e.g., UMLS, are utilized to annotate the extracted entities and relations with terms from the ontology or controlled vocabulary. The annotated data is integrated into a knowledge graph. A unified schema is used to describe the meaning of the integrated data as well as the main properties and relations. As proof of concept, we show the results of applying the proposed framework to integrate clinical records from lung cancer patients with data extracted from open data sources like Drugbank and PubMed. The created knowledge graph enables the discovery of interactions between drugs in the treatments prescribed to lung cancer patients.","year":2019,"ENTRYTYPE":"inproceedings","doi":"10.1109\/CBMS.2019.00116","jcs_value":null,"scimago_value":null},{"author":"Tan, Julian SK and Ang, Ai Kiar and Lu, Liu and Gan, Sheena WQ and Corral, Marilyn G","title":"Quality Analytics in a Big Data supply chain: Commodity data analytics for quality engineering","keywords":"Supply chains;Big data;Manufacturing;Data visualization;Market research;Industries;Supply Chain;Analytics;Industrie 4.0;IoT;Big Data;Internet of Things;Predictive;Prescriptive;Cognitive;Descriptive;Data Management;Data Source;Systems of Engagement;Systems of Records;Quality;Commodity","abstract":"While the world is experiencing a global shortage of natural resources, a new one in the form of Digital Data has emerged! The ability to harness this new resource has become a renewed basis for competitive advantage where leveraging Big Data effectively means winning in the marketplace. It is going to transform industries and professions around the world. However, traditional data management techniques and analytical methodologies that has taken us from the late 20th century and into the early 21st century are not sustainable in today's business environment where organizations are constantly being challenged to right size the work force, increase labor productivity, increase customer satisfaction and at the same time improving product quality and reliability.","year":2016,"ENTRYTYPE":"inproceedings","doi":"10.1109\/TENCON.2016.7848697","jcs_value":null,"scimago_value":null},{"author":"Stojanovic, Ljiljana and Dinic, Marko and Stojanovic, Nenad and Stojadinovic, Aleksandar","title":"Big-data-driven anomaly detection in industry (4.0): An approach and a case study","keywords":"Real-time systems;Quality control;Process control;Production;Monitoring;Industries;Big data;Big Data;Anomaly Detection;CEP;Data-driven Quality Control;Industrie 4.0","abstract":"In this paper we present a novel approach for data-driven Quality Management in industry processes that enables a multidimensional analysis of the anomalies that can appear and their real-time detection in the running system. The approach revolutionizes the way how quality control (and esp. anomaly detection) will be realized in production processes influenced by many parameters that can be in complex nonlinear correlations. It consists of two main steps: learning the normal behavior of the system (based on past data) and detecting an anomalous behavior in the real-time (by processing real-time data). The approach is especially suitable for modern industry systems that follow Industry 4.0 principles of ubiquity sensing and proactive responding. One of the main advantages is the self-adaptive nature of the approach due to its data-driven orientation, so that the model and parameters of the approach will be continuously updated to the dynamicity of data. The approach has been applied in the process of manufacturing microwave ovens (Whirlpool) and in this paper we present results for the data-driven quality control of one of the most critical parts - microwave oven fan. Due to the high speed of the rotation, every item has to be very precisely produced (according to the CAD model), which requires very strong quality control process.","year":2016,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData.2016.7840777","jcs_value":null,"scimago_value":null},{"author":"Nowling, Ronald J. and Vyas, Jay","title":"A Domain-Driven, Generative Data Model for Big Pet Store","keywords":"Hidden Markov models;Data models;Probability density function;Big data;Benchmark testing;Generators;big data;synthetic data sets;data generation;benchmarking;testing;probabilistic models","abstract":"Generating large amounts of semantically-rich data for testing big data workflows is paramount for scalable performance benchmarking and quality assurance in modern machine-learning and analytics workloads. The most obvious use case for such a generative algorithm is in conjunction with a big data application blueprint, which can be used by developers (to test their emerging big data solutions) as well as end users (as a starting point for validating infrastructure installations, building novel applications, and learning analytics methods). We present a new domain-driven, generative data model for Big Pet Store, a big data application blueprint for the Hadoop ecosystem included in the Apache Big Top distribution. We describe the model and demonstrate its ability to generate semantically-rich data at variable scale ranging from a single machine to a large cluster. We validate the model by using the generated data to answer questions about customer locations and purchasing habits for a fictional targeted advertising campaign, a common business use case.","year":2014,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BDCloud.2014.38","jcs_value":null,"scimago_value":null},{"author":"Zhou, Lixiao and Huang, Maohai","title":"Challenges of Software Testing for Astronomical Big Data","keywords":"Big Data;Software testing;Software;Astronomy;Data analysis;Distributed databases;software testing; big data; astronomical software","abstract":"Astronomy has been one of the first areas of science to embrace and learn from big data. The amount of data we have on our universe is doubling every year thanks to big telescopes and better light detectors. Most leading research is based on data from a handful of very expensive telescopes. Undoubtedly, the data quality is the key basis for the leading scientific findings. It is imperative that software testers understand that big data is about far more than simply data volume. This paper will analyze characteristics, types, methods, strategies, problems, challenges and propose some possible solutions of software testing for astronomical big data.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigDataCongress.2017.91","jcs_value":null,"scimago_value":null},{"author":"Juddoo, Suraj","title":"Overview of data quality challenges in the context of Big Data","keywords":"Big data;Context;Quality management;Frequency measurement;Organizations;Big Data;Data quality;Data profiling;Data cleansing;data quality rules;dimensions;metrics","abstract":"Data quality management systems are thoroughly researched topics and have resulted in many tools and techniques developed by both academia and industry. However, the advent of Big Data might pose some serious questions pertaining to the applicability of existing data quality concepts. There is a debate concerning the importance of data quality for Big Data; one school of thought argues that high data quality methods are essential for deriving higher level analytics while another school of thought argues that data quality level will not be so important as the volume of Big Data would be used to produce patterns and some amount of dirty data will not mask the analytic results which might be derived. This paper aims to investigate various components and activities forming part of data quality management such as dimensions, metrics, data quality rules, data profiling and data cleansing. The result list existing challenges and future research areas associated with Big Data for data quality management.","year":2015,"ENTRYTYPE":"inproceedings","doi":"10.1109\/CCCS.2015.7374131","jcs_value":null,"scimago_value":null},{"author":"Lincy, S.S. Blessy Trencia and Kumar, N. Suresh","title":"An enhanced pre-processing model for big data processing: A quality framework","keywords":"Big Data;Data models;Algorithm design and analysis;Medical services;Data analysis;Brain modeling;Feature extraction;Big data;pre-processing;relief algorithm;fast mRMR;SparkR","abstract":"With the ever growing trends and technologies a huge volume of data is being evolved each and every second big data has become a supreme approach in data inception, accession, processing and analyzing the heterogeneous, huge amount of data so as to derive useful insights out of it. With data and without quality there is no point in having the data. Thus, data with quality is required to use or leverage the data in a more appropriate manner. With the evolution of big data many technologies are being developed. The input to it must be processed in such a way that the quality data yields quality effective results. An effective pre-processing model is proposed in this paper for the processing of the big data. Using relief algorithm and fast mRMR together as a hybrid approach can be used for the pre-processing of the data. Analysis shows that this hybrid approach is more effective and can greatly enhance the quality of the data. This approach can yield better performance upon the big data platform using the Spark framework.","year":2017,"ENTRYTYPE":"inproceedings","doi":"10.1109\/IGEHT.2017.8094109","jcs_value":null,"scimago_value":null},{"author":"Wang, Wenjing and Yang, Shengquan","title":"Research on Air Quality Forecasting Based on Big Data and Neural Network","keywords":"Air quality;Predictive models;Atmospheric modeling;Data models;Big Data;Biological neural networks;AQI Prediction;Big Data;Neural Network","abstract":"Aiming at the problem that existing air quality prediction models cannot efficiently and accurately predict air quality in a big data environment, an air quality prediction method based on a big data platform to implement a distributed neural network is proposed. Collect historical data of the six pollutant concentrations that affect the air quality index and use it as input to a neural network model; A distributed neural network model containing the AQI change rule in the distributed neural network structure is adopted to realize the short-term prediction of the AQI. Experimental results show that air quality prediction models based on big data and neural networks can reveal the development trend of air quality through self-learning characteristics. And has higher prediction accuracy, It can provide a scientific basis for the degree of urban air pollution and help people make appropriate measures for different AQI levels.","year":2020,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICCNEA50255.2020.00045","jcs_value":null,"scimago_value":null},{"author":"Wang, Songyun and Yuan, Jiabin and Li, Xin and Qian, Zhuzhong and Arena, Fabio and You, Ilsun","title":"Active Data Replica Recovery for Quality-Assurance Big Data Analysis in IC-IoT","keywords":"Nonvolatile memory;Quality of service;Data analysis;Data centers;Bandwidth;Big Data;Robustness;Big data analysis;data recovery;IC-IoT;NVM;QoS improvement","abstract":"QoS-aware big data analysis is critical in Information-Centric Internet of Things (IC-IoT) system to support various applications like smart city, smart grid, smart health, intelligent transportation systems, and so on. The employment of non-volatile memory (NVM) in cloud or edge system provides good opportunity to improve quality of data analysis tasks. However, we have to face the data recovery problem led by NVM failure due to the limited write endurance. In this paper, we investigate the data recovery problem for QoS guarantee and system robustness, followed by proposing a rarity-aware data recovery algorithm. The core idea is to establish the rarity indicator to evaluate the replica distribution and service requirement comprehensively. With this idea, we give the lost replicas with distinguishing priority and eliminate the unnecessary replicas. Then, the data replicas are recovered stage by stage to guarantee QoS and provide system robustness. From our extensive experiments and simulations, it is shown that the proposed algorithm has significant performance improvement on QoS and robustness than the traditional direct data recovery method. Besides, the algorithm gives an acceptable data recovery time.","year":2019,"ENTRYTYPE":"article","doi":"10.1109\/ACCESS.2019.2932259","jcs_value":null,"scimago_value":null},{"author":"Huimin, Li and Guomin, Song","title":"Research on the Teaching Reform of Finance and Accounting Major under the Background of Big Data","keywords":"Training;Technological innovation;Cloud computing;Education;Finance;Information services;Big Data;Big data;Major in Accounting;Teaching reform","abstract":"With the development of information and intelligent technology such as Internet, big data and cloud computing, the concept of big data in education and teaching has also been widely concerned by the society and colleges and universities. The application of big data technology broadens educational resources and teaching channels, and provides a new space for teaching reform and talent training of accounting majors. Based on the background of big data and combined with the characteristics of finance and accounting majors, this paper integrates big data thinking into the teaching reform process of finance and accounting majors, integrates the needs of big data information resources and application ability cultivation of finance and accounting majors, improves the training quality of finance and accounting professionals, and promotes the teaching reform of finance and accounting majors.","year":2020,"ENTRYTYPE":"inproceedings","doi":"10.1109\/ICBASE51474.2020.00023","jcs_value":null,"scimago_value":null},{"author":"Wang, Jianwu and Crawl, Daniel and Purawat, Shweta and Nguyen, Mai and Altintas, Ilkay","title":"Big data provenance: Challenges, state of the art and opportunities","keywords":"Big data;Data models;Distributed databases;Sparks;Engines;Programming;Context;Big Data;provenance;workflows;distributed data-parallel programming models","abstract":"Ability to track provenance is a key feature of scientific workflows to support data lineage and reproducibility. The challenges that are introduced by the volume, variety and velocity of Big Data, also pose related challenges for provenance and quality of Big Data, defined as veracity. The increasing size and variety of distributed Big Data provenance information bring new technical challenges and opportunities throughout the provenance lifecycle including recording, querying, sharing and utilization. This paper discusses the challenges and opportunities of Big Data provenance related to the veracity of the datasets themselves and the provenance of the analytical processes that analyze these datasets. It also explains our current efforts towards tracking and utilizing Big Data provenance using workflows as a programming model to analyze Big Data.","year":2015,"ENTRYTYPE":"inproceedings","doi":"10.1109\/BigData.2015.7364047","jcs_value":null,"scimago_value":null}]